# -----------计算机网络----------------------------------------------------------------------------------------------------

# 1.计算机网络-HTTP-get与post

## 1-1：get与post的区别

1. Get是请求从服务器获取资源，Post用于传输实体本体
2. get和post请求都能使用额外的参数，get参数是以查询字符串出现在URL中，post参数存储在实体主体中
3. Http方法不会改变服务器状态，get方法是安全的，而post由于是传送实体主体内容，这个内容可能是用户上传的表单数据，上传成功后，服务器可能把这个数据
   存储到数据库中，因此状态也就发生了变化
4. get在调用多次时，客户端收到的结果是一样的， 所以是幕等；post调用多次，会增加多行记录，不是幕等
6. get可缓存，post不可缓存
7. 对于get请求，浏览器会把http 头和数据一并发送出去，服务器响应200；post请求，浏览器先发送header，服务器响应之后，浏览器在发送数据，服务器响应
   200

# 2.计算机网络-HTTP-报文结构与状态码

## 2-1：状态码

2xx （3种）

   1. 200 ：表示从客户端发送给服务器的请求被正常处理并返回；

   2. 204 ：表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中不含实体的主体部分（没有资源可以返回）；

   3. 206 ：表示客户端进行了范围请求。

3xx （5种）

   301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；

   302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；

         301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）

   303 See Other：表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；

         302与303的区别：后者明确表示客户端应当采用GET方式获取资源

   304 Not Modified：表示客户端发送附带条件（是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、If-None-Match、If-Range、If-Unmodified-Since中任一首部）的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回改状态码；

   307 Temporary Redirect：临时重定向，与303有着相同的含义，307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；

4xx （4种）

   400 Bad Request：表示请求报文中存在语法错误；
   401 Unauthorized：未经许可，需要通过HTTP认证；
   403 Forbidden：服务器拒绝该次访问（访问权限出现问题）
   404 Not Found：表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用；

5xx （2种）

   500 Inter Server Error：表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时；
   503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，无法处理请求；

## 2-2：HTTP请求组成

一个HTTP请求报文由四个部分组成：请求行、请求头、空行、请求数据。

1. 请求行

请求行由请求方法字段、URL字段和HTTP协议版本字段3个字段组成，它们用空格分隔。比如 GET /data/info.html HTTP/1.1

2. 请求头部

HTTP客户程序(例如浏览器)，向服务器发送请求的时候必须指明请求类型(一般是GET或者 POST)。如有必要，客户程序还可以选择发送其他的请求头。大多数请求头并不是必需的，。

常见的请求头字段含义：

Accept： 浏览器可接受的MIME类型。

Accept-Charset：浏览器可接受的字符集。

Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。

Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到。

Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头应答

Content-Length：表示请求消息正文的长度。

Host： 客户机过这个头告诉服务器，想访问的主机名。Host头域指定请求资源的Intenet主机和端口号，必须表示请求url的原始服务器或网关的位置。HTTP/1.1请求必须包含主机头域，否则系统会以400状态码返回。

If-Modified-Since：客户机通过这个头告诉服务器，资源的缓存时间。只有当所请求的内容在指定的时间后又经过修改才返回它，否则返回304“Not Modified”应答。

Referer：客户机通过这个头告诉服务器，它是从哪个资源来访问服务器的(防盗链)。包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。

User-Agent：User-Agent头域的内容包含发出请求的用户信息。浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用。

Cookie：客户机通过这个头可以向服务器带数据，这是最重要的请求头信息之一。

Pragma：指定“no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝。

From：请求发送者的email地址，由一些特殊的Web客户程序使用，浏览器不会用到它。

Connection：处理完这次请求后是否断开连接还是继续保持连接。如果Servlet看到这里的值为“Keep- Alive”，或者看到请求使用的是HTTP 1.1(HTTP 1.1默认进行持久连接)，它就可以利用持久连接的优点，当页面包含
多个元素时(例如Applet，图片)，显著地减少下载所需要的时间。要实现这一点，Servlet需要在应答中发送一个Content-Length头，最简单的实现方法是：先把内容写入 ByteArrayOutputStream，然后在正式写出内容
之前计算它的大小。

Range：Range头域可以请求实体的一个或者多个子范围。例如，

表示头500个字节：bytes=0-499

表示第二个500字节：bytes=500-999

表示最后500个字节：bytes=-500

表示500字节以后的范围：bytes=500-

第一个和最后一个字节：bytes=0-0,-1

同时指定几个范围：bytes=500-600,601-999
  
但是服务器可以忽略此请求头，如果无条件GET包含Range请求头，响应会以状态码206(PartialContent)返回而不是以200 (OK)。

UA-Pixels，UA-Color，UA-OS，UA-CPU：由某些版本的IE浏览器所发送的非标准的请求头，表示屏幕大小、颜色深度、操作系统和CPU类型。

3. 空行

它的作用是通过一个空行，告诉服务器请求头部到此为止。

4. 请求数据
若方法字段是POST,则通常来说此处放置的就是要提交的数据


# 3.计算机网络-HTTP-HTTP的1.0-3.0

## 3-1：HTTP1.0优缺点

<font color=red size='5'>I.优点</font>

1. HTTP基本的报⽂格式就是header + body，头部信息也是key-value简单⽂本的形式。

2. HTTP协议⾥的各类请求⽅法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发⼈员⾃定义和扩充。

3. HTTP由于是⼯作在应⽤层，则它下层可以随意变化。

4. 应⽤⼴泛和跨平台

<font color=red size='5'>II.缺点</font>

1. ⽆状态
由于⽆状态，它在完成有关联性的操作时会⾮常麻烦。例如登录->添加购物⻋->下单->结算->⽀付，这系列操作都要知道⽤户的身份才⾏。但服务器不知道这些请求是有关联的，每次都要问⼀遍身份信息。

2. 明⽂传输

明⽂意味着在传输过程中的信息，是可⽅便阅读的，通过浏览器的控制台或抓包软件都可以直接⾁眼查看，信息的内容都毫⽆隐私可⾔，很容易就能被窃取。

## 3-2：HTTP/1.1相对于HTTP1.0改善

1. ⻓连接，早期HTTP/1.0，那就是每发起⼀个请求需要三次握手四次挥手等等操作，增加了通信开销。为了解决这些问题，HTTP/1.1提出了⻓���接的通信⽅式只要任意⼀端没有明确提出断开连接，则保持 TCP 连接状态。
   
2. HTTP/1.1 采⽤了⻓连接的⽅式，可在同⼀个 TCP 连接⾥⾯，客户端可以发起多个请求，只要第⼀个请求发出去了，不必等其回来，就可以发第⼆个请求出去，可以减少整体的响应时间。

3. 错误状态响应码，HTTP1.1新增了很多错误装填响应码，让开发者更加了解错误根源

4. 在HTTP1.0中主要使⽤header⾥的If-Modified-Since,Expires来做为缓存判断的标准，在HTTP1.1中引⼊了更多的缓存控制策略

5. HTTP1.0中，存在⼀些浪费带宽的现象，例如客户端只是需要某个对象的⼀部分，⽽服务器却将整个对象送过来了，并且不⽀持断点续传功能，HTTP1.1则在请求头引⼊了range头域，它允许只请求资源的某个部分

## 3-3：HTTP1.1缺点
1. 请求 / 响应头部未经压缩就发送，⾸部信息越多延迟越⼤。
2. 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端⼀直请求不到数据，也就是队头阻塞；
3. 请求只能从客户端开始，服务器只能被动响应。


## 3-4：HTTP⻓连接,短连接(也是TCP连接,短连接)

1. 在HTTP/1.0中默认使⽤短连接。也就是说，客户端和服务器每进⾏⼀次HTTP操作，就建⽴⼀次连接，任务结束就中断连接。
2. 从HTTP/1.1起，默认使⽤⻓连接。客户端和服务器之间⽤于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使⽤这⼀条已经建⽴的连接

## 3-5：HTTP/2 做了什么优化？

1. HTTP/2 会压缩头如果你同时发出多个请求，他们的头是⼀样的或是相似的，那么，协议会帮你消除重复的部分。
  
  这就是所谓的 HPACK 算法：在客户端和服务器同时维护⼀张头信息表，所有字段都会存⼊这个表，⽣成⼀个索引号，以后就不发送同样字段了，只发送索引号，这样就提⾼速度了。

2. HTTP/2全⾯采⽤了⼆进制格式，头信息和数据体都是⼆进制，计算机收到报⽂后，直接解析⼆进制报⽂，这增加了数据传输的效率。

3. HTTP/2的数据包不是按顺序发送的

4. HTTP/2是可以在⼀个连接中并发多个请求或回应。

5. HTTP/2 还在⼀定程度上改善了传统的「请求 - 应答」⼯作模式，服务不再是被动地响应，也可以主动向客户端发送消息。

## 3-6：HTTP/2有哪些缺陷？ 

多个HTTP请求在复⽤⼀个TCP连接，下层的TCP协议是不知道有多少个HTTP请求的。所以⼀旦发⽣了丢包现象，就会触发TCP的重传机制，这样在⼀个TCP连接中的所有的 HTTP请求都必须等待这个丢了的包被重传回来。

## 3-7：HTTP/3做了哪些优化？

HTTP/3使用基于UDP协议的QUIC协议来实现的

HTTP/3把HTTP下层的TCP协议改成了UDP

因为UDP发⽣是不管顺序，也不管丢包的，所以不会出现HTTP/1.1的队头阻塞和HTTP/2的⼀个丢包全部重传问题。

但是由于UDP是不可靠传输的，而基于UDP的QUIC协议可以实现类似TCP的可靠性传输。主要是依赖

1. 当某个流发⽣丢包时，只会阻塞这个流， 其他流不会受到影响。
2. 更改了头部压缩算法，升级成了 QPack 。
3. QUIC 直接把以往的TCP和TLS/1.3的6次交互合并成了3次，减少了交互次数。


## 3-8：http工作流程

域名解析 -> 三次握手 -> 发起HTTP请求 -> 响应HTTP请求并得到HTML代码 -> 浏览器解析HTML代码  -> 浏览器对页面进行渲染呈现给用户

# 4.计算机网络-HTTP-HTTPs

## 4-1：HTTP与HTTPS区别

1. HTTP 是超⽂本传输协议，信息是明⽂传输，存在安全⻛险的问题。 HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP ⽹络层之间加⼊了 SSL/TLS 安全协议，使得报⽂能够加密传输。

2. HTTP 连接建⽴相对简单，TCP 三次握⼿之后便可进⾏HTTP的报⽂传输。⽽ HTTPS 在 TCP 三次握⼿之后，还需进⾏SSL/TLS的握⼿过程，才可进⼊加密报⽂传输。

3. HTTP 的端⼝号是 80， HTTPS 的端⼝号是 443。

4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。

## 4-2：HTTPS 解决了 HTTP 的哪些问题？

HTTP 由于是明⽂传输，所以安全上存在以下三个⻛险：

1. 窃听⻛险，⽐如通信链路上可以获取通信内容，⽤户号容易没。

2. 篡改⻛险，⽐如强制植⼊垃圾⼴告，视觉污染，⽤户眼容易瞎。

3. 冒充⻛险，⽐如冒充淘宝⽹站，⽤户钱容易没。

HTTPS 在 HTTP 与 TCP 层之间加⼊了 SSL/TLS 协议，可以很好的解决了上述的⻛险：

1. 混合加密的⽅式实现信息的机密性，解决了窃听的⻛险。

HTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：

在通信建立 前 采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密；在 通信过程中 全部使用对称加密的「会话秘钥」的方式加密明文数据。

采用「混合加密」的方式的原因：

对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。

2. 摘要算法的⽅式来实现完整性，它能够为数据⽣成独⼀⽆⼆的「指纹」，指纹⽤于校验数据的完整性，解决了篡改的⻛险。

客户端在发送明文之前会通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同加密成密文后，发送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，若「指纹」相同，说明数据是完整的。

3. 将服务器公钥放⼊到数字证书中，解决了冒充的⻛险。

## 4-3：HTTPS 是如何建⽴连接的？其间交互了什么？

1. 客户端向服务器索要并验证服务器的公钥。
2. 双⽅协商⽣产「会话秘钥」。
3. 双⽅采⽤「会话秘钥」进⾏加密通信。


## 4-4：SSL/TLS握⼿

1. ClientHello

⾸先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。在这⼀步，客户端主要向服务器发送以下信息：

（1）客户端⽀持的 SSL/TLS 协议版本。

（2）客户端⽣产用于「会话秘钥」的随机数。

（3）客户端⽀持的密码套件列表。

2. SeverHello

服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello 。服务器回应的内容有如下内容：

（1）确认 SSL/ TLS 协议版本，如果浏览器不⽀持，则关闭加密通信。

（2）服务器⽣产的随机数（ Server Random ），后⾯⽤于⽣产「会话秘钥」。

（3）确认的密码套件列表，如 RSA 加密算法。

（4）服务器的数字证书。

3. 客户端回应

客户端收到服务器的回应之后，⾸先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。

如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使⽤它加密报⽂，向服务器发送如下信息：

（1）⼀个随机数（ pre-master key ）

（2）加密通信算法改变通知

（3）客户端握⼿结束通知

4. 服务器的最后回应

服务器收到客户端的第三个随机数（ pre-master key ）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后，向客户端发⽣最后的信息：

（1）加密通信算法改变通知。

（2）服务器握⼿结束通知。

# 5.加密

## 5-1：HTTPS的加密过程

1. 用户在浏览器发起HTTPS请求（如 https://www.mogu.com/），默认使用服务端的443端口进行连接；

2. HTTPS需要使用一套CA数字证书，证书内会附带一个公钥Pub，而与之对应的私钥Private保留在服务端不公开；

3. 服务端收到请求，返回配置好的包含公钥Pub的证书给客户端；

4. 客户端收到证书，校验合法性，主要包括是否在有效期内、证书的域名与请求的域名是否匹配，上一级证书是否有效
   （递归判断，直到判断到系统内置或浏览器配置好的根证书），如果不通过，则显示HTTPS警告信息，如果通过则继续；

5. 客户端生成一个用于对称加密的随机Key，并用证书内的公钥Pub进行加密，发送给服务端；

6. 服务端收到随机Key的密文，使用与公钥Pub配对的私钥Private进行解密，得到客户端真正想发送的随机Key；

7. 服务端使用客户端发送过来的随机Key对要传输的HTTP数据进行对称加密，将密文返回客户端；

8. 客户端使用随机Key对称解密密文，得到HTTP数据明文；

9.  后续HTTPS请求使用之前交换好的随机Key进行对称加解密。

## 5-2：加密算法

1. 对称加密：密钥只有⼀个，加密解密为同⼀个密码，且加解密速度快，典型的对称加密，算法有DES、 AES等；

优点：算法公开、计算量小、加密速度快、加密效率高，适合加密比较大的数据。
缺点：
交易双方需要使用相同的密钥，也就无法避免密钥的传输，而密钥在传输过程中无法保证不被截获，因此对称加密的安全性得不到保证。

每对用户每次使用对称加密算法时，都需要使用其他人不知道的惟一密钥，这会使得发收信双方所拥有的钥匙数量急剧增长，密钥管理成为双方的负担。对称加密算法在分布式网络系统上使用较为困难，主要是因为密钥管理困难，使用成本较高。

2. ⾮对称加密：密钥成对出现（且根据公钥⽆法推知私钥，根据私钥也⽆法推知公钥），加密解密使⽤不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的⾮对称加密算法有RSA、 DSA等。

优点：算法公开，加密和解密使用不同的钥匙，私钥不需要通过网络进行传输，安全性很高。
缺点：计算量比较大，加密和解密速度相比对称加密慢很多。

## 5-3：加密通信过程




# 6.拆包粘包

## 6-1：什么是TCP粘包？怎么解决这个问题

TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，

从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，

例如基于tcp的套接字客户端往服务端上传文件，发送时文件内容是按照一段一段的字节流发送的，

在接收方看了，根本不知道该文件的字节流从何处开始，在何处结束

所谓粘包问题主要还是因为接收方不知道消息之间的界限，不知道一次性提取多少字节的数据所造成的。

## 6-2：粘包的原因

1. 发送方原因

TCP默认使用Nagle算法（主要作用：会将数据量小的，且时间间隔较短的数据一次性发给对方），

而Nagle算法主要做两件事：

只有上一个分组得到确认，才会发送下一个分组

收集多个小分组，在一个确认到来时一起发送

Nagle算法造成了发送方可能会出现粘包问题

2. 接收方原因
   
TCP接收到数据包时，并不会马上交到应用层进行处理，

或者说应用层并不会立即处理。实际上，TCP将接收到的数据包保存在接收缓存里，

然后应用程序主动从缓存读取收到的分组。

这样一来，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，

多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。

## 6-3：粘包解决方案

（1）发送方

对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭算法。

（2）接收方

接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。

（2）应用层

解决办法：循环处理，应用程序从接收缓存中读取分组时，

读完一条数据，就应该循环读取下一条数据，直到所有数据都被处理完成，之后开始处理每条数据的长度

因为每条数据有固定的格式（开始符，结束符），但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。

发送每条数据时，将数据的长度一并发送，

例如规定数据的前4位是数据的长度，应用层在处理时可以根据长度来判断每个分组的开始和结束位置。

## 6-4：UDP会不会产生粘包问题呢？

UDP则是面向消息传输的，是有保护消息边界的，接收方一次只接受一条独立的信息，所以不存在粘包问题。

保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，接收端一次只能接受一条独立的消息

举个例子：有三个数据包，大小分别为2k、4k、6k，如果采用UDP发送的话，不管接受方的接收缓存有多大，我们必须要进行至少三次以上的发送才能把数据包发送完，但是使用TCP协议发送的话，我们只需要接受方的接收缓存有12k的大小，就可以一次把这3个数据包全部发送完毕。

## 6-5：HTTP拆包粘包

一个有报文的请求到服务器时，请求头里都会有content_length，这个指定了报文的大小，

报文如果很大的时候，会通过一部分一部分的发送请求，直到结束，

当这个过程中，出现多个请求，第一个请求会带有请求头信息，前面一个请求的如果发送的报文如果没有满时，

会把后面一个请求的内容填上，这个操作就叫粘包。

这样粘包后，它会通过content_length字段的大小，来做拆包。


# 7.计算机网络-HTTP-Cookie与Session

## 7-1：Cookie 和 Session 的区别

1. 安全性： Session 比 Cookie 安全，Session 是存储在服务器端的，Cookie 是存储在客户端的。

2. 存取值的类型不同：Cookie 只支持存字符串数据，想要设置其他类型的数据，需要将其转换成字符串，Session 可以存任意数据类型。

3. 有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效。

4. 存储大小不同： 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie，但是当访问量过多，会占用过多的服务器资源。

# 8.计算机网络-HTTP-Cookie

## 8-1：Cookie作用

cookie是服务路发送到用户浏览器并保存在本地的小快数据，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器

## 8-2：HTTP是不保存状态的协议,如何保存⽤户状态?

通过Session机制解决，Session的主要作⽤就是通过服务端记录⽤户的状态。

如应用场景购物⻋，当你要添加商品到购物⻋的时候，系统不知道是哪个⽤户操作的，因为HTTP协议是⽆状态的。服务端给特定的⽤户创建特定Session之后就可以标
识这个⽤户并且跟踪这个⽤户了（⼀般情况下，服务器会在⼀定时间内保存这个Session，过了时间限制，就会销毁这个Session）

## 8-3：Cookie 被禁⽤怎么办?

最常⽤的就是利⽤ URL 重写把 Session ID 直接附加在URL路径的后⾯。

## 8-4：cookie如何放置攻击

需要在HTTP头部配上，set-cookie：httponly-。这个属性可以防止XSS,它会禁止javascript脚本来访问cookie。

# 9.计算机网络-HTTP-Session

## 9-1：Session用户登录状态过程

1. 用户进行登录时，用户提交包含用户名和密码的表单，放入HTTP请求报文中，
2. 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis中，它在Redis中Key称为Session ID:
3. 服务器返回的响应报文的Se-Coeo首部字段包含了这个Session ID.客户端收到响应报文之后将该Cookie值存入浏览器中:
4. 客户编之后对间一 个服务器进行请求时会包含该Cookie值，服务器收到之后提取出Session ID.从Redis中取出用户信息，维续之前的业务操作。

## 9-2：如何保存session

在服务端保存Session的⽅法很多，最常⽤的就是内存和数据库(⽐如是使⽤内存数据库redis保存)。

## 9-3：如何实现 Session 跟踪呢？

⼤部分情况下，我们都是通过在Cookie 中附加⼀个 Session ID 来⽅式来跟踪。

## 9-4：Session机制（多个session如何识别）

session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。

当程序需要为某个客户端的请求创建一个session的时候，

服务器首先检查这个客户端的请求里是否已包含了一个session标识 - 称为session id，

如果已包含一个session id则说明以前已经为此客户端创建过session，

服务器就按照session id把这个session检索出来使用（如果检索不到，可能会新建一个），

如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，

session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，

这个session id将被在本次响应中返回给客户端保存。

# 10.计算机网络-HTTP-token

## 10-1：token的验证流程

1. 客户端使用用户名跟密码请求登录

2. 服务端收到请求，去验证用户名与密码

3. 验证成功后，服务端会签发一个 token 并把这个 token 发送给客户端

4. 客户端收到 token 以后，会把它存储起来，比如放在 cookie 里或者 localStorage 里

5. 客户端每次向服务端请求资源的时候需要带着服务端签发的 token

6. 服务端收到请求，然后去验证客户端请求里面带着的 token ，如果验证成功，就向客户端返回请求的数据

## 10-2：token和cookie实现的区别

1. Session 是一种记录服务器和客户端会话状态的机制，使服务端有状态化，可以记录会话信息。而Token 是令牌，访问资源接口（API）时所需要的资源凭证。Token 使服务端无状态化，不会存储会话信息。

2. Token每一个请求都有签名还能防止监听以及重放攻击，而Session就必须依赖链路层来保障通讯安全了。

3. 如果你的用户数据可能需要和第三方共享，或者允许第三方调用 API 接口，用Token 。如果永远只是自己的网站，自己的 App，用什么就无所谓了。

# 11.计算机网络-HTTP-url/uri

## 11-1：URI和URL的区别是什么?

URI的作⽤像身份证号⼀样， URL的作⽤更像家庭住址⼀样。

# 12.计算机网络-综合应用-输入网址

## 12-1：输入网址过程

1. 输入地址,对URL进⾏解析，从⽽⽣成发送给Web服务器的请求信息。
 
2. 浏览器查找域名的IP地址,因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。　

1) 浏览器会首先查看本地硬盘的hosts文件，看看其中有没有和这个域名对应的规则，如果有的话就直接使
   用hosts文件里面的ip地址。

2) 如果在本地的hosts文件没有能够找到对应的ip地址，浏览器会发出一个DNS请求到本地DNS服务器

3) 查询你输入的网址的DNS请求到达本地DNS服务器之后，本地DNS服务器会首先查询它的缓存记录，如果缓
   存中有此条记录，就可以直接返回结果，此过程是递归的方式进行查询。如果没有，本地DNS服务器还要向DNS根服务器进行查询。

4) 根DNS服务器没有记录具体的域名和IP地址的对应关系，而是告诉本地DNS服务器，你可以到域服务器上去
   继续查询，并给出域服务器的地址。这种过程是迭代的过程。

5) 本地DNS服务器继续向域服务器发出请求，比如说请求的对象是.com域服务器。.com域服务器收到请求之
   后，也不会直接返回域名和IP地址的对应关系，而是告诉本地DNS服务器，你的域名的解析服务器的地址

6) 最后，本地DNS服务器向域名的解析服务器发出请求，这时就能收到一个域名和IP地址对应关系，本地DNS
   服务器不仅要把IP地址返回给用户电脑，还要把这个对应关系保存在缓存中，以备下次别的用户查询时，可以直接返回结果，加快网络访问。

3. 浏览器向web服务器发送一个HTTP请求

通过DNS获取到IP后，就可以把HTTP的传输⼯作交给操作系统中的协议栈。

协议栈的内部分为⼏个部分，分别承担不同的⼯作。上下关系是有⼀定的规则的，上⾯的部分会向下⾯的部分委
托⼯作，下⾯的部分收到委托的⼯作并执⾏。

应⽤程序也就是浏览器通过调⽤ Socket 库，来委托协议栈⼯作。

协议栈的上半部分有两块，分别是负责收发数据的 TCP 和 UDP 协议，它们两会接受应⽤层的委托执⾏收发数据的操作。

协议栈的下⼀半是⽤IP协议控制⽹络包收发操作，在互联⽹上传数据时，数据会被切分成⼀块块的⽹络包，⽽将⽹络包发送给对⽅的操作就是由 IP 负责的。

IP 下⾯的⽹卡驱动程序负责控制⽹卡硬件，⽽最下⾯的⽹卡则负责完成实际的收发操作，也就是对⽹线
中的信号执⾏发送和接收操作。

拿到域名对应的IP地址之后，浏览器会以一个随机端口向服务器的WEB程序80端口发起TCP的连接请求。这个连接请求到达服务器端后，进入到网卡，然后是进入到内核的TCP/IP协议栈，还有可能要经过防火墙的过滤，最终到达WEB程序，最终建立了TCP/IP的连接。


4. 服务器的永久重定向响应

服务器给浏览器响应一个301永久重定向响应，这样浏览器就会访问3w了。

5. 浏览器跟踪重定向地址，因为现在浏览器知道了 "http://www.google.com/"才是要访问的正确地址，所以它会发送另一个http请求

6. 服务器处理请求

http请求发送到了服务器，后端从在固定的端口接收到TCP报文开始，它会对TCP连接进行处理，对HTTP协议进行解析，并按照报文格式进一步封装成HTTP Request对象，供上层使用。

 
7. 服务器返回一个HTTP响应　

服务器收到了我们的请求，也处理我们的请求，到这一步，它会把它的处理结果返回，也就是返回一个HTPP响应。

8. 浏览器显示 HTML,并请求获取嵌入在HTML的资源

## 12-2：为什么域名要分级设计

DNS 中的域名都是⽤句点来分隔的，代表了不同层次之间的界限。

域名的层级关系类似⼀个树状结构：
根 DNS 服务器
顶级域 DNS 服务器（com）
权威 DNS 服务器（server.com）

因此，客户端只要能够找到任意⼀台 DNS 服务器，就可以通过它找到根域 DNS 服务器，然后再⼀路顺
藤摸⽠找到位于下层的某台⽬标 DNS 服务器。

## 12-3：重定向原因

1. 网站调整（如改变网页目录结构）；

2. 网页被移到一个新地址；

3. 网页扩展名改变(如应用需要把.php改成.Html或.shtml)。

这种情况下，如果不做重定向，则用户收藏夹或搜索引擎数据库中旧地址只能让访问客户得到一个404页面错误信息，访问流量白白丧失；再者某些注册了多个域名的网站，也需要通过重定向让访问这些域名的用户自动跳转到主站点等。

# 13.Ping

## 13-1：ping

它是用来检查网络是否通畅或者网络连接速度的命令。 

主要是网络上 的机器都有唯一确定的 IP 地址，我们给目标 IP 地址发送一个数据包，对方就要返回一个同 样大小的数据包， 根据返回的数据包我们可以确定目标主机的存在，可以初步判断目标主机 的操作系统等。

## 13-2：工作流程



# 14.各层协议

## 14-1：OSI与TCP/IP各层的结构与功能,都有哪些协议?

1. 应用层

为应用程序提供服务并且规定通信的规范和细节

常见的协议:
* HTTP(超文本传输协议)
* FTP(文件传输协议)
* TELNET(远程登录协议)
* SMTP(简单邮件传输协议)
* DNS(域名解析协议)

6. 表示层

主要负责数据格式的转换

5. 会话层

负责建立和断开通信连接

4. 传输层

是唯一负责总体的数据传输和数据控制的一层。

* TCP: ~~面向连接 ,可靠性强, 传输效率低~~
* UDP: ~~无连接,可靠性弱,传输效率快~~

3.网络层

将数据传输到目标地址；主要负责寻找地址和路由选择，网络层还可以实现拥塞控制、网际互连等功能

* IP
* IPX
* RIP
* OSPF等

2.数据链路层

物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。

* ARP
* RARP
* SDLC
* HDLC
* PPP
* STP
* 帧中继等

1. 物理层

负责0、1比特流(0/1序列)与电压的高低、光的闪灭之间的转换


## 14-2：⽹络层与数据链路层有什么关系呢？

1. IP 的作⽤是主机之间通信⽤的，负责在「没有直连」的两个⽹络之间进⾏通信传输
2. MAC 的作⽤则是实现「直连」的两个设备之间通信。

理解一下：

就比如说，你想从xx村到海南市，你不得做公交车、汽车、火车、轮船到海南

那么你这整个的一个路线图，就是一个网络层，行程开端就是xx村---->>>源IP，目的IP---->行程结束就是海南

那么我从xx村到xx镇相当于是在这个区间内移动路线，也就是数据链路层，其中，xx村好⽐源 MAC 地址，xx镇好⽐⽬的 MAC 地址。
（只要是在线路（网络层包含的都是））

这个xx村、海南不会发生变化，但是中间的位置会一直在变，也就是说

IP源、目的不会变， Mac源、目的会变化

## 14-3：网络层的路由算法

1. 最短路径路由算法

   1. 每个节点用从源节点沿已知最佳路径到该节点的距离来标注，标注分为临时性标注和永久性标注
   2. 初始时，所有节点都为临时性标注，标注为无穷大
   3. 将源节点标注为0，且为永久性标注，并令其为工作节点
   4. 检查与工作节点相邻的临时性节点，若该节点到工作节点的距离与工作节点的标注之和小于该节点的标注，则用新计算得到的和重新标注该节点
   5. 在整个图中查找具有最小值的临时性标注节点，将其变为永久性节点，并成为下一轮检查的工作节点
   6. 重复第四、五步，直到目的节点成为工作节点

2. 泛洪算法

一种将数据包发送到所有网络节点的简单方法,

每个节点通过将其发送到所有其他链接之外来泛洪在传入链接上接收到的新数据包，它属于静态算法

3. 选择性扩散
它是一种泛洪方法的一种改进，将进来的每个数据包仅发送到与正确方向接近的线路上

算法思想
1.每个节点都知道到其邻居的链接的距离
2.每个节点向所有邻居通告已知距离最小的向量
3.每个节点使用接收到的向量来更新自己的向量
4.定期重复

## 14-4：DNS过程

1、在浏览器中输入www.qq.com域名，操作系统会先检查自己本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。 

2、如果hosts里没有这个域名的映射，则查找本地DNS解析器缓存，是否有这个网址映射关系，如果有，直接返回，完成域名解析。 

3、如果hosts与本地DNS解析器缓存都没有相应的网址映射关系，首先会找TCP/ip参数中设置的首选DNS服务器，在此我们叫它本地DNS服务器，此服务器收到查询时，如果要查询的域名，包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析，此解析具有权威性。 

4、如果要查询的域名，不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析，此解析不具有权威性。 

5、如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置（是否设置转发器）进行查询，如果未用转发模式，本地DNS就把请求发至13台根DNS，根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。本地DNS服务器收到IP信息后，将会联系负责.com域的这台服务器。这台负责.com域的服务器收到请求后，如果自己无法解析，它就会找一个管理.com域的下一级DNS服务器地址(qq.com)给本地DNS服务器。当本地DNS服务器收到这个地址后，就会找qq.com域服务器，重复上面的动作，进行查询，直至找到www.qq.com主机。 

6、如果用的是转发模式，此DNS服务器就会把请求转发至上一级DNS服务器，由上一级服务器进行解析，上一级服务器如果不能解析，或找根DNS或把转请求转至上上级，以此循环。不管是本地DNS服务器用是是转发，还是根提示，最后都是把结果返回给本地DNS服务器，由此DNS服务器再返回给客户机。

## 14-5：DNS原理

运行在用户主机上的某些应用程序（如Webl浏览器或者邮件阅读器）需要将主机名转换为IP地址。

这些应用程序将调用DNS的客户机端，并指明需要被转换的主机名。

（在很多基于UNIX的机器上，应用程序为了执行这种转换需要调用函数gethostbyname（））。

用户主机的DNS客户端接收到后，向网络中发送一个DNS查询报文。

所有DNS请求和回答报文使用的UDP数据报经过端口53发送经过若干ms到若干s的延时后，

用户主机上的DNS客户端接收到一个提供所希望映射的DNS回答报文。

这个查询结果则被传递到调用DNS的应用程序。


# 15.TCP的三次握手

## 15-1：TCP三次握手流程

0. 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态

1. 客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序号」字段中，

   同时把 SYN 标志位置为 1 ，表示 SYN 报文。

   接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态。

2. 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn），
   
   将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 
   
   接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态。

3. 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，
   
   首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，
   
   最后把报文发送给服务端，这次报文可以携带客户到服务器的数据，之后客户端处于 ESTABLISHED 状态。

   服务器收到客户端的应答报文后，也进入 ESTABLISHED 状态。

从上面的过程可以发现第三次握手是可以携带数据的，前两次握手是不可以携带数据的，这也是面试常问的题。


## 15-2：TCP为什么要三次握⼿

1. 三次握⼿才可以 阻⽌ 重复 历史连接的初始化（主要原因）
   
  * 比如说，客户端连续发送多次SYN建⽴连接的报⽂，在⽹络拥堵情况下：

    * ⼀个「旧 SYN 报⽂」⽐「最新的 SYN报⽂」早到达了服务端；
    * 那么此时服务端就会回⼀个SYN+ACK报⽂给客户端；
    * 客户端收到后可以根据⾃身的上下⽂，判断这是⼀个历史连接，比如说序列号过期或超时，那么客户端就会发送 RST 报
      ⽂给服务端，表示中⽌这⼀次连接。

  * 如果是两次握⼿连接，就不能判断当前连接是否是历史连接

2. 三次握⼿才可以 同步 双⽅的 初始 序列号
   
   序列号是作为TCP可靠传输的⼀个关键因素，它的作⽤其实是：

   * 接收⽅可以去除重复的数据；
   * 接收⽅可以根据数据包的序列号按序接收；
   * 可以标识发送出去的数据包中，哪些是已经被对⽅收到的；

   - 四次握⼿其实也能够同步双⽅的初始化序号，但由于客户端传输ACK和SYN可以优化成⼀步，所以就成了「三次握⼿」
   - 两次握⼿只保证了⼀⽅的初始序列号能被对⽅成功接收，没办法保证双⽅的初始序列号都能被确认接收。

3. 三次握⼿才可以避免资源浪费
  
  如果只有「两次握⼿」，当客户端的 SYN 请求连接在⽹络中阻塞，客户端没有接收到 ACK 报⽂，重复发送多次 SYN 报⽂，
  那么服务器在收到请求后就会建⽴多个冗余的⽆效链接，造成不必要的资源浪费。
  
## 15-3：TCP为什么SYN

![avatar](http://qd6kny79g.bkt.clouddn.com/02-TCP%E9%9D%A2%E7%BB%8F.jpg)

接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。

## 15-4：TCP除了SYN，为什么还要 ACK

双⽅通信⽆误必须是两者互相发送信息都⽆误。传了 SYN，证明发送⽅到接收⽅的通道没有问题，但是
接收⽅到发送⽅的通道还需要 ACK 信号来进⾏验证。

## 15-5：什么是 SYN 攻击？如何避免 SYN 攻击？

TCP 连接建立是需要三次握手，假设攻击者短时间伪造不同 IP 地址的 SYN 报文，

服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，

但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，

久而久之就会占满服务端的 SYN 接收队列（未连接队列），使得服务器不能为正常用户服务。

1. 避免 SYN 攻击方式一

通过修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理。

当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。控制该队列的最大值

2. 避免 SYN 攻击方式二

正常流程下,

当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「 SYN 队列」；

接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文；

服务端接收到 ACK 报文后，从「 SYN 队列」移除放入到「 Accept 队列」；

应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。

但是

如果应用程序过慢时，就会导致「 Accept 队列」被占满。

如果不断受到 SYN 攻击，就会导致「 SYN 队列」被占满。

当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」；

计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端，

服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」。

最后应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。

## 15-5：如何对三次握手进行性能优化

1. 三次握⼿建⽴连接的⾸要⽬的是「同步序列号」。只有同步了序列号才有可靠传输，所以当客户端发起 SYN 包时，可以通
   过tcp_syn_retries 控制其重传的次数，⽐如内⽹通讯不畅时，就可以适当调低重试次数，尽快把错误暴露给应⽤程序

2. 当服务端SYN半连接队列溢出后，会导致后续连接被丢弃，可以通过backlog等参数来调整 SYN 半连接队列的⼤⼩。

3. TCP Fast Open 功能可以绕过三次握⼿，使得 HTTP 请求减少了 1 个 RTT 的时间，所以也是一种性能优化方案

## 15-6：如何绕过三次握手发送数据

TCP Fast Open 功能可以绕过三次握⼿，使得 HTTP 请求减少了1个RTT的时间， 

第⼀次发起 HTTP GET请求的时候，还是需要正常的三次握⼿流程。

之后发起 HTTP GET请求的时候，可以绕过三次握⼿，这就减少了握⼿带来的 1 个 RTT 的时间消耗。

## 15-7：TCP Fast Open的过程

I、客户端⾸次建⽴连接时的过程：
1. 客户端发送SYN报⽂，该报⽂包含Fast Open选项，且该选项的Cookie为空；

2. ⽀持 TCP Fast Open 的服务器⽣成 Cookie，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；

3. 客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。


II、如果客户端再次向服务器建⽴连接时的过程：

1. 客户端发送 SYN 报⽂，该报⽂包含「数据」以及此前记录的 Cookie；
2. ⽀持 TCP Fast Open 的服务器会对收到 Cookie 进⾏校验：如果 Cookie 有效，服务器将在 SYNACK 报⽂中对 SYN 和
   「数据」进⾏确认，服务器随后将「数据」递送⾄相应的应⽤程序；如果Cookie ⽆效，服务器将丢弃 SYN 报⽂中包含的
   「数据」，且其随后发出的 SYN-ACK 报⽂将只确认 SYN 的对应序列号；
3. 如果服务器接受了 SYN 报⽂中的「数据」，服务器可在握⼿完成之前发送「数据」， 这就减少了握⼿带来的 1 个 RTT 的
   时间消耗；
4. 客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报⽂中发送的「数据」没有被确
   认，则客户端将重新发送「数据」；

## 15-8：为什么需要 TCP 协议？

IP 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。

如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。

因为 TCP 是一个工作在传输层的可靠数据传输的服务，它能确保接收端接收的网络包是无损坏、无间隔、非冗余和按序的。

## 15-9：什么是 TCP 连接？

这个我还真不太清楚，但是我看到过关于传输控制协议的定义

简单来说就是，用于保证可靠性和流量控制维护的某些状态信息，

## 15-10：如何唯一确定一个 TCP 连接呢？

TCP 四元组可以唯一的确定一个连接，四元组包括：

源地址

源端口

目的地址

目的端口

源地址和目的地址的字段是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。

源端口和目的端口的字段（16位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。

## 15-11：有一个 IP 的服务器监听了一个端口，它的 TCP 的最大连接数是多少？

服务器通常固定在某个本地端口上监听，等待客户端的连接请求。

因此，客户端 IP 和 端口是可变的，其理论值计算公式应该是:

客户端的IP数X客户端的端口数

## 15-12：服务端最大并发 TCP 连接数远不能达到理论上限

首先主要是文件描述符限制，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目；

另一个是内存限制，每个 TCP 连接都要占用一定内存，操作系统是有限的。

## 15-15：为什么客户端和服务端的初始序列号 ISN 是不相同的？

因为网络中的报文会延迟、会复制重发、也有可能丢失，

这样会造成的不同连接之间产生互相影响，所以为了避免互相影响，客户端和服务端的初始序列号是随机且不同的。

## 15-16：什么是Mss

除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度；

MTU：一个网络包的最大长度，以太网中一般为 1500 字节；

## 15-17：既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？

当 IP 层有一个超过 MTU 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，

把数据分片成若干片，保证每一个分片都小于 MTU。

把一份 IP 数据报进行分片以后，由目标主机的 IP 层来进行重新组装后，在交给上一层 TCP 传输层。

但这存在隐患的，那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传。

因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。

当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，则不会响应 ACK 给对方，那么发送方的 TCP 在超时后，就会重发「整个 TCP 报文（头部 + 数据）」。

所以 IP 层进行分片传输，是非常没有效率的。

所以，为了达到最佳的传输效率， TCP 协议在建立连接的时候通常要协商双方的 MSS 值，

当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。



# 16.四次挥手

## 16-1：TCP四次挥手流程

客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN 报文，之后客户端进入 FIN_WAIT_1 状态。

服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSED_WAIT 状态。

客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。

等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。

客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态

服务器收到了 ACK 应答报文后，就进入了 CLOSE 状态，至此服务端已经完成连接的关闭。

客户端在经过 2MSL 一段时间后，自动进入 CLOSE 状态，至此客户端也完成连接的关闭。

## 16-2：TCP为什么要四次挥手

四次挥手主要是从FIN过程进行分析：

1. 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。

2. 服务器收到客户端的 FIN 报⽂时，先回⼀个 ACK 应答报⽂，⽽服务端可能还有数据需要处理和发送，等服务端不再发送数
   据时，才发送 FIN 报⽂给客户端来表示同意现在关闭连接。

服务端通常需要等待完成数据的发送和处理，所以服务端的ACK和FIN⼀般都会分开发送，从⽽⽐三次握⼿导致多了⼀次

## 16-3：如何对四次挥手进行优化

1. 主动发起FIN报⽂断开连接的⼀⽅，如果迟迟没收到对⽅的 ACK 回复，则会重传 FIN 报⽂，重传的次数由 
   tcp_orphan_retries 参数决定。

当主动⽅收到 ACK 报⽂后，连接就进⼊ FIN_WAIT2 状态，根据关闭的⽅式不同，优化的⽅式也不同：

* 如果是 close 函数关闭的连接，那么它就是孤⼉连接。如果在系统设置的时间内 （tcp_fin_timeout） 没有收到对⽅的 
  FIN 报⽂，连接就直接关闭。同时，为了应对孤⼉连接占⽤太多的资源， tcp_max_orphans定义了最⼤孤⼉连接的数量，超
  过时连接就会直接释放。

* 反之是 shutdown 函数关闭的连接，则不受此参数限制；
  
2. 当主动⽅接收到 FIN 报⽂，并返回 ACK 后，主动⽅的连接进⼊ TIME_WAIT 状态。为了防⽌ TIME_WAIT 状态占⽤太多的
   资源， tcp_max_tw_buckets 定义了最⼤数量，超过时连接也会直接释放。

3. 被动方关闭的连接，它在回复 ACK 后就进⼊了 CLOSE_WAIT 状态，等待进程调⽤ close函数关闭连接。因此，出现⼤量 
   CLOSE_WAIT 状态的连接时，应当从应⽤程序中找问题。
   
4. 当被动⽅发送 FIN 报⽂后，连接就进⼊ LAST_ACK 状态，在未等到 ACK 时，会在tcp_orphan_retries 参数的控制下重
   发 FIN 报⽂。

## 16-4：为什么TIME_WAIT 等待的时间是 2MSL？

⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包被接收⽅处理后⼜会向对⽅发送响应，所以⼀来⼀回需要等待2 
倍的时间。

## 16-5：为什么需要TIME_WAIT状态？（已经主动关闭连接了为啥还要保持资源一段时间呢？）

1. 防⽌具有相同端口的的「旧」数据包被收到；
   
   比如说服务端在关闭连接之前发送一个报⽂，但是被⽹络延迟了。这时有相同端⼝的TCP连接被复⽤后，被延迟的报文抵达
   了客户端，那么客户端是有可能正常接收这个过期的报⽂，这就会产⽣数据错乱等严重的问题。而使用TIME_WAIT这个时
   间，⾜以让两个⽅向上的数据包都被丢弃

2. 保证连接正确关闭

  比如客户端四次挥⼿的最后⼀个ACK报⽂如果在⽹络中被丢失了，此时如果客户端TIME-WAIT 过短或没有，则就直接进⼊
  了 CLOSED 状态了，那么服务端则会⼀直处在 LASE_ACK状态。当客户端发起建⽴连接的 SYN 请求报⽂后，服务端会发送 
  RST 报⽂给客户端，连接建⽴的过程就会被终⽌。如果有这个时间，就会正常关闭，即使没有收到ACK报文，我也有时间重发
  并关闭

## 16-6：TIME_WAIT 过多有什么危害？

1. 内存资源占⽤；

2. 对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝；被占满就会导致⽆法创建新的连接。

## 16-7：如何优化 TIME_WAIT？

p146页

## 16-8：如果已经建⽴了连接，但是客户端突然出现故障了怎么办？

TCP 有⼀个机制是保活机制。

定义在一个时间段内，如果没有任何连接相关的活动， TCP 保活机制会开始作⽤，每隔⼀个时间间隔，发送⼀个探测报⽂，该

探测报⽂包含的数据⾮常少，如果连续⼏个探测报⽂都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息

通知给上层应⽤程序。

# 17.TCP传输数据优化方案

## 17-1：TCP传输数据优化

TCP 会保证每⼀个报⽂都能够抵达对⽅，报⽂发出去后，必须接收到对⽅返回的确认报⽂ ACK，如果迟迟未收到，就会超时重发该报⽂，直到收到对⽅的 ACK 为⽌。所以， TCP 报⽂发出去后，并不会⽴⻢从内存中删除，
因为重传时还需要⽤到它。这种⽅式的缺点是效率⽐较低的

可以采用并⾏批量发送报⽂，再批量确认报⽂即可，但是当接收⽅硬件不如发送⽅，或者系统繁忙、资源紧张时，是⽆法瞬间处理这么多报⽂的。于是，这些报⽂只能被丢掉，使得⽹络效率⾮常低。

为了解决这种现象发⽣， TCP 提供⼀种机制可以让「发送⽅」根据「接收⽅」的实际接收能⼒控制发送的数据量

因为⽹络的传输能⼒是有限的，当发送⽅依据发送窗⼝，发送超过⽹络处理能⼒的报⽂时，路由器会直接丢弃这些报⽂。影响了传输速度，发送缓冲区的⼤⼩最好是往带宽时延积靠近

# 18.TCP与UDP

## 18-1：TCP与UDP区别

![avatar](http://qd6kny79g.bkt.clouddn.com/03-TCP.jpg)

1. 连接方面来看

TCP 是面向连接的传输层协议，传输数据前先要建立连接。

UDP 是不需要连接，即刻传输数据。

2. 服务对象

TCP 是一对一的两点服务，即一条连接只有两个端点。

UDP 支持一对一、一对多、多对多的交互通信

3. 可靠性

TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。

UDP 是尽最大努力交付，不保证可靠交付数据。

4. 拥塞控制、流量控制

TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。

UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。

5. 首部开销

TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的。

UDP 首部只有 8 个字节，并且是固定不变的，开销较小。

## 18-2：TCP 和 UDP 应用场景

由于TCP是面向连接，能保证数据的可靠性交付，因此经常用于：

  * FTP 文件传输

  * HTTP / HTTPS

接受邮件、远程登录

由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：

  * 包总量较少的通信，如 DNS 、SNMP 等

  * 视频、音频等多媒体通信

  * 广播通信

QQ聊天、在线视频、网络语音电话

# 19.TCP

## 18-3：TCP的作用

TCP 协议的作用是，保证数据通信的完整性和可靠性，防止丢包

因为IP 协议只是一个地址协议，并不保证数据包的完整。如果路由器丢包（比如缓存满了，新进来的数据包就会丢失），

就需要发现丢了哪一个包，以及如何重新发送这个包。这就要依靠 TCP 协议。


## 18-4：TCP 数据包的大小

TCP 负载实际为1400字节左右。

以太网数据包（packet）的大小是固定的，1522字节。其中， 1500 字节是负载（payload），22字节是头信息（head）。

IP 数据包在以太网数据包的负载里面，它也有自己的头信息，最少需要20字节，所以 IP 数据包的负载最多为1480字节。

TCP 数据包在 IP 数据包的负载里面。它的头信息最少也需要20字节，

因此 TCP 数据包的最大负载是 1480 - 20 = 1460 字节。

由于 IP 和 TCP 协议往往有额外的头信息，所以 TCP 负载实际为1400字节左右。

   * 注：可不可以压缩加快速度---》HTTP/2进行了优化


## 18-5：TCP 数据包的编号（SEQ）

一个包1400字节，那么一次性发送大量数据，就必须分成多个包

发送的时候，TCP 协议为每个包编号，以便接收的一方按照顺序还原。万一发生丢包，也可以知道丢失的是哪一个包。

第一个包的编号是一个随机数。假设为1号包。假定这个包的负载长度是100字节，那么可以推算出下一个包的编号应该是101。

每个数据包都可以得到两个编号：自身的编号，以及下一个包的编号。接收方由此知道，应该按照什么顺序将它们还原成原始文件。

## 18-6：TCP 数据包的组装

收到TCP数据包以后，组装还原是操作系统完成的。应用程序不会直接处理 TCP 数据包。

对于应用程序来说，不用关心数据通信的细节。除非线路异常，收到的总是完整的数据。

应用程序需要的数据放在TCP数据包里面，有自己的格式（比如HTTP协议）。

TCP 并没有提供任何机制，表示原始文件的大小，这由应用层的协议来规定。比如，HTTP 协议就有一个头信息Content-Length，表示信息体的大小。对于操作系统来说，就是持续地接收 TCP 数据包，将它们按照顺序组装好，一个包都不少。
操作系统不会去处理 TCP 数据包里面的数据。一旦组装好 TCP 数据包，就把它们转交给应用程序。TCP 数据包里面有一个端口（port）参数，就是用来指定转交给监听该端口的应用程序。

# 20.UDP



# 19.TCP与IP

## 19-1：TCP与IP的区别

IP层接收由更低层（网络接口层例如以太网设备驱动程序）发来的数据包，并把该数据包发送到更高层—TCP层；

IP层也把从TCP接收来的数据包传送到更低层。

我认为TCP和IP的关系是：IP提供基本的数据传送，而高层的TCP对这些数据包做进一步加工，如提供端口号等等。

# 20.协议如何保证可靠传输方式

## 20-1：TCP如何保证稳定传输

1. 确认应答+序列号：TCP给发送的每⼀个包进⾏编号，接收⽅对数据包进⾏排序，把有序数据传送给应⽤层。

2. 校验和：TCP 将保持它⾸部和数据的检验和。⽬的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错， TCP将丢弃这个报⽂段和不确认收到此
   报⽂段。

3. 流量控制：TCP 连接的每⼀⽅都有固定⼤⼩的缓冲空间， TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收⽅来不及处理发送⽅的数据，能
   提示发送⽅降低发送的速率，防⽌包丢失。 TCP 使⽤的流量控制协议是可变⼤⼩的滑动窗⼝协议。 （TCP 利⽤滑动窗⼝实现流量控制）

4. 拥塞控制：当⽹络拥塞时，减少数据的发送。

5. ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完⼀个分组就停⽌发送，等待对⽅确认。在收到确认后再发下⼀个分组。

6. 超时重传： 当 TCP 发出⼀个段后，它启动⼀个定时器，等待⽬的端确认收到这个报⽂段。如果不能及时收到⼀个确认，将重发这个报⽂段。

## 20-2：UDP如何做可靠传输

1、超时重传

2、有序接受

3、应答确认

4、滑动窗口流量控制

# 21.重传机制

## 21-1：常见的重传机制

      1. 超时重传
      2. 快速重传
      3. SACK
      4. D-SACK

## 21-2：超时重传

在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据。

## 21-3：什么时候会发生超时重传

- 数据包丢失
- 确认应答丢失

## 21-4：超时重传存在的问题

如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。这样超时周期可能相对较长

## 21-5：快速重传

它不以时间为驱动，而是以数据驱动重传

比如说，发送方发出了 1，2，3，4，5 份数据：

第一份 Seq1 先送到了，于是就 Ack 回 2；
结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；
后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；
发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。
最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。

所以说，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段

## 21-6：快速重传的问题

快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是重传的时候，是重传之前的一个，还是重传所有的问题。

## 21-7：SACK方法

这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知
道了这些信息，就可以只重传丢失的数据。

比如说发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现只有某段数据丢失，则重发时，就只选择了这个 TCP 段进行重
复。

## 21-8：D-SACK

主要是使用了 SACK 来告诉「发送方」有哪些数据被重复接收了

例子看p157

## 21-9：D-SACK好处

1. 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了;
2. 可以知道是不是「发送方」的数据包被网络延迟了;
3. 可以知道网络中是不是把「发送方」的数据包给复制了;

# 22.滑动窗口与流量控制

## 22-1：引入窗口概念的原因

TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个，效率比较低，如果数据包的往返时间越长，通信的效率就越低

## 22-2：什么是窗口

在往返时间较长的情况下，它也不会降低网络通信的效率，这个就是窗口

窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值

窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据
就可以从缓存区清除。

比如说ACK 100 确认应答报文丢失，也没关系，因为可以通过下一个确认应答进行确认，只要发送方收到了 ACK 200 确认应答，就意味着 200 之前的所有数据「接收方」都收到了。

## 22-3：窗口大小由哪一方决定？

通常窗口的大小是由接收方的窗口大小Window来决定的

这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。

发送方发送的数据大小不能超过接收方的窗�������大小，否则接收方就无法正常接收到数据。

## 22-4：发送方的窗口

一共分为了四部分

1. 是已发送并收到 ACK确认的数据
2. 是已发送但未收到 ACK确认的数据
3. 是未发送但总大小在接收方处理范围内（接收方还有空间）
4. 是未发送但总大小超过接收方处理范围（接收方没有空间）

p160-163未整理

## 22-5：流量控制

如果一直发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。为了解决这种现象发生，TCP 提供一种机制可以「发
送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。

## 22-6：流量控制的过程

p165

## 22-7：

## 22-8：


## 22-9：TCP 是如何解决窗口关闭时，潜在的死锁现象呢？

为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。

如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。

如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；

如果接收窗口不是 0，那么死锁的局面就可以被打破了。

# 23.拥塞控制

## 23-1：为什么要有拥塞控制呀，不是有流量控制了吗？

流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络的中发生了什么

在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更
大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大

为了避免「发送方」的数据填满整个网络，有了拥塞控制


## 23-2：什么是拥塞控制

在某段时间，若对⽹络中某⼀资源的需求超过了该资源所能提供的可⽤部分，⽹络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防⽌过多的数据注⼊到⽹络中，这样就可以使⽹络中的路由器或链路不致过载。拥塞控制所要做的都有⼀个前提，就是⽹络能够承受现有的⽹络负荷。

## 23-3：什么是拥塞窗口？和发送窗口有什么关系呢？

拥塞窗口是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。

发送窗口和接收窗口是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是是拥塞窗口和接收窗口中的最小值。

拥塞窗口 cwnd 变化的规则：

只要网络中没有出现拥塞， cwnd 就会增大；

但网络中出现了拥塞， cwnd 就减少；

## 23-4：那么怎么知道当前网络是否出现了拥塞呢？

只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了用拥塞。

## 23-5：拥塞控制算法

TCP的拥塞控制采⽤了四种算法：

- 慢启动
- 拥塞避免
- 拥塞发生
- 快速恢复

1. 慢开始： 当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1
2. 拥塞避免： 每当收到一个 ACK 时，cwnd 增加 1/cwnd.
3. 拥塞发生算法：
4. 快重传与快恢复： 它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了， TCP 将会使⽤定时器来要求传输暂停。在暂停的这段时间内，没有新的或复
                   制的数据包被发送。有了 FRR，如果接收机接收到⼀个不按顺序的数据段，它会⽴即给发送机发送⼀个重复确认。如果发送机接收到三个重
                   复确认，它会假定确认件指出的数据段丢失了，并⽴即重传这些丢失的数据段。有了FRR，就不会因为重传时要求的暂停被耽误。 　当有单
                   独的数据包丢失时，快速重传和恢复（FRR）能最有效地⼯作。当有多个数据信息包在某⼀段很短的时间内丢失时，它则不能很有效地⼯作。

## 23-6：那慢启动涨到什么时候是个头呢？

有一个叫慢启动门限 ssthresh （slow start threshold）状态变量。

- 当 cwnd < ssthresh 时，使用慢启动算法。
- 当 cwnd >= ssthresh 时，就会使用「拥塞避免算法」。

## 23-7：重传机制何时结束

当触发了重传机制，也就进入了「拥塞发生算法」。

# 24.ARQ协议

## 24-1：什么是ARQ协议

ARQ协议是⾃动重传请求，他是OSI模型中数据链路层和传输层的错误纠正协议之⼀。它通过使⽤确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送⽅在发送后⼀段时间之内没有收到确认帧，它通常会重新发送。 ARQ包括停⽌等待ARQ协议和连续ARQ协议。

## 24-2：什么是停⽌等待ARQ协议
停⽌等待协议是为了实现可靠传输的，它的基本原理就是每发完⼀个分组就停⽌发送，等待对⽅确认（回复ACK）。如果过了⼀段时间（超时时间后），还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，直到收到确认后再发下⼀个分组；在停⽌等待协议中，若接收⽅收到重复分组，就丢弃该分组，但同时还要发送确认；

1. 优点： 简单
2. 缺点： 信道利⽤率低，等待时间⻓

## 24-3: 什么是连续ARQ协议

连续 ARQ 协议可提⾼信道利⽤率。发送⽅维持⼀个发送窗⼝，凡位于发送窗⼝内的分组可以连续发送出去，⽽不需要等待对⽅确认。接收⽅⼀般采⽤累计确认，对按序到达的最后⼀个分组发送确认，表明到这个分组为⽌的所有分组都已经正确收到了。

1. 优点： 信道利⽤率⾼，容易实现，即使确认丢失，也不必重传。
2. 缺点： 不能向发送⽅反映出接收⽅已经正确收到的所有分组的信息。 
   * ⽐如：发送⽅发送了5条消息，中间第三条丢失（3号），这时接收⽅只能对前两个发送确认。发送⽅⽆法知道后三个分组的下落，⽽只好把后三个全部重传⼀次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的N 个消息。

# 25.Socket

## 25-1： TCP 应该如何 Socket 编程？

服务端和客户端初始化 socket，得到文件描述符；

服务端调用 bind，将绑定在 IP 地址和端口;

服务端调用 listen，进行监听；

服务端调用 accept，等待客户端连接；

客户端调用 connect，向服务器端的地址和端口发起连接请求；

服务端 accept 返回用于传输的 socket 的文件描述符；

客户端调用 write 写入数据；服务端调用 read 读取数据；

客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。


# ------操作系统--------------------------------------------

# 1. 内存管理-虚拟内存

## 1-1：什么是虚拟地址

我们可以把进程所使用的地址「隔离」开来，
让操作系统为每个进程分配独立的一套「虚拟地址」，
自己使用自己的地址就行，互不干涉。

## 1-2：操作系统是如何管理虚拟地址与物理地址之间的关系？

主要有两种方式，分别是

1. 内存分段
2. 内存分页

# 2.内存管理-内存分段

## 2-1：什么是内存分段

由于程序是由若干个逻辑分段组成的，
比如说是由代码分段、数据分段、栈段、堆段组成。
不同的段是有不同的属性的，
所以就用分段（Segmentation）的形式把这些段分离出来。

## 2-2：分段机制下，虚拟地址和物理地址是如何映射的？

分段机制下的虚拟地址由两部分组成，段选择子和段内偏移量

段选择子就保存在段寄存器里面。段选择子里面最重要的是段号，

用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。

虚拟地址中的段内偏移量应该位于 0 和段界限之间，

如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

也就是说虚拟地址是通过段表与物理地址进行映射的，

分段机制会把程序的虚拟地址分成 4 个段，

每个段在段表中有一个项，

在这一项找到段的基地址，

再加上偏移量，于是就能找到物理内存中的地址

## 2-3：访问某段偏移量xxx的虚拟地址

如访问段 3 中偏移量 500 的虚拟地址，

我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。

## 2-4：内存分段缺陷

1. 内存碎片的问题。

   比如说有 1G 的物理内存，用户执行了多个程序，其中：
         QQ占用了512MB内存，浏览器占用了128MB内存，音乐占用了256 MB内存。
这个时候，如果我们关闭了浏览器，则空闲内存还有256MB。
如果这个256MB不是连续的，被分成了两段128MB内存，这就会导致没有空间再打开一个200MB的程序。
这样就会在两处产生内存碎片问题：
    1）外部内存碎片，也就是产生了多个不连续的小物理内存，
                     导致新的程序无法被装载；
    2）内部内存碎片，程序所有的内存都被装载到了物理内存，
                     但是这个程序有部分的内存可能并不是很常使用
                     这也会导致内存的浪费；

2. 内存交换的效率低的问题

   对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，
   产生了内存碎片，那不得不重新 Swap 内存区域，这个过程会产生性能瓶颈。
   因为硬盘的访问速度要比内存慢太多了，每一次内存交换，
   我们都需要把一大段连续的内存数据写到硬盘上。
   所以，如果内存交换的时候，
   交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。




## 2-5：如何解决内存分段的缺陷

1. 解决外部内存碎片的问题就是内存交换。

   比如说可以把音乐程序占用的那256MB内存写到硬盘上，
   然后再从硬盘上读回来到内存里。
   不过再读回的时候，我们不能装载回原来的位置，
   而是紧紧跟着那已经被占用了的 512MB 内存后面。
   这样就能空缺出连续的 256MB 空间，
   于是新的 200MB 程序就可以装载进来。

2. 使用内存分页来解决

# 3. 内存管理-内存分页

## 3-1：为什么有内存分页（内存分页定义）

虽然分段的好处就是能产生连续的内存空间，
但是会出现内存碎片和内存交换的空间太大的问题。
要解决这些问题，那么就要想出能少出现一些内存碎片的办法。
另外，当需要进行内存交换的时候，
让需要交换写入或者从磁盘装载的数据更少一点，
这样就可以解决问题了。这个办法，也就是内存分页（Paging）。
分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。
这样一个连续并且尺寸固定的内存空间，这个也就是页。
在 Linux 下，每一页的大小为 4KB。虚拟地址与物理地址之间通过页表来映射

页表实际上存储在 CPU 的内存管理单元（MMU）中，
于是 CPU 就可以直接通过 MMU，找出要实际要访问的物理内存地址。
而当进程访问的虚拟地址在页表中查不到时，
系统会产生一个缺页异常，
进入系统内核空间分配物理内存、更新进程页表，
最后再返回用户空间，恢复进程的运行。

## 3-2：分页是怎么解决分段的内存碎片、内存交换效率低的问题？

因为采用了分页，那么释放的内存都是以页为单位释放的，
也就不会产生无法给进程使用的小内存。
如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」
的内存页面给释放掉，也就是暂时写在硬盘上，也是换出（Swap Out）。
一旦需要的时候，再加载进来，称为换入（Swap In）。
所以，一次性写入磁盘的也只有少数的一个页或者几个页，
不会花太多时间，内存交换的效率就相对比较高。
分页的方式使得我们在加载程序的时候，
不再需要一次性都把程序加载到物理内存中。
我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，
并不真的把页加载到物理内存里，而是只有在程序运行中，
需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。

## 3-3：分页机制下，虚拟地址和物理地址是如何映射的？

在分页机制下，虚拟地址分为两部分，
页号和页内偏移。页号作为页表的索引，
页表包含物理页每页所在物理内存的基地址，
这个基地址与页内偏移的组合就形成了物理内存地址

对于一个内存地址转换，其实就是这样三个步骤：

1. 第一步把虚拟内存地址，切分成页号和偏移量；

2. 第二步根据页号，从页表里面，查询对应的物理页号；

3. 第三步直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

## 3-4：简单的分页有什么缺陷吗？

有空间上的缺陷。

因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。
比如说在32位的环境下，虚拟地址空间共有4GB，如果说一个页的大小是4KB（2^12），
那么就需要大约几百万 （2^20）个页，
每个「页表项」需要 4 个字节大小来存储，
那么整个 4GB 空间的映射就需要有 4MB 的内存来存储页表。
这 4MB 大小的页表，看起来也不是很大。
但是因为每个进程都是有自己的虚拟地址空间的，也是有自己的页表的。
那么，100 个进程的话，就需要 400MB 的内存来存储页表，
这是非常大的内存了，

## 3-5：简单的分页缺陷的解决方案

要解决简单的分页问题，就需要采用多级页表（Multi-Level Page Table）的解决方案。

因为对于单页表的实现方式，在32位和页大小4KB的环境下，
一个进程的页表需要装下100多万个「页表项」，
并且每个页表项是占用4字节大小的，
于是相当于每个页表需占用4MB大小的空间。
我们把这个100多万个「页表项」的单级页表再分页，
将页表（一级页表）分为 1024 个页表（二级页表），
每个表（二级页表）中包含 1024 个「页表项」，形成二级分页。

## 3-6：分了二级表，内存不是变大了呢

每个进程都有 4GB 的虚拟地址空间，
而显然对于大多数程序来说，其使用到的空间远未达到 4GB，
因为会存在部分对应的页表项都是空的，根本没有分配，
对于已分配的页表项，如果存在最近一定时间未访问的页表，
在物理内存紧张的情况下，操作系统会将页面换出到硬盘，
也就是说不会占用物理内存。
如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，
但如果某个一级页表的页表项没有被用到，
也就不需要创建这个页表项对应的二级页表了，
也就是可以在需要时才创建二级页表。

## 3-7：为什么不分级的页表就做不到这样节约内存呢？

因为保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。
假如虚拟地址在页表中找不到对应的页表项，
计算机系统就不能工作了。
所以页表一定要覆盖全部虚拟地址空间，
而不分级的页表就需要有 100 多万个页表项来映射，
而二级分页则只需要1024个页表项
（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。
我们可以把二级分页再推广到多级页表，
就会发现页表占用的内存空间更少了，
这样其实对于64位的系统来说，两级分页肯定不够了，就变成了四级目录，

1. 全局页目录项 PGD（Page Global Directory）；
2. 上层页目录项 PUD（Page Upper Directory）；
3. 中间页目录项 PMD（Page Middle Directory）；
4. 页表项 PTE（Page Table Entry）；

## 3-8：多级页表的缺陷以及解决方案

多级页表虽然解决了空间上的问题，
但是虚拟地址到物理地址的转换就多了几道转换的工序，
这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。
程序是有局部性的，即在一段时间内，
整个程序的执行仅限于程序中的某一部分。
相应地，执行所访问的存储空间也局限于某个内存区域。
然后，在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，
这个 Cache 就是 TLB（Translation Lookaside Buffer） ，
通常称为页表缓存、转址旁路缓存、快表等
在 CPU 芯片里面，封装了内存管理单元（Memory Management Unit）芯片，
它用来完成地址转换和 TLB 的访问与交互。
有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。
TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。

# 4.内存管理-段页式内存管理

## 4-1：什么是段页式内存管理

因为内存分段和内存分页并不是对立的，
它们是可以组合起来在同一个系统中使用的，
那么组合起来后，通常称为段页式内存管理

## 4-2：段页式内存管理实现的方式

先将程序划分为多个有逻辑意义的段
接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；
这样，地址结构就由段号、段内页号和页内位移三部分组成。
用于段页式地址变换的数据结构是每一个程序一张段表，
每个段又建立一张页表，段表中的地址是页表的起始地址，
而页表中的地址则为某页的物理页号
段页式地址变换中要得到物理地址须经过三次内存访问：
第一次访问段表，得到页表起始地址；
第二次访问页表，得到物理页号；
第三次将物理页号与页内位移组合，得到物理地址。

# 5. 内存管理-linux内存管理

## 5-1：Linux 操作系统采用了哪种方式来管理内存

Linux 内存主要采用的是页式内存管理，
但同时也不可避免地涉及了段机制

Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），
也就是所有的段的起始地址都是一样的。
在Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，
所面对的地址空间都是线性地址空间（虚拟地址），
这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护

## 5-2：Linux 的虚拟地址空间是如何分布的？

在 Linux 操作系统中，
虚拟地址空间的内部又被分为内核空间和用户空间两部分，

## 5-3：内核空间与用户空间的区别

1. 进程在用户态时，只能访问用户空间内存；

2. 只有进入内核态后，才可以访问内核空间的内存；

虽然每个进程都各自有独立的虚拟内存，
但是每个虚拟内存中的内核地址，
其实关联的都是相同的物理内存。
这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

## 5-4：用户空间分布

有 7 种不同的内存段：

1. 程序文件段，包括二进制可执行代码；

2. 已初始化数据段，包括静态常量；

3. 未初始化数据段，包括未初始化的静态变量；

4. 堆段，包括动态分配的内存，从低地址开始向上增长；

5. 文件映射段，包括动态库、共享内存等，
   从低地址开始向上增长（跟硬件和内核版本有关）

6. 栈段，包括局部变量和函数调用的上下文等。
   栈的大小是固定的，一般是 8 MB。
   当然系统也提供了参数，以便我们自定义大小；

# 6.内存管理-页面置换算法

## 6-1：页面置换算法

1. OPT ⻚⾯置换算法（最佳⻚⾯置换算法） ：
   
   最佳(Optimal, OPT)置换算法所选择的被淘汰⻚⾯将是以后永不使⽤的，
   或者是在最⻓时间内不再被访问的⻚⾯,这样可以保证获得最低的缺⻚率。
   但由于⼈们⽬前⽆法预知进程在内存下的若千⻚⾯中哪个是未来最⻓时间内不再被访问的，
   因⽽该算法⽆法实现。⼀般作为衡量其他置换算法的⽅法。

2. FIFO（First In First Out） ⻚⾯置换算法（先进先出⻚⾯置换算法） : 
   
   总是淘汰最先进⼊内存的⻚⾯，即选择在内存中驻留时间最久的⻚⾯进⾏淘汰。

3. LRU （Least Currently Used）⻚⾯置换算法（最近最久未使⽤⻚⾯置换算法） ： 
   
   LRU算法赋予每个⻚⾯⼀个访问字段，⽤来记录⼀个⻚⾯⾃上次被访问以来所经历的时间 T，
   当须淘汰⼀个⻚⾯时，选择现有⻚⾯中其 T 值最⼤的，即最近最久未使⽤的⻚⾯予以淘汰。

4. LFU （Least Frequently Used）⻚⾯置换算法（最少使⽤⻚⾯置换算法） : 
   
   该置换算法选择在之前时期使⽤最少的⻚⾯作为淘汰⻚。

## 6-2：⻚⾯置换算法的作⽤?

地址映射过程中，若在⻚⾯中发现所要访问的⻚⾯不在内存中，则发⽣缺⻚中断。
缺⻚中断 就是要访问的⻚不在主存，需要操作系统将其调⼊主存后再进⾏访问。 在这个时候，被
内存映射的⽂件实际上成了⼀个分⻚交换⽂件。
当发⽣缺⻚中断时，如果当前内存中并没有空闲的⻚⾯，操作系统就必须在内存选择⼀个⻚⾯将其移出
内存，以便为即将调⼊的⻚⾯让出空间。⽤来选择淘汰哪⼀⻚的规则叫做⻚⾯置换算法，我们可以把⻚
⾯置换算法看成是淘汰⻚⾯的规则。

## 6-3：手写LRU缓存

主要思路就是

1.可以使用最基础的单向链表处理
2.使用双向链表,可以加入hash表做优化
3.最简单的实现是使用JDK中自带的LinkedHashMap,
  需要重写removeEldestEntry()方法,
  这是LinkedHashMap提供的一个删除最老条目的方法;

```java
class LRUNode{
    private String key;
    private String value;
    private LRUNode pre;
    private LRUNode next;

    public LRUNode(String key, String value) {
        this.key = key;
        this.value = value;
    }
}


private volatile LRUNode head;
private volatile Integer lenght = 0;

public LRUcache(Integer lenght){
    this.lenght = lenght;
}

public  boolean put(String key,String value){

    LRUNode cur = this.head;
    Integer curLenght = 1;
    while (null != cur && cur.next != null){
        curLenght++;
        if(cur.key.equals(key)){
            //包含 修改其前后node的指向
            cur.pre.next = cur.next;
            cur.next.pre = cur.pre;

            cur.pre = null;
            cur.next = head;
            head.pre = cur;
            return true;
        }else {
            cur = cur.next;
        }
    }
    //没有找到
    LRUNode newNode = new LRUNode(key,value);
    if(curLenght >= lenght){
        //删除最后一个对象
        cur.pre.next = null;
        cur.pre = null;
        newNode.next = head;
        head.pre = newNode;

    }else{
        if(head != null){
            head.pre = newNode;
            newNode.next = head;
        }
    }
    head = newNode;
    return true;
}

public LRUNode get(String key){
    if(key.isEmpty()){
        throw new RuntimeException("key not is find");
    }
    LRUNode cur = head;
    while (lenght != 0 && cur != null){
        if(cur.key.equals(key)){
            if(null != cur.pre && null != cur.next){
                cur.pre.next = cur.next;
                cur.next.pre = cur.pre;

            }else if(null != cur.pre){
                cur.pre.next = null;
            }

            cur.pre = null;
            cur.next = head;
            head.pre = cur;
            head = cur;
            return cur;
        }
        cur = cur.next;
    }
    throw new NullPointerException("this key not find");
}
```
# 7.进程调度算法

1. 先来先服务调度算法

2. 最短作业优先调度算法

3. 高响应比优先调度算法

4. 时间片轮转调度算法

5. 最高优先级调度算法

6. 多级反馈队列调度算法


1. 非抢占式的先来先服务（First Come First Severd, FCFS）算法

先来后到，每次从就绪队列选择最先进入队列的进程，

然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。

这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。

FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。

2. 最短作业优先（Shortest Job First, SJF）调度算法 
   
     它会优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。

      这显然对长作业不利，很容易造成一种极端现象。

     比如，一个长作业在就绪队列等待运行，
     
     而这个就绪队列有非常多的短作业，
     
     那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。

3. 高响应比优先 （Highest Response Ratio Next, HRRN）调度算法
   
   主要是权衡了短作业和长作业。

   每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，
   
   「响应比优先级」的计算公式：
    
    优先权=（等待时间+要求服务时间）/要求服务时间

    - 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；
    - 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，
      这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，
      当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；

4. 时间片轮转（Round Robin, RR）调度算法。

每个进程被分配一个时间段，称为时间片（Quantum），即允许该进程在该时间段中运行。

如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；

如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；

另外，时间片的长度就是一个很关键的点：

- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
- 如果设得太长又可能引起对短作业进程的响应时间变长；

注：通常时间片设为 20ms~50ms 通常是一个比较合理的折中值。

5. 最高优先级调度算法

希望调度程序能从就绪队列中选择最高优先级的进程进行运行，

进程的优先级可以分为，静态优先级或动态优先级：

- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
- 动态优先级：根据进程的动态变化调整优先级，
  
  比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，
  
  则升高其优先级，也就是随着时间的推移增加等待进程的优先级。该算法也有两种处理优先级高的方法，非抢占式和抢占式：

非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。

抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。
        但是依然有缺点，可能会导致低优先级的进程永远不会运行。

6. 多级反馈队列（Multilevel Feedback Queue）调度算法

「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。

「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从高到低，同时优先级越高时间片越短；

新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，

如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；

当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。

如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，

接着让较高优先级的进程运行；可以发现，对于短作业可能可以在第一级队列很快被处理完。

对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，

虽然等待的时间变长了，但是运行时间也会更长了，所以该算法很好的兼顾了长短作业，同时有较好的响应时间。


# 8.磁盘调度算法

1. 先来先服务算法

2. 最短寻道时间优先算法

3. 扫描算法算法

4. 循环扫描算法

5. LOOK 与 C-LOOK 算法

2. 





# 7.文件系统组成

## 7-1：什么是文件系统

文件系统是操作系统中负责管理持久数据的子系统，
就是负责把用户的文件存到磁盘硬件中，
因为即使计算机断电了，磁盘里的数据并不会丢失，所以可以持久化的保存文件。


# 8.虚拟文件系统


# 9.文件的使用


# 10.文件的存储

# 11.空闲空间管理


# 12.文件系统的结构

# 13.目录的存储


# 14.软链接和硬链接

# 1. 虚拟内存

# 2.页面置换算法

1. 最优页面置换算法
当一个缺页中断发生时，对于保存在内存中的每一个逻辑页面，计算在它的下一次访问之前，还需要等待多长时间，从中选择时间最长的那个，作为被置换的页面
这是一种理想的页面置换算法，在实际系统中是无法实现的，因为操作系统无从知道每一个页面要等待多长时间以后才会被访问
可以作为其它算法性能评价的依据

2. 先进先出页面置换算法(First-In First-Out，FIFO)
选择在内存中驻留时间最长的页面并淘汰它
OS维护一个链表，记录了所有页面位于内存当中的逻辑页面，从链表的排列顺序来看，链首页面的驻留时间最长，页尾页面的驻留时间最短，当发生一个缺页中断时，把链首页面淘汰出去，并把新的页面添加到链表的末尾

3. 最近最少使用算法(Least Recently Used，LRU)
当一个缺页中断发生时，选择最久未使用的那个页面，并淘汰它
此算法是最优页面置换算的一个近似，其依据是程序的局部性原理，即在最近一小段时间内，如果某些页面被频繁的���问，那么在将来的一段时间内，它们还可能会再次被频繁的访问；反之，如果在过去某些页面长时间未被访问，那么在将来他们还可能会长时间得不到访问

4. 时钟页面置换算法
需要用到页表项当中的访问位，当一个页面被装入内存中时，把该位初始化为0，然后如果这个页面被访问(读/写)，则把该位置1
把各个页面组织成环形链表(类似钟表面)，把指针指向最老的页面(最先进来的)
当发生一个缺页中断时，考察指针所指向的最老页面，若它的访问位为0，立即淘汰；若访问位为1，则把该位置0，然后指针往下移动一个，如此下去，直到找到被淘汰的页面，然后把指针移动到它的下一个

5. 二次机会算法
此方法与时钟页面置换算法有些类似，只是二次机会算法考察的是页表项中的 access 和 dirty 两个位
还是将各个页面组织成环形链表，当发生缺页中断时，考察 access 和 dirty两个位，进行第一轮扫描若找到两个位都是0的页，直接淘汰；第一轮没有淘汰页，第二轮扫描 (access == 0 && dirty == 1) 的页，找到直接淘汰掉；第二轮扫描没有淘汰页，第三轮扫描将 access 位全部置0，再进行前两轮扫描

6. 最不经常用算法(Least Frequently Used，LFU)
当缺页中断发生时，选择访问次数最少的那个页面，并淘汰之


## 2-2：LRU 使用Java 中哪种数据结构来实现

# 3.为什么要设置用户态与核心态



