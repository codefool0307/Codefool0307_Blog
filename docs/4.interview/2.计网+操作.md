<!-- TOC -->

- [56.各层协议](#56各层协议)
  - [56.1 OSI与TCP/IP各层的结构与功能,都有哪些协议?](#561-osi与tcpip各层的结构与功能都有哪些协议)
  - [56.2 ⽹络层与数据链路层有什么关系呢？](#562-络层与数据链路层有什么关系呢)
  - [56.3 网络层的路由算法](#563-网络层的路由算法)
  - [56.4 各种协议的端口号](#564-各种协议的端口号)
  - [56.5 四组元/五元组/七元组](#565-四组元五元组七元组)
- [57.Get与Post](#57get与post)
  - [57.1 get与post的区别](#571-get与post的区别)
  - [57.2 幕等](#572-幕等)
    - [57.2.1 概念](#5721-概念)
  - [57.3 为什么都用get而不用post？](#573-为什么都用get而不用post)
  - [57.4 get与post的应用场景](#574-get与post的应用场景)
  - [57.2 后台获取前端数据方法](#572-后台获取前端数据方法)
- [58.状态码](#58状态码)
  - [58.1 状态码种类](#581-状态码种类)
- [59.报文结构](#59报文结构)
  - [59.1 报文组成](#591-报文组成)
- [60.HTTP](#60http)
  - [60.1 HTTP常用字段](#601-http常用字段)
    - [60.1.1 Host字段](#6011-host字段)
    - [60.1.2 Content-Length字段](#6012-content-length字段)
    - [60.1.3 Connection字段](#6013-connection字段)
    - [60.1.4 Content-Type字段](#6014-content-type字段)
    - [60.1.15 Content-Encoding字段](#60115-content-encoding字段)
  - [60.2 HTTP的工作流程](#602-http的工作流程)
    - [60.2.1 一个 TCP 连接后是否会在一个HTTP请求完成后断开？什么情况下会断开？](#6021-一个-tcp-连接后是否会在一个http请求完成后断开什么情况下会断开)
    - [60.2.2 HTTP慢的原因和解决方式](#6022-http慢的原因和解决方式)
      - [60.2.2.1 HTTP慢的原因](#60221-http慢的原因)
      - [60.2.2.2 如何解决HTTP慢](#60222-如何解决http慢)
  - [60.3 HTTP存在的风险/问题](#603-http存在的风险问题)
  - [60.4 分类](#604-分类)
  - [60.4 持久连接与非持久连接（长连接与短连接）](#604-持久连接与非持久连接长连接与短连接)
    - [60.4.1 HTTP哪些是持久连接？](#6041-http哪些是持久连接)
    - [60.4.2 非持久链接](#6042-非持久链接)
      - [60.4.2.1 如何使用](#60421-如何使用)
      - [60.4.2.2 如何改善时间延时问题](#60422-如何改善时间延时问题)
      - [60.4.2.3 缺点](#60423-缺点)
    - [60.4.3 持久连接](#6043-持久连接)
    - [60.4.4 参数](#6044-参数)
      - [60.4.4.1 TCP的keep alive和HTTP的Keep-alive区别](#60441-tcp的keep-alive和http的keep-alive区别)
  - [60.2 HTTP 1.0](#602-http-10)
    - [60.2.1 HTTP 1.0优缺点](#6021-http-10优缺点)
      - [60.2.1.1 HTTP为什么是无连接和无状态的](#60211-http为什么是无连接和无状态的)
    - [60.2.2 无状态的解决方案](#6022-无状态的解决方案)
  - [60.3 HTTP 1.1](#603-http-11)
    - [60.3.1 HTTP 1.1相对于HTTP 1.0改善](#6031-http-11相对于http-10改善)
    - [60.3.2 HTTP 1.1缺点](#6032-http-11缺点)
    - [60.3.3 HTTP 1.1优化方案](#6033-http-11优化方案)
  - [60.4 HTTP 2](#604-http-2)
    - [60.4.1 HTTP 2如何兼容HTTP 1.1](#6041-http-2如何兼容http-11)
    - [60.4.2 HTTP 2优势](#6042-http-2优势)
      - [60.4.2.1 头部压缩](#60421-头部压缩)
        - [60.4.2.1.1 头部压缩方案](#604211-头部压缩方案)
        - [60.4.2.1.2 静态表编码](#604212-静态表编码)
        - [60.4.2.1.3 动态表编码](#604213-动态表编码)
      - [60.4.2.2 二进制帧](#60422-二进制帧)
        - [60.4.2.2.1 概念](#604221-概念)
        - [60.4.2.2.2](#604222)
      - [60.4.2.3 并发传输](#60423-并发传输)
        - [60.4.2.3.1 并发传输如何实现的](#604231-并发传输如何实现的)
      - [60.4.2.4 服务器主动推送资源](#60424-服务器主动推送资源)
        - [60.4.2.4.1 http 2.0 主动推送如何实现的](#604241-http-20-主动推送如何实现的)
    - [60.4.3 HTTP 2有哪些缺陷](#6043-http-2有哪些缺陷)
    - [60.4.4 HTTP 2做了什么优化](#6044-http-2做了什么优化)
  - [60.5 HTTP-3](#605-http-3)
    - [60.5.1 QUIC协议](#6051-quic协议)
      - [60.5.1.1 特点](#60511-特点)
    - [60.5.1 HTTP 3做了哪些优化](#6051-http-3做了哪些优化)
      - [60.5.1.1 如果HPACK头部首次传递丢包，如何解决](#60511-如果hpack头部首次传递丢包如何解决)
- [61.HTTPS](#61https)
  - [61.1 什么是HTTPS](#611-什么是https)
  - [61.2 HTTP与HTTPS区别](#612-http与https区别)
    - [61.2.1 为何不所有的网站都使用HTTPS](#6121-为何不所有的网站都使用https)
  - [61.3 HTTPS 解决了 HTTP 的哪些问题？（为什么要HTTPS）](#613-https-解决了-http-的哪些问题为什么要https)
  - [61.4 加密算法](#614-加密算法)
    - [61.4.1 分类](#6141-分类)
    - [61.4.2 Https对称加解密的过程](#6142-https对称加解密的过程)
    - [61.4.3 Https非对称加密过程](#6143-https非对称加密过程)
  - [61.5 摘要算法](#615-摘要算法)
  - [61.6 数字证书](#616-数字证书)
    - [61.6.1 数字证书包含什么](#6161-数字证书包含什么)
    - [61.6.2 作用](#6162-作用)
    - [61.6.3 证书怎么来的](#6163-证书怎么来的)
    - [61.6.4 数字证书的流程](#6164-数字证书的流程)
    - [61.6.5 证书链](#6165-证书链)
      - [61.6.5.1 为什么要有证书链](#61651-为什么要有证书链)
  - [61.7 SSL/TLS协议](#617-ssltls协议)
    - [61.7.1 工作流程](#6171-工作流程)
    - [61.7.2 四次握手分类](#6172-四次握手分类)
    - [61.7.3 RSA的四次握手](#6173-rsa的四次握手)
      - [61.7.3.1 四次握手流程](#61731-四次握手流程)
      - [61.7.3.2 缺点](#61732-缺点)
    - [61.7.4 ECDHE的四次握手](#6174-ecdhe的四次握手)
      - [61.7.4.1 四次握手流程](#61741-四次握手流程)
      - [61.7.5.2 ECDHE 算法](#61752-ecdhe-算法)
    - [61.7.5 RSA和ECDHE握⼿过程的区别](#6175-rsa和ecdhe握过程的区别)
  - [61.8 https性能消耗](#618-https性能消耗)
    - [61.8.1 https在哪里有性能损耗](#6181-https在哪里有性能损耗)
  - [61.9 HTTPS优化](#619-https优化)
- [62.拆包粘包](#62拆包粘包)
  - [62.1 粘包](#621-粘包)
    - [62.1.1 概念](#6211-概念)
    - [62.1.2 粘包的原因](#6212-粘包的原因)
    - [62.1.3 粘包解决方案](#6213-粘包解决方案)
    - [621.1.4 UDP会不会产生粘包问题呢](#62114-udp会不会产生粘包问题呢)
  - [62.2 拆包](#622-拆包)
  - [62.3 HTTP拆包粘包](#623-http拆包粘包)
- [63.Cookie与Session和token](#63cookie与session和token)
  - [63.1 Cookie](#631-cookie)
    - [63.1.1 概念](#6311-概念)
    - [63.1.2 作用](#6312-作用)
    - [63.1.3 Cookie被禁⽤怎么办](#6313-cookie被禁怎么办)
    - [63.1.4 cookie被禁用了，session还能用么](#6314-cookie被禁用了session还能用么)
    - [63.1.15 cookie如何放置攻击](#63115-cookie如何放置攻击)
  - [63.2 Session](#632-session)
    - [63.2.1 概念](#6321-概念)
    - [63.2.2 作用](#6322-作用)
    - [63.2.3 HTTP是不保存状态的协议,如何保存⽤户状态](#6323-http是不保存状态的协议如何保存户状态)
    - [62.2.4 Session用户登录状态过程](#6224-session用户登录状态过程)
    - [62.2.5 如何保存session](#6225-如何保存session)
    - [62.2.6 如何实现 Session 跟踪呢？](#6226-如何实现-session-跟踪呢)
    - [62.2.7 Session机制（多个session如何识别）](#6227-session机制多个session如何识别)
  - [63.3 token](#633-token)
    - [63.3.1 概念](#6331-概念)
    - [63.3.2 作用](#6332-作用)
    - [63.3.3 机制/流程](#6333-机制流程)
  - [63.4 区别](#634-区别)
    - [63.4.1 Cookie 和 Session 的区别](#6341-cookie-和-session-的区别)
    - [64.4.2 token和cookie实现的区别](#6442-token和cookie实现的区别)
- [64.输入网址](#64输入网址)
  - [64.1 输入网址牵扯到的技术](#641-输入网址牵扯到的技术)
  - [64.1 输入网址过程](#641-输入网址过程)
  - [64.2 DNS](#642-dns)
    - [64.2.1 概念](#6421-概念)
    - [64.2.2 作用](#6422-作用)
    - [64.2.3 机制](#6423-机制)
    - [64.2.4 DNS解析超时优化](#6424-dns解析超时优化)
  - [64.3 URI和URL的区别是什么?](#643-uri和url的区别是什么)
  - [64.4 为什么域名要分级设计](#644-为什么域名要分级设计)
  - [64.5 页面跳转方式](#645-页面跳转方式)
    - [64.5.1 分类](#6451-分类)
    - [64.5.2 重定向](#6452-重定向)
      - [64.5.2.1 概念](#64521-概念)
      - [64.5.2.2 作用](#64522-作用)
      - [64.5.2.3 重定向原因](#64523-重定向原因)
    - [64.5.2 转发](#6452-转发)
      - [64.5.2.1 概念](#64521-概念-1)
      - [64.5.2.2 作用](#64522-作用-1)
      - [64.5.2.3 转发原因](#64523-转发原因)
    - [64.5.3 转发与重定向的区别](#6453-转发与重定向的区别)
- [65. 各种协议（TCP/UDP/IP/ARP）](#65-各种协议tcpudpiparp)
  - [65.1 TCP](#651-tcp)
    - [65.1.1 概念](#6511-概念)
      - [65.1.1.1 为什么TCP面向流](#65111-为什么tcp面向流)
    - [65.1.2 TCP三次握手](#6512-tcp三次握手)
      - [65.1.2.1 TCP三次握手流程](#65121-tcp三次握手流程)
        - [65.1.2.1.1 TCP为什么有SYN](#651211-tcp为什么有syn)
        - [65.1.2.1.2 什么是 SYN 攻击？如何避免 SYN 攻击？](#651212-什么是-syn-攻击如何避免-syn-攻击)
        - [65.1.2.1.3 TCP除了SYN，为什么还要 ACK](#651213-tcp除了syn为什么还要-ack)
        - [65.1.2.1.4 为什么客户端和服务端的初始序列号ISN是不相同的？](#651214-为什么客户端和服务端的初始序列号isn是不相同的)
      - [65.1.2.2 TCP为什么要三次握⼿](#65122-tcp为什么要三次握)
      - [65.1.2.3 如何对三次握手进行性能优化](#65123-如何对三次握手进行性能优化)
      - [65.1.2.4 如何绕过三次握手发送数据](#65124-如何绕过三次握手发送数据)
      - [65.1.2.5 TCP Fast Open的过程](#65125-tcp-fast-open的过程)
    - [65.1.3 TCP的四次挥手](#6513-tcp的四次挥手)
      - [65.1.3.1 TCP四次挥手流程](#65131-tcp四次挥手流程)
        - [65.1.3.1.1 客户端主动调⽤了 close ，会发⽣什么？](#651311-客户端主动调了-close-会发什么)
      - [65.1.3.2 TCP为什么要四次挥手](#65132-tcp为什么要四次挥手)
      - [65.1.3.3 如何对四次挥手进行优化](#65133-如何对四次挥手进行优化)
      - [65.1.3.4 第四次挥手后为什么不立即断开？](#65134-第四次挥手后为什么不立即断开)
      - [65.1.3.5 参数](#65135-参数)
        - [65.1.3.5.1 TIME_WAIT](#651351-time_wait)
          - [65.1.3.5.1.1 概念](#6513511-概念)
          - [65.1.3.5.1.2 为什么TIME_WAIT等待的时间是2MSL？](#6513512-为什么time_wait等待的时间是2msl)
          - [65.1.3.5.1.3 为什么需要TIME_WAIT状态？](#6513513-为什么需要time_wait状态)
          - [65.1.3.5.1.4 TIME_WAIT 过多有什么危害？](#6513514-time_wait-过多有什么危害)
          - [65.1.3.5.1.5 如何优化 TIME_WAIT？](#6513515-如何优化-time_wait)
        - [65.1.3.5.2 CLOSE-WAIT](#651352-close-wait)
          - [65.1.3.5.2.1 概念](#6513521-概念)
          - [65.1.3.5.2.2 如果一直处于CLOSE-WAIT是什么原因导致的](#6513522-如果一直处于close-wait是什么原因导致的)
        - [65.1.3.5.2 CLOSE_WAIT和TIME_WAIT的区别](#651352-close_wait和time_wait的区别)
    - [65.1.4 TCP传输数据不一致](#6514-tcp传输数据不一致)
      - [65.1.4.1 TCP传输不一致](#65141-tcp传输不一致)
      - [65.1.4.2 传输数据优化](#65142-传输数据优化)
    - [65.1.5 TCP保活机制](#6515-tcp保活机制)
    - [65.1.6 什么是TCP连接？](#6516-什么是tcp连接)
      - [65.1.6.1 如何唯一确定一个 TCP 连接呢？](#65161-如何唯一确定一个-tcp-连接呢)
      - [65.1.6.2 TCP的最大连接数是多少？](#65162-tcp的最大连接数是多少)
      - [65.1.6.3 TCP连接数为什么远不能达到理论上限](#65163-tcp连接数为什么远不能达到理论上限)
    - [65.1.7 初始序列号](#6517-初始序列号)
      - [65.1.7.1 为什么客户端和服务端的初始序列号 ISN 是不相同的？](#65171-为什么客户端和服务端的初始序列号-isn-是不相同的)
    - [65.1.8 Mss](#6518-mss)
      - [65.1.8.1 概念](#65181-概念)
      - [65.1.8.2 IP层会分片，为什么TCP层还需要MSS呢？](#65182-ip层会分片为什么tcp层还需要mss呢)
    - [65.1.9 TCP首部字段](#6519-tcp首部字段)
    - [65.1.10 TCP作用](#65110-tcp作用)
    - [65.1.11 TCP数据包的大小](#65111-tcp数据包的大小)
    - [65.1.12 TCP 数据包的编号（SEQ）](#65112-tcp-数据包的编号seq)
    - [65.1.13 TCP 数据包的组装](#65113-tcp-数据包的组装)
    - [65.1.14 可靠性传输](#65114-可靠性传输)
    - [65.1.15 TCP半/全连接队列](#65115-tcp半全连接队列)
      - [65.1.15.1 什么是TCP半/全连接队列](#651151-什么是tcp半全连接队列)
  - [65.2 UDP](#652-udp)
    - [65.2.1 概念](#6521-概念)
      - [65.2.1.1 为什么UDP头部没有⾸部⻓度字段](#65211-为什么udp头部没有部度字段)
  - [65.3 IP](#653-ip)
    - [65.3.1 分类](#6531-分类)
  - [65.4 DNS](#654-dns)
    - [65.4.1 概念](#6541-概念)
    - [65.4.2 域名解析工作原理](#6542-域名解析工作原理)
  - [65.5 ARP](#655-arp)
    - [65.1 概念](#651-概念)
    - [66.2 工作流程](#662-工作流程)
  - [65.6 DHCP](#656-dhcp)
    - [65.6.1 概念](#6561-概念)
    - [65.6.2 流程](#6562-流程)
  - [65.3 区别](#653-区别)
    - [65.3.1 TCP与UDP区别](#6531-tcp与udp区别)
    - [65.3.2 TCP与IP的区别](#6532-tcp与ip的区别)
  - [65.4 应用场景](#654-应用场景)
    - [65.4.1 TCP应用场景](#6541-tcp应用场景)
    - [65.4.2 UDP应用场景](#6542-udp应用场景)
    - [65.4.3 IP应用场景](#6543-ip应用场景)
    - [65.4.4 ARP应用场景](#6544-arp应用场景)
- [67.可靠性传输](#67可靠性传输)
  - [67.1 TCP可靠性传输方案](#671-tcp可靠性传输方案)
  - [67.2 重传机制](#672-重传机制)
    - [67.2.1 常见的重传机制](#6721-常见的重传机制)
    - [67.2.2 超时重传](#6722-超时重传)
      - [67.2.2.1 概念](#67221-概念)
      - [67.2.2.2 什么时候会发生超时重传](#67222-什么时候会发生超时重传)
      - [67.2.2.3 超时重传存在的问题](#67223-超时重传存在的问题)
    - [67.2.3 快速重传](#6723-快速重传)
      - [67.2.3.1 概念](#67231-概念)
      - [67.2.3.2 快速重传问题](#67232-快速重传问题)
    - [67.2.4 SACK方法](#6724-sack方法)
      - [67.2.4.1 概念](#67241-概念)
    - [67.2.5 D-SACK](#6725-d-sack)
      - [67.2.5.1 概念](#67251-概念)
      - [67.2.5.2 D-SACK好处](#67252-d-sack好处)
  - [67.3 滑动窗口](#673-滑动窗口)
    - [22-1：引入窗口概念的原因](#22-1引入窗口概念的原因)
      - [22-2：什么是窗口](#22-2什么是窗口)
      - [22-3：窗口大小由哪一方决定？](#22-3窗口大小由哪一方决定)
      - [22-4：发送方的窗口](#22-4发送方的窗口)
      - [22-9：TCP 是如何解决窗口关闭时，潜在的死锁现象呢？](#22-9tcp-是如何解决窗口关闭时潜在的死锁现象呢)
  - [67.4 流量控制](#674-流量控制)
    - [67.4.1 概念](#6741-概念)
  - [67.5 拥塞控制](#675-拥塞控制)
    - [67.5.1 为什么要有拥塞控制呀，不是有流量控制了吗？](#6751-为什么要有拥塞控制呀不是有流量控制了吗)
    - [67.5.2 什么是拥塞控制](#6752-什么是拥塞控制)
    - [67.5.3 什么是拥塞窗口？和发送窗口有什么关系呢？](#6753-什么是拥塞窗口和发送窗口有什么关系呢)
    - [67.5.4 那么怎么知道当前网络是否出现了拥塞呢？](#6754-那么怎么知道当前网络是否出现了拥塞呢)
    - [67.5.5 拥塞控制算法](#6755-拥塞控制算法)
      - [67.5.5.1 慢启动](#67551-慢启动)
        - [67.5.5.1.1 规则](#675511-规则)
          - [67.5.5.1.1.1 慢启动涨到什么时候是个头呢？](#6755111-慢启动涨到什么时候是个头呢)
      - [67.5.5.2 拥塞避免算法](#67552-拥塞避免算法)
        - [67.5.5.2.1 规则](#675521-规则)
          - [67.5.5.2.1.1 重传机制何时结束](#6755211-重传机制何时结束)
      - [67.5.5.3 拥塞发生](#67553-拥塞发生)
        - [67.5.5.3.1 分类](#675531-分类)
        - [67.5.5.3.2 超时重传的拥塞发⽣算法](#675532-超时重传的拥塞发算法)
        - [67.5.5.3.3 快速重传的拥塞发⽣算法](#675533-快速重传的拥塞发算法)
        - [67.5.5.3.3 快速恢复算法](#675533-快速恢复算法)
          - [67.5.5.3.3.2 规则](#6755332-规则)
  - [67.6 ARQ协议](#676-arq协议)
    - [24-1：什么是ARQ协议](#24-1什么是arq协议)
    - [24-2：什么是停⽌等待ARQ协议](#24-2什么是停等待arq协议)
    - [24-3: 什么是连续ARQ协议](#24-3-什么是连续arq协议)
  - [67.7 UDP如何做可靠传输](#677-udp如何做可靠传输)
- [68.Scoket](#68scoket)
  - [68.1 概念](#681-概念)
  - [68.2 Socket编程](#682-socket编程)
  - [socket例子](#socket例子)
- [------操作系统--------------------------------------------](#------操作系统--------------------------------------------)
- [1. 内存管理-虚拟内存](#1-内存管理-虚拟内存)
  - [1-1：什么是虚拟地址](#1-1什么是虚拟地址)
  - [1-2：操作系统是如何管理虚拟地址与物理地址之间的关系？](#1-2操作系统是如何管理虚拟地址与物理地址之间的关系)
  - [1-3：什么是虚拟内存](#1-3什么是虚拟内存)
    - [1-3-1：虚拟内存的优缺点](#1-3-1虚拟内存的优缺点)
    - [1-3-2：为什么虚拟内存可以大于物理内存](#1-3-2为什么虚拟内存可以大于物理内存)
- [2.内存管理-内存分段](#2内存管理-内存分段)
  - [2-1：什么是内存分段](#2-1什么是内存分段)
  - [2-2：分段机制下，虚拟地址和物理地址是如何映射的？](#2-2分段机制下虚拟地址和物理地址是如何映射的)
  - [2-3：访问某段偏移量xxx的虚拟地址](#2-3访问某段偏移量xxx的虚拟地址)
  - [2-4：内存分段缺陷](#2-4内存分段缺陷)
  - [2-5：如何解决内存分段的缺陷](#2-5如何解决内存分段的缺陷)
- [3. 内存管理-内存分页](#3-内存管理-内存分页)
  - [3-1：为什么有内存分页（内存分页定义）](#3-1为什么有内存分页内存分页定义)
  - [3-2：分页是怎么解决分段的内存碎片、内存交换效率低的问题？](#3-2分页是怎么解决分段的内存碎片内存交换效率低的问题)
  - [3-3：分页机制下，虚拟地址和物理地址是如何映射的？](#3-3分页机制下虚拟地址和物理地址是如何映射的)
  - [3-4：简单的分页有什么缺陷吗？](#3-4简单的分页有什么缺陷吗)
  - [3-5：简单的分页缺陷的解决方案](#3-5简单的分页缺陷的解决方案)
  - [3-6：分了二级表，内存不是变大了呢](#3-6分了二级表内存不是变大了呢)
  - [3-7：为什么不分级的页表就做不到这样节约内存呢？](#3-7为什么不分级的页表就做不到这样节约内存呢)
  - [3-8：多级页表的缺陷以及解决方案](#3-8多级页表的缺陷以及解决方案)
- [4.内存管理-段页式内存管理](#4内存管理-段页式内存管理)
  - [4-1：什么是段页式内存管理](#4-1什么是段页式内存管理)
  - [4-2：段页式内存管理实现的方式](#4-2段页式内存管理实现的方式)
- [5. 内存管理-linux内存管理](#5-内存管理-linux内存管理)
  - [5-1：Linux 操作系统采用了哪种方式来管理内存](#5-1linux-操作系统采用了哪种方式来管理内存)
  - [5-2：Linux 的虚拟地址空间是如何分布的？](#5-2linux-的虚拟地址空间是如何分布的)
  - [5-3：内核空间与用户空间的区别](#5-3内核空间与用户空间的区别)
  - [5-4：用户空间分布](#5-4用户空间分布)
- [6.用户态和内核态](#6用户态和内核态)
  - [6-1：为什么要有用户态和内核态？](#6-1为什么要有用户态和内核态)
  - [6-2：用户态与内核态的切换](#6-2用户态与内核态的切换)
  - [6-3：用户态和内核态的概念区别](#6-3用户态和内核态的概念区别)
- [7.内存管理-页面置换算法](#7内存管理-页面置换算法)
  - [7-1：页面置换算法](#7-1页面置换算法)
  - [6-2：⻚⾯置换算法的作⽤?](#6-2置换算法的作)
  - [6-3：手写LRU缓存](#6-3手写lru缓存)
- [8.调度算法](#8调度算法)
  - [1. 进程调度算法](#1-进程调度算法)
    - [-1-1：什么时候会发⽣ CPU 调度呢](#-1-1什么时候会发-cpu-调度呢)
    - [-1-2：调度算法](#-1-2调度算法)
  - [2.页面置换算法](#2页面置换算法)
    - [-2-1：缺页](#-2-1缺页)
    - [-2-2：缺页中断流程](#-2-2缺页中断流程)
    - [-2-3：页面置换算法](#-2-3页面置换算法)
  - [3.磁盘调度算法](#3磁盘调度算法)
  - [-3-1：常用算法](#-3-1常用算法)
  - [单核cpu需要考虑线程安全](#单核cpu需要考虑线程安全)
- [7.文件系统组成](#7文件系统组成)
  - [7-1：什么是文件系统](#7-1什么是文件系统)
  - [7-2：文件系统的组成](#7-2文件系统的组成)
    - [7-2-1：⽬录项和⽬录是⼀个东⻄呢](#7-2-1录项和录是个东呢)
    - [7-2-2：⽂件数据是如何存储在磁盘](#7-2-2件数据是如何存储在磁盘)
- [8-1：键盘敲⼊字⺟时，期间发⽣了什么？](#8-1键盘敲字时期间发了什么)

<!-- /TOC -->

# 56.各层协议

## 56.1 OSI与TCP/IP各层的结构与功能,都有哪些协议?

7. `应用层`
为应用程序提供服务并且规定通信的规范和细节
   常见的协议:
   * HTTP(超文本传输协议)
   * FTP(文件传输协议)
   * TELNET(远程登录协议)
   * SMTP(简单邮件传输协议)
   * DNS(域名解析协议)
6. `表示层`
主要负责数据格式的转换
5. `会话层`
负责建立和断开通信连接
   * 结构化查询语言（ SQL, Structured Query Language ）
   * 网络文件系统（ NFS ， Network File System ）
   * 远程过程调用（ RPC ， Remote Procedure Call ）
4. `传输层`
   是唯一负责总体的数据传输和数据控制的一层。
   * TCP: ~~面向连接 ,可靠性强, 传输效率低~~
   * UDP: ~~无连接,可靠性弱,传输效率快~~
3. `网络层`
   将数据传输到目标地址；主要负责寻找地址和路由选择，网络层还可以实现拥塞控制、网际互连等功能
   * IP
   * IPX
   * RIP
   * OSPF等
2. `数据链路层`
   物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。
   * ARP<font color=pink size='6'>（比较有争议，因为有的书籍放置了在网络层，有的书籍放置在了数据链路层，主要看）</font>
        注：最好的回答是，ARP 是询问具有某个 IP 地址的设备的 MAC 地址的，
            介于数据链路层和网络层之间。有的划分为数据链路层，这是根据封装方式来划分的，
            ARP 数据包封装成帧。有的划分为网络层，这是根据协议内容划分的，
            ARP 报文中有 IP 信息。我觉得应该划分为链路层。
     <font color='#2345'>华为曾经面试问过我</font>
   * RARP
   * SDLC
   * HDLC
   * PPP
   * STP
   * 帧中继等
1. `物理层`
   负责0、1比特流(0/1序列)与电压的高低、光的闪灭之间的转换

## 56.2 ⽹络层与数据链路层有什么关系呢？

1. IP 的作⽤是主机之间通信⽤的，负责在「没有直连」的两个⽹络之间进⾏通信传输
2. MAC 的作⽤则是实现「直连」的两个设备之间通信。
理解一下：
就比如说，你想从xx村到海南市，你不得做公交车、汽车、火车、轮船到海南
那么你这整个的一个路线图，就是一个网络层，行程开端就是xx村---->>>源IP，目的IP---->行程结束就是海南
那么我从xx村到xx镇相当于是在这个区间内移动路线，也就是数据链路层，
其中，xx村好⽐源 MAC 地址，xx镇好⽐⽬的 MAC 地址。
（只要是在线路（网络层包含的都是））
这个xx村、海南不会发生变化，但是中间的位置会一直在变，也就是说
IP源、目的不会变， Mac源、目的会变化

## 56.3 网络层的路由算法

1. 全局式路由选择算法：所有路由器掌握完整的网络拓扑和链路费用信息，例如链路状态(LS)路由算法
2. 分散式路由选择算法；路由器只掌握物理相连的邻居以及链路费用，例如距离向量(DV)路由算法

链路状态路由选择算法可以用Dijksua算法实现。

## 56.4 各种协议的端口号

（1）HTTP：80号端口以提供服务。
（2）DNS：DNS用的是53号端口。　　
（3）SNMP：简单网络管理协议，使用161号端口

## 56.5 四组元/五元组/七元组

四元组：源IP地址、目的IP地址、源端口、目的端口
五元组：源IP地址、目的IP地址、协议号、源端口、目的端口
七元组：源IP地址、目的IP地址、协议号、源端口、目的端口，服务类型以及接口索引

# 57.Get与Post

## 57.1 get与post的区别

1. GET产生一个TCP数据包；POST产生两个TCP数据包。
   因为对于一个get请求，浏览器会把http 头和数据一并发送出去，服务器响应200；
   post请求，浏览器先发送header，服务器响应之后，浏览器在发送数据，服务器响应200
2. Get ⽅法的含义是请求从服务器获取资源，这个资源可以是静态的⽂本、⻚⾯、图⽚视频等等
   ⽐如，你打开我的⽂章，浏览器就会发送 GET 请求给服务器，服务器就会返回⽂章的所有⽂字及资源
   ⽽ POST ⽅法则是相反操作，它向 URI 指定的资源提交数据，数据就放在报⽂的 body ⾥。
   ⽐如，你在我⽂章底部，敲⼊了留⾔后点击「提交」，浏览器就会执⾏⼀次 POST 请求，把你的留⾔⽂字放进了报⽂ body ⾥，
   然后拼接好 POST 请求头，通过 TCP 协议发送给服务器。
3. 就对于服务器资源是否发生破坏来说，
   GET ⽅法就是安全且幂等的，因为它是「只读」操作，⽆论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。
   POST 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，所以不是幂等的。
4. get传送的数据量较小，不能大于2KB。post传送的数据量较大，
   一般被默认为不受限制。但理论上，IIS4中最大量为80KB，IIS5中为100KB。
5. GET请求能够被保存在浏览器的浏览历史里面（密码等重要数据GET提交，别人查看历史记录，就可以直接看到这些私密数据）
   POST不进行缓存。

## 57.2 幕等

### 57.2.1 概念

多个请求返回相同的结果

## 57.3 为什么都用get而不用post？

1. 主要还是减低服务器流量压力
   比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。
   POST 请求就不那么轻松了。POST 表示可能改变服务器上的资源的请求。仍然以新闻站点为例，读者对文章的注解应该通过 POST 请求实现，
   比如说对数据请求频繁，数据不敏感且数据量在普通浏览器最小限定的2k范围内，这样的情况使用GET。其他地方使用POST。
2. 多数浏览器对于POST采用两阶段发送数据的，get只需要一阶段就可以了
   如果通信时间增加，这段时间客户端与服务器端一直保持连接状态，在服务器侧负载可能会增加，可靠性会下降。

## 57.4 get与post的应用场景

get主要是用来查询数据，比如说搜索某些内容
pist论坛上回贴、在博客上评论用的就比较多了


## 57.2 后台获取前端数据方法

1. Servlet

```java
@RequestMapping("allCity3")
    public String queryAllCity3(HttpServletRequest request, HttpServletResponse response) {
        List<City> cities = cityService.queryAllCity();
        for (City city : cities) {
            city.setAreas(null);
        }
        // 保存到session中
        /*HttpSession session = request.getSession();
        session.setAttribute("cities", cities);*/
        // 保存到request中
        request.setAttribute("cities", cities);
        // 返回的页面（这里配合相应的springMvc视图解析器）
        return "main";
    }
```

2. ModelAndView
把数据保存到ModelAndView中，直接传送到前端页面

```java
// 把要保存的数据保存进ModelAndView中
modelAndView.addObject("areaStatisticsList", areaStatisticsList);
// 设置跳转页面
modelAndView.setViewName("areaStatistics");
// 返回值
return modelAndView;
```

3. 通过@RequestParam获取
   形式如：@RequestParam(value="username") String userName

4. 通过@RequestBody注解获取
   主要用来接收前端传递给后端的json数据(后台函数形参可以String，也可以是类)；
   GET方式无请求体，所以使用@RequestBody接收数据时，
   前端不能使用GET方式提交数据，而是用POST方式进行提交。
   @RequestBody一般用来处理非Content-Type: application/x-www-form-urlencoded编码格式的数据。

# 58.状态码

## 58.1 状态码种类

2xx （3种）
   1. 200 ：表示从客户端发送给服务器的请求被正常处理并返回；
   2. 204 ：表示客户端发送给客户端的请求得到了成功处理，
   3. 206 ：表示客户端进行了范围请求。
3xx （5种）
   1. 301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；
   2. 302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；
         301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）
   3. 303 See Other：表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；
         302与303的区别：后者明确表示客户端应当采用GET方式获取资源
   4. 304 Not Modified：表示客户端发送附带条件
                       （是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、
                         If-None-Match、If-Range、If-Unmodified-Since中任一首部）
                         的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回改状态码；
   5. 307 Temporary Redirect：临时重定向，与303有着相同的含义，
      307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；
4xx （4种）
   1. 400 Bad Request：表示请求报文中存在语法错误；
   2. 401 Unauthorized：未经许可，需要通过HTTP认证；
   3. 403 Forbidden：服务器拒绝该次访问（访问权限出现问题）
   4. 404 Not Found：表示服务器上无法找到请求的资源，除此之外，
                     也可以在服务器拒绝请求但不想给拒绝原因时使用；
5xx （2种）
   1. 500 Inter Server Error：表示服务器在执行请求时发生了错误，
                              也有可能是web应用存在的bug或某些临时的错误时；
   2. 503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，
                              无法处理请求；

# 59.报文结构

## 59.1 报文组成

一个HTTP请求报文由四个部分组成：请求行、请求头、空行、请求数据。
1. 请求行
   请求行由请求方法字段、URL字段和HTTP协议版本字段3个字段组成，
   它们用空格分隔。比如 GET /data/info.html HTTP/1.1
2. 请求头部
   HTTP客户程序(例如浏览器)，向服务器发送请求的时候必须指明请求类型(一般是GET或者 POST)。
   如有必要，客户程序还可以选择发送其他的请求头。大多数请求头并不是必需的，。
   常见的请求头字段以及他的含义：
      1. Accept：浏览器可接受的MIME类型。
      2. Accept-Charset：浏览器可接受的字符集。
      3. Accept-Encoding：浏览器能够进行解码的数据编码方式，
                          比如gzip。Servlet能够向支持gzip的浏览器
                          返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。
      4. Accept-Language：浏览器语言种类，
      5. Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头应答
      6. Content-Length：表示请求消息正文的长度。
      7. Host： 客户机告诉服务器，想访问的主机名。
      8. If-Modified-Since：客户机告诉服务器，资源的缓存时间。
      9. Referer：客户机告诉服务器，它是从哪个资源来访问服务器的(防盗链)。
      10. User-Agent：User-Agent头域的内容包含发出请求的用户信息。
      11. Cookie：客户机通过这个头可以向服务器带数据，这是最重要的请求头信息之一。
3. 空行
   它的作用是通过一个空行，告诉服务器请求头部到此为止。
4. 请求数据
   若方法字段是POST,则通常来说此处放置的就是要提交的数据

# 60.HTTP

## 60.1 HTTP常用字段

1. Host字段
2. Content-Length字段
3. Connection字段
4. Content-Type字段
5. Content-Encoding字段

### 60.1.1 Host字段

客户端发送请求的时候，用来制定服务器的域名，这样我就可以往同一台服务器不同网站进行访问了

### 60.1.2 Content-Length字段

服务器返回数据的时候，就会有Content-Length字段，然后浏览器就会收到多少个字节

### 60.1.3 Connection字段

用于客户端要求服务端使用TCP进行持久连接
但是http 1.1 可以是持久连接

### 60.1.4 Content-Type字段

主要是服务器告诉客户端我是什么样的数据格式

### 60.1.15 Content-Encoding字段

说明数据的压缩方法或者格式

## 60.2 HTTP的工作流程

域名解析 -> 三次握手 -> 发起HTTP请求 -> 响应HTTP请求并得到HTML代码
-> 浏览器解析HTML代码  -> 浏览器对页面进行渲染呈现给用户

### 60.2.1 一个 TCP 连接后是否会在一个HTTP请求完成后断开？什么情况下会断开？

在 HTTP/1.0 中，一个服务器在发送完一个 HTTP 响应后，会断开 TCP 链接。
但是这样每次请求都会重新建立和断开 TCP 连接，代价过大。所以虽然标准中没有设定，
某些服务器对 Connection: keep-alive 的 Header 进行了支持。
意思是说，完成这个 HTTP 请求之后，不要断开 HTTP 请求使用的 TCP 连接。
这样的好处是连接可以被重新使用，之后发送 HTTP 请求的时候不需要重新建立 TCP 连接，
以及如果维持连接，那么 SSL 的开销也可以避免

### 60.2.2 HTTP慢的原因和解决方式

#### 60.2.2.1 HTTP慢的原因

1. 可能是带宽，网络基础建设完善后，带宽基本不是太大的问题

2. 延迟。HTTP/1.0主要的问题在于连接无法复用和head of line blocking.
   1）连接无法复用，导致每次请求经历3次握手和慢启动。
      3次握手在高延迟的场景下影响较为明显，慢启动则对文件类大请求影响较大
   
   2）head of line blocking，导致带宽无法被充分利用，以及后续的健康请求被阻塞。
      例如，有5个请求需要同时发出。HTTP/1.0时，
      只有在第一个请求的response回来之后后续的请求才会逐个发出。
      如果请求1的request没有及时抵达服务器或response没有及时返回，
      后续的请求就被耽搁了。

#### 60.2.2.2 如何解决HTTP慢

1. 解决连接无法复用
   http/1.0协议头里可以设置Connection:Keep-Alive或者Connection:Close，
   选择是否允许在一定时间内复用连接（时间可由服务器控制）。
   但是这对App端的请求成效不大，因为App端的请求比较分散且时间跨度相对较大。

      方案1.基于tcp的长连接
      移动端建立一条自己的长链接通道，通道的实现是基于tcp协议。
      基于tcp的socket编程技术难度相对复杂很多，而且需要自己定制协议。
      但信息的上报和推送变得更及时，
      请求量爆发的时间点还能减轻服务器压力（避免频繁创建和销毁连接）

      方案2.http long-polling
      客户端在初始状态发送一个polling请求到服务器，服务器并不会马上返回业务数据，
      而是等待有新的业务数据产生的时候再返回，所以链接会一直被保持。
      一但结束当前连接，马上又会发送一个新的polling请求，如此反复，保证一个连接被保持。
      存在问题：
      1）增加了服务器的压力
      2）网络环境复杂场景下，需要考虑怎么重建健康的连接通道
      3）polling的方式稳定性不好
      4）polling的response可能被中间代理cache住
      ……

      方案3.http streaming
      和long-polling不同的是，streaming方式通过再server response
      的头部增加“Transfer Encoding:chuncked”来告诉客户端后续还有新的数据到来
      存在问题：
      1）有些代理服务器会等待服务器的response结束之后才将结果推送给请求客户端。streaming不会结束response
      2）业务数据无法按照请求分割
      ……

      方案4.web socket
      和传统的tcp socket相似，基于tcp协议，提供双向的数据通道。
      它的优势是提供了message的概念，比基于字节流的tcp socket使用更简单。
      技术较新，不是所有浏览器都提供了支持。

2. 解决head of line blocking

它的原因是队列的第一个数据包（队头）受阻而导致整列数据包受阻

方案1.http pipelining
几乎在同一时间把request发向了服务器

## 60.3 HTTP存在的风险/问题

1. 通信使⽤明⽂（不加密），内容可能会被窃听。⽐如， 账号信息容易泄漏，那你号没了。
2. 不验证通信⽅的身份，因此有可能遭遇伪装。⽐如， 访问假的淘宝、拼多多，那你钱没了。
3. ⽆法证明报⽂的完整性，所以有可能已遭篡改。⽐如， ⽹⻚上植⼊垃圾⼴告，视觉污染，眼没了。

## 60.4 分类

1. HTTP 1.0
2. HTTP 1.1
3. HTTP 2
4. HTTP 3

## 60.4 持久连接与非持久连接（长连接与短连接）

### 60.4.1 HTTP哪些是持久连接？

HTTP/1.0 使用非持久连接。
HTTP/1.1 默认使用持久连接。

### 60.4.2 非持久链接

#### 60.4.2.1 如何使用

比如说，客户端请求一个页面。假设该页面包含1个HTML文件和10个JPEG图像

1. 首先，HTTP客户端与服务器主机某个网址中的HTTP服务器建立一个TCP连接。
2. 然后，HTTP客户端发送HTTP请求消息。 包含了比如说html文件。
3. 然后，HTTP服务器接收请求消息，从服务器主机内存或硬盘拿去除对象/sompath/index.html，发出该对象的响应消息。
4. 然后，HTTP服务器告知TCP关闭这个TCP连接(TCP要等客户收到这个响应消息后，才会真正终止这个连接)。
5. 之后，HTTP客户接收响应消息。TCP连接终止。 该消息标明所拆装的对象是一个HTML文件。客户取出文件，分析后发现10个JPEG对象的引用。
6. 给每一个引用到的JPEG对象重复步骤操作。

#### 60.4.2.2 如何改善时间延时问题

通过并行的TCP连接同时取到其中的某些对象。使用并行TCP连接，可以缩短响应时间。、

#### 60.4.2.3 缺点

1. 每个连接，TCP得在客户端和服务端分配TCP缓冲区，并维持TCP变量。
   对于同时为来自数百个不同客户的请求提供服务的web服务器来说，这会严重增加其负担。
2. 每个对象都有2个RTT的延迟。
3. 每个对象都遭受TCP缓启动，因为每个TCP连接都起始于缓启动阶段。

### 60.4.3 持久连接

### 60.4.4 参数

通过HTTP的Keep-alive是要让一个TCP连接活久点

#### 60.4.4.1 TCP的keep alive和HTTP的Keep-alive区别

TCP的keep alive是检查当前TCP连接是否活着
HTTP的Keep-alive是要让一个TCP连接活久点

## 60.2 HTTP 1.0

### 60.2.1 HTTP 1.0优缺点

`I.优点`

1. HTTP基本的报⽂格式就是header + body，头部信息也是key-value简单⽂本的形式。
2. HTTP协议⾥的各类请求⽅法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，
   都允许开发⼈员⾃定义和扩充。
3. HTTP由于是⼯作在应⽤层，则它下层可以随意变化。
4. 应⽤⼴泛和跨平台

`II.缺点`

1. ⽆状态
   由于⽆状态，它在完成有关联性的操作时会⾮常麻烦。
   例如登录->添加购物⻋->下单->结算->⽀付，这系列操作都要知道⽤户的身份才⾏。
   但服务器不知道这些请求是有关联的，每次都要问⼀遍身份信息。
2. 明⽂传输
   明⽂意味着在传输过程中的信息，是可⽅便阅读的，
   通过浏览器的控制台或抓包软件都可以直接⾁眼查看，
   信息的内容都毫⽆隐私可⾔，很容易就能被窃取。
3. 不安全

#### 60.2.1.1 HTTP为什么是无连接和无状态的

1. 对于无连接来说
   限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。

2. 对于无状态来说
   大大减轻了服务器记忆负担，从而保持较快的响应速度。

### 60.2.2 无状态的解决方案

1. Cookie 技术
   因为Cookie通过在请求和响应报⽂中写⼊Cookie信息来控制客户端的状态。
   相当于，在客户端第⼀次请求后，服务器会下发⼀个装有客户信息的「⼩贴纸」，后续客户端请求服务器的时候，
   带上「⼩贴纸」，服务器就能认得了了，



## 60.3 HTTP 1.1

### 60.3.1 HTTP 1.1相对于HTTP 1.0改善

1. 因为早期HTTP/1.0，那就是每发起⼀个请求需要三次握手四次挥手等等操作，
   增加了通信开销。为了解决这些问题，HTTP/1.1提出了⻓连接的通信⽅式只要任意⼀端没有明确提出断开连接，
   则保持 TCP 连接状态。
2. HTTP/1.1 采⽤了⻓连接的⽅式，可在同⼀个 TCP 连接⾥⾯，客户端可以发起多个请求，
   只要第⼀个请求发出去了，不必等其回来，就可以发第⼆个请求出去，可以减少整体的响应时间。
3. 错误状态响应码，HTTP1.1新增了很多错误装填响应码，让开发者更加了解错误根源
4. 在HTTP1.0中主要使⽤header⾥的
   If-Modified-Since,Expires来做为缓存判断的标准，在HTTP1.1中引⼊了更多的缓存控制策略
5. HTTP1.0中，存在⼀些浪费带宽的现象，例如客户端只是需要某个对象的⼀部分，⽽服务器却将整个对象送过来了，
   并且不⽀持断点续传功能，HTTP1.1则在请求头引⼊了range头域，它允许只请求资源的某个部分

### 60.3.2 HTTP 1.1缺点

1. 延迟难以下降 ，虽然现在⽹络的「带宽」相⽐以前变多了，但是延迟降到⼀定幅度后，就很难再下降了，
2. 并发连接有限 ，⾕歌浏览器最⼤并发连接数是 6 个，
   ⽽且每⼀个连接都要经过 TCP 和 TLS 握⼿耗时，以及TCP 慢启动过程给流量带来的影响；
3. 队头阻塞问题 ，同⼀连接只能在完成⼀个 HTTP 事务（请求和响应）后，才能处理下⼀个事务；
4. HTTP  头部巨⼤且重复 ，由于 HTTP 协议是⽆状态的，每⼀个请求都得携带 HTTP 头部，
   特别是对于有携带cookie 的头部，⽽ cookie 的⼤⼩通常很⼤；
5. 不⽀持服务器推送消息 ，因此当客户端需要获取通知时，
   只能通过定时器不断地拉取消息，这⽆疑浪费⼤量了带宽和服务器资源。
6. 报⽂中 Header 部分存在的问题：含很多固定的字段，
   ⽐如Cookie、User Agent、Accept 等，这些字段加起来也⾼达⼏百字节甚⾄上千字节，同时字段重复

### 60.3.3 HTTP 1.1优化方案

1. 方案一：尽量避免发送HTTP请求；
2. 方案二：在需要发送HTTP请求时，考虑如何减少请求次数；
3. 方案三：减少服务器的HTTP响应的数据⼤⼩；

方案一：
如何避免发送HTTP请求呢？
对于⼀些具有重复性的 HTTP 请求，⽐如每次请求得到的数据都⼀样的，我们可以把这对「请求-响应」的数据都缓存在本地，
那么下次就直接读取本地的数据，不必在通过⽹络获取服务器的响应了，这样的话 HTTP/1.1的性能肯定⾁眼可⻅的提升。
所以，通过缓存技术，而HTTP协议的头部有不少是针对缓存的字段。

1. 首先，客户端会把第⼀次请求以及响应的数据保存在本地磁盘上，其中将请求的 URL 作为 key，⽽响应作为 value，两者形成映射关系。
2. 这样当后续发起相同的请求时，就可以先在本地磁盘上通过 key 查到对应的 value，也就是响应，如果找到了，就直接从本地读取该响应。
   读取本次磁盘的速度肯定⽐⽹络请求快得多，
3. 同时对于缓存的响应如果不是最新的，玩意拿错了，HTTP头部有一个过期时间参数，我记得是Expires
   就是说服务器在发送 HTTP 响应时，会估算⼀个过期的时间，并把这个信息放到响应头部中，这样客户端在查看响应头部的信息时，
   ⼀旦发现缓存的响应是过期的，则就会重新发送⽹络请求。
4. 但是如果客户端从第⼀次请求得到的响应头部中发现该响应过期了，客户端重新发送请求，假设服务器上的资源并没有变更，还是⽼样⼦，
   HTTP做了一个设定，在请求的etag头部带上第⼀次请求的响应头部中的摘要，这个摘要是唯⼀标识响应的资源，当服务器收到请求后，
   会将本地资源的摘要与请求中的摘要做个⽐较。如果不同，那么说明客户端的缓存已经没有价值，服务器在响应中带上最新的资源。
   如果相同，说明客户端的缓存还是可以继续使⽤的，那么服务器仅返回不含有包体的 304 Not Modified 响应，
   告诉客户端仍然有效，这样就可以减少响应资源在⽹络中传输的延时

方案二：
减少请求次数，这样的话，可以通过：1）减少重定向请求次数；2）合并请求；3）延迟发送请求

1. 如果重定向请求越多，那么客户端就要多次发起HTTP请求，每⼀次的HTTP请求都得经过⽹络，这⽆疑会越降低⽹络性能。
   另外，服务端这⼀⽅往往不只有⼀台服务器，⽐如源服务器上⼀级是代理服务器，然后代理服务器才与客户端通信，
   这时客户端重定向就会导致客户端与代理服务器之间需要2次消息传递，
   如果重定向的⼯作交由代理服务器完成，就能减少 HTTP 请求次数了
2. 如果把多个访问⼩⽂件的请求合并成⼀个⼤的请求，虽然传输的总资源还是⼀样，但是减少请求，也就意味着减少了重复发送的 HTTP 头部。
   有的⽹⻚会含有很多⼩图⽚、⼩图标，有多少个⼩图⽚，客户端就要发起多少次请求。那么对于这些⼩图⽚，我们可以考虑使⽤
   CSS Image Sprites 技术、或者webpack打包工具，或者将图⽚的⼆进制数据⽤ base64 编码后，以URL的形式潜⼊到HTML⽂件，跟随 HTML ⽂件⼀并发送.把它们合成⼀个⼤图⽚，这样浏览器就可以⽤⼀次请求获得⼀个⼤图⽚，
   然后再根据 CSS 数据把⼤图⽚切割成多张⼩图⽚。
   通过将多个⼩图⽚合并成⼀个⼤图⽚来减少 HTTP 请求的次数，以减少 HTTP 请求的次数，从⽽减少⽹络的开销。

3. ⼀般 HTML ⾥会含有很多 HTTP 的 URL，当前不需要的资源，我们没必要也获取过来，于是可以通过「按需获取」的⽅式，
   来减少第⼀时间的 HTTP 请求次数。请求⽹⻚的时候，没必要把全部资源都获取到，⽽是只获取当前⽤户所看到的⻚⾯资源
   当⽤户向下滑动⻚⾯的时候，再向服务器获取接下来的资源，这样就达到了延迟发送请求的效果。

方案三：
减少http相应的数据大小，可以考虑对相应的资源进行压缩，从而提升网络传输效率。一般的压缩方式有两种：无损和有损
1. 无损压缩：⾸先，我们针对代码的语法规则进⾏压缩，因为通常代码⽂件都有很多换⾏符或者空格，把这些多余的符号给去除掉。
           接下来，就是⽆损压缩了，需要对原始资源建⽴统计模型，利⽤这个统计模型，将常出现的数据⽤较短的⼆进制⽐
           特序列表示，将不常出现的数据⽤较⻓的⼆进制⽐特序列表示，⽣成⼆进制⽐特序列⼀般是「霍夫曼编码」算法。
           gzip 就是⽐较常⻅的⽆损压缩。客户端⽀持的压缩算法，
           会在 HTTP 请求中通过头部中的 Accept-Encoding 字段告诉服务器：
           服务器收到后，会从中选择⼀个服务器⽀持的或者合适的压缩算法，然后使⽤此压缩算法对响应资源进⾏压缩，
           最后通过响应头部中的 content-encoding 字段告诉客户端该资源使⽤的压缩算法。
           gzip 的压缩效率相⽐Google推出的Br算法低，所以如果可以，服务器应该选择压缩效率更⾼的 br 压缩算法。

2. 有损压缩：解压的数据会与原始数据不同但是⾮常接近。有损压缩主要将次要的数据舍弃，牺牲⼀些质量来减少数据量、提⾼压缩⽐，
           这种⽅法经常⽤于压缩多媒体数据，⽐如⾳频、视频、图⽚。可以通过 HTTP 请求头部中的 Accept 字段⾥
           的「 q 质量因⼦」，告诉服务器期望的资源质量。
           ⽐如说，⼀个在看书的视频，画⾯通常只有⼈物的⼿和书桌上的书是会有变化的，⽽其他地⽅通常都是静态的，
           于是只需要在⼀个静态的关键帧，使⽤增量数据来表达后续的帧，这样便减少了很多数据，提⾼了⽹络传输的性能。

## 60.4 HTTP 2

### 60.4.1 HTTP 2如何兼容HTTP 1.1

1. HTTP/2 没有在 URI ⾥引⼊新的协议名，仍然⽤「http://」表示明⽂协议，⽤「https://」表示加密协议，

2. 只在应⽤层做了改变，还是基于 TCP 协议传输，应⽤层⽅⾯为了保持功能上的兼容，
   HTTP/2 把 HTTP 分解成了「语义」和「语法」两个部分，「语义」层不做改动，与 HTTP/1.1 完全⼀致，
   ⽐如请求⽅法、状态码、头字段等规则保留不变。

### 60.4.2 HTTP 2优势

1. HTTP 2会压缩头如果你同时发出多个请求，
   他们的头是⼀样的或是相似的，那么，协议会帮你消除重复的部分。
   使用HPACK 算法：在客户端和服务器同时维护⼀张头信息表，所有字段都会存⼊这个表，
   ⽣成⼀个索引号，以后就不发送同样字段了，只发送索引号，这样就提⾼速度了。
2. HTTP 2全⾯采⽤了⼆进制格式，头信息和数据体都是⼆进制，
         计算机收到报⽂后，直接解析⼆进制报⽂，这增加了数据传输的效率。
3. HTTP 2的数据包不是按顺序发送的
4. HTTP 2是可以在⼀个连接中并发多个请求或回应。
5. HTTP 2还在⼀定程度上改善了传统的「请求 - 应答」⼯作模式，
          服务不再是被动地响应，也可以主动向客户端发送消息。

#### 60.4.2.1 头部压缩

##### 60.4.2.1.1 头部压缩方案

使用了HPACK算法HPACK 算法主要包含三个组成部分：
1. 静态字典；
2. 动态字典；
3. Huffman 编码（压缩算法）；

客户端和服务器两端都会建⽴和维护「字典」，⽤⻓度较⼩的索引号表示重复的字符串，再⽤Huffman编码压缩数据，可达到较高的⾼压缩率

##### 60.4.2.1.2 静态表编码

HTTP/2 为⾼频出现在头部的字符串和字段建⽴了⼀张静态表，它是写⼊到 HTTP/2 框架⾥的，不会变化的，静态表⾥共有61组
表中的 Index 表示索引（Key），Header Value 表示索引对应的 Value，Header Name 表示字段的名字，
⽐如Index为2代表GET，Index为8代表状态码200。

HTTP/2 头部由于基于⼆进制编码，就不需要冒号空格和末尾的\r\n作为分隔符，于是改⽤表示字符串⻓度（Value
Length）来分割 Index 和 Value。

比如说server头部字段，在 HTTP/1.1 的形式：
nghttpx\r\n算上冒号空格和末尾的\r\n，共占⽤了 17 字节，
⽽使⽤了静态表和 Huffman 编码，可以将它压缩成 8 字节，压缩率达到了一半。

##### 60.4.2.1.3 动态表编码

#### 60.4.2.2 二进制帧

##### 60.4.2.2.1 概念

HTTP/2 把响应报⽂划分成了两个帧，HEADERS（⾸部）和 DATA（消息负载）是帧的类型，
也就是说⼀条HTTP响应，划分成了两个帧来传输，并且采⽤⼆进制来编码。

##### 60.4.2.2.2 

#### 60.4.2.3 并发传输

##### 60.4.2.3.1 并发传输如何实现的

并发传输主要是通过Stream设计的，1个TCP连接包含⼀个或者多个Stream，Stream是HTTP/2 并发的关键技术；
Stream⾥可以包含1个或多个Message， Message对应HTTP/1中的请求或响应，由HTTP头部和包体构成；
Message ⾥包含⼀条或者多个Frame， Frame是HTTP/2最⼩单位，以⼆进制压缩格式存放 HTTP/1 中的内容（头部和包体）；
不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ） ，因为每个帧的头部会携带 Stream ID 信息，
所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，⽽同⼀ Stream 内部的帧必须是严格有序的。

客户端和服务器双⽅都可以建⽴Stream， Stream ID也是有区别的，客户端建⽴的Stream必须是奇数号，
⽽服务器建⽴的 Stream 必须是偶数号。
同⼀个连接中的 Stream ID 是不能复⽤的，只能顺序递增，所以当 Stream ID 耗尽时，需要发⼀个控制帧⽤来关闭TCP连接。
HTTP/2通过Stream实现的并发，⽐HTTP/1.1通过TCP连接实现并发要快的多，因为当 HTTP/2 实现100个并发Stream时，
只需要建⽴⼀次 TCP 连接，⽽ HTTP/1.1 需要建⽴ 100 个 TCP 连接，每个 TCP 连接都要经过TCP 握⼿、慢启动
以及 TLS 握⼿过程，这些都是很耗时的。
HTTP/2 还可以对每个 Stream 设置不同优先级，帧头中的「标志位」可以设置优先级，
⽐如客户端访问HTML/CSS 和图⽚资源时，希望服务器先传递 HTML/CSS，再传图⽚，
那么就可以通过设置 Stream 的优先级来实现，以此提⾼⽤户体验。

#### 60.4.2.4 服务器主动推送资源

##### 60.4.2.4.1 http 2.0 主动推送如何实现的

客户端发起的请求，必须使⽤的是奇数号 Stream，服务器主动的推送，使⽤的是偶数号 Stream。
服务器在推送资源时，会通过 PUSH_PROMISE 帧传输 HTTP 头部，并通过帧中的 Promised Stream ID 字段告知客户端，
接下来会在哪个偶数号 Stream 中发送包体。
比如说在Stream1中通知客户端CSS资源即将到来，然后在Stream2中发送CSS资源，Stream1和Stream2可以并发的。

### 60.4.3 HTTP 2有哪些缺陷

1. 队头阻塞，HTTP/2 多个请求是跑在⼀个TCP连接中的，那么当TCP丢包时，整个TCP都要等待重传，那么就会阻塞该
   TCP 连接中的所有请求。因为TCP是字节流协议，TCP层必须保证收到的字节数据是完整且有序的，
   如果序列号较低的TCP段在⽹络传输中丢失了，即使序列号较⾼的TCP段已经被接收了，
   应⽤层也⽆法从内核中读取到这部分数据，从 HTTP 视⻆看，就是请求被阻塞了。
   比如说发送⽅发送了很多个packet，每个packet都有⾃⼰的序号，也可以认为是TCP的序列号，
   假设packet3在⽹络中丢失了，即使packet4-6被接收⽅收到后，由于内核中的TCP数据不是连续的，
   于是接收⽅的应⽤层就⽆法从内核中读取到，只有等到packet3重传后，接收⽅的应⽤层才可以从内核中读取到数据，
   这就是 HTTP/2 的队头阻塞问题，是在 TCP 层⾯发⽣的。
2. TCP 与 TLS 的握⼿时延迟；
   发起 HTTP 请求时，需要经过 TCP 三次握⼿和 TLS 四次握⼿（TLS 1.2）的过程，
   因此共需要 3 个 RTT 的时延才能发出请求数据。
   还有就是TCP 由于具有「拥塞控制」的特性，所以刚建⽴连接的 TCP 会有个「慢启动」的过程，它会对 TCP 连接
   产⽣"减速"效果。
2. ⽹络迁移需要重新连接；
   ⼀个 TCP 连接是由四元组（源 IP 地址，源端⼝，⽬标 IP 地址，⽬标端⼝）确定的，这意味着如果 IP 地址或者端
   ⼝变动了，就会导致需要 TCP 与 TLS 重新握⼿，这不利于移动设备切换⽹络的场景，⽐如 4G ⽹络环境切换成WIFI。

### 60.4.4 HTTP 2做了什么优化

放弃 TCP 协议，转⽽使⽤ UDP 协议作为传输层议，HTTP/3 协议就是这样做的

## 60.5 HTTP-3

### 60.5.1 QUIC协议

#### 60.5.1.1 特点

1. ⽆队头阻塞；
   QUIC 连接上的多个Stream之间并没有依赖，都是独⽴的，某个流发⽣丢包了，只会影响该流，其他流不受影响
   QUIC协议是可以在同⼀条连接上并发传输多个Stream，Stream可以认为就是⼀条HTTP请求。
   由于 QUIC 使⽤的传输协议是 UDP， UDP 不关⼼数据包的顺序，如果数据包丢失， UDP 也不关⼼。
   不过 QUIC 协议会保证数据包的可靠性，每个数据包都有⼀个序号唯⼀标识。当某个流中的⼀个数据包丢失了，
   即使该流的其他数据包到达了，数据也⽆法被 HTTP/3 读取，直到 QUIC 重传丢失的报⽂，数据才会交给 HTTP/3。
   ⽽其他流的数据报⽂只要被完整接收， HTTP/3 就可以读取到数据。

2. 更快的连接建⽴；
   QUIC 内部包含了 TLS，它在⾃⼰的帧会携带 TLS ⾥的“记录”，再加上 QUIC 使⽤的是 TLS1.3，
   因此仅需 1 个 RTT 就可以「同时」完成建⽴连接与密钥协商，甚⾄在第⼆次连接的时候，
   应⽤数据包可以和 QUIC 握⼿信息（连接信息 + TLS 信息）⼀起发送，达到 0-RTT 的效果。

3. 连接迁移；
   QUIC 协议没有⽤四元组的⽅式来“绑定”连接，⽽是通过连接 ID来标记通信的两个端点，
   客户端和服务器可以各⾃选择⼀组 ID 来标记⾃⼰，因此即使移动设备的⽹络变化后，导致 IP 地址变化了，
   只要仍保有上下⽂信息（⽐如连接 ID、 TLS 密钥等），就可以“⽆缝”地复⽤原连接，消除重连的成本，
   没有丝毫卡顿感，达到了连接迁移的功能。

### 60.5.1 HTTP 3做了哪些优化

1. 不需要在二进制帧定义Stream，直接使用QUIC的Stream
2. 使用了QPACK算法，静态表扩大到了91项
                  动态表解码方式发生了变化，在⾸次请求-响应后，双⽅会将未包含在静态表中的Header项
                  更新各⾃的动态表，接着后续传输时仅⽤1个数字表示，然后对⽅可以根据这1个数字
                  从动态表查到对应的数据，就不必每次都传输⻓⻓的数据，⼤⼤提升了编码效率。

#### 60.5.1.1 如果HPACK头部首次传递丢包，如何解决

QUIC 会有两个特殊的单向流，所谓的单项流只有⼀端可以发送消息，双向则指两端都可以发送消息，
传输 HTTP消息时⽤的是双向流，这两个单向流的⽤法：
1. ⼀个叫 QPACK Encoder Stream， ⽤于将⼀个字典（key-value）传递给对⽅，
   ⽐如⾯对不属于静态表的HTTP 请求头部，客户端可以通过这个 Stream 发送字典；
2. ⼀个叫 QPACK Decoder Stream，⽤于响应对⽅，告诉它刚发的字典已经更新到⾃⼰的本地动态表了，
   后续就可以使⽤这个字典来编码了。
这两个特殊的单向流是⽤来同步双⽅的动态表，编码⽅收到解码⽅更新确认的通知后，才使⽤动态表编码 HTTP 头部。

# 61.HTTPS

## 61.1 什么是HTTPS

HTTPS是在HTTP上建立SSL加密层，并对传输数据进行加密，是HTTP协议的安全版

## 61.2 HTTP与HTTPS区别

1. HTTP 是超⽂本传输协议，信息是明⽂传输，存在安全⻛险的问题。 
   HTTPS 则解决 HTTP 不安全的缺陷，在TCP 和 HTTP ⽹络层之间加⼊了 SSL/TLS 安全协议，使得报⽂能够加密传输。
2. HTTP 连接建⽴相对简单， TCP 三次握⼿之后便可进⾏ HTTP 的报⽂传输。
   HTTPS 在 TCP 三次握⼿之后，还需进⾏ SSL/TLS 的握⼿过程，才可进⼊加密报⽂传输。
3. HTTP 的端⼝号是 80， HTTPS 的端⼝号是 443。
4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。

### 61.2.1 为何不所有的网站都使用HTTPS

1. 首先，需要权威CA颁发的SSL证书。
   从证书的选择、购买到部署，传统的模式下都会比较耗时耗力。
2. 其次，HTTPS普遍认为性能消耗要大于HTTP，
   因为与纯文本通信相比，加密通信会消耗更多的CPU及内存资源。
   如果每次通信都加密，会消耗相当多的资源，
   平摊到一台计算机上时，
   能够处理的请求数量必定也会随之减少。
   但事实并非如此，
   用户可以通过性能优化、把证书部署在SLB或CDN，
   来解决此问题。
3. 除此之外，想要节约购买证书的开销也是原因之一。
   要进行HTTPS通信，证书是必不可少的。
   而使用的证书必须向认证机构（CA）购买。

## 61.3 HTTPS 解决了 HTTP 的哪些问题？（为什么要HTTPS）

HTTP 由于是明⽂传输，所以安全上存在以下三个⻛险：
1. 通信使⽤明⽂（不加密），内容可能会被窃听。⽐如， 账号信息容易泄漏，那你号没了。
2. 不验证通信⽅的身份，因此有可能遭遇伪装。⽐如， 访问假的淘宝、拼多多，那你钱没了。
3. ⽆法证明报⽂的完整性，所以有可能已遭篡改。⽐如， ⽹⻚上植⼊垃圾⼴告，视觉污染，眼没了。

所以

HTTPS 在 HTTP 与 TCP 层之间加⼊了 SSL/TLS 协议，可以很好的解决了上述的⻛险：
1. 混合加密的⽅式实现信息的机密性，解决了窃听的⻛险。
   HTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：
   在通信建立 前 采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密；
   在通信过程中 全部使用对称加密的「会话秘钥」的方式加密明文数据。
   采用「混合加密」的方式的原因：
      对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
      非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。
2. 摘要算法的⽅式来实现完整性，它能够为数据⽣成独⼀⽆⼆的「指纹」，
   指纹⽤于校验数据的完整性，解决了篡改的⻛险。
   客户端在发送明文之前会通过摘要算法算出明文的「指纹」，
   发送的时候把「指纹 + 明文」一同加密成密文后，发送给服务器，
   服务器解密后，用相同的摘要算法算出发送过来的明文，
   通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，
   若「指纹」相同，说明数据是完整的。
3. 将服务器公钥放⼊到数字证书中，解决了冒充的⻛险。
   客户端先向服务器端索要公钥，然后⽤公钥加密信息，服务器收到密⽂后，⽤⾃⼰的私钥解密。

## 61.4 加密算法

### 61.4.1 分类

1. 对称加密：密钥只有⼀个，加密解密为同⼀个密码，且加解密速度快，典型的对称加密，算法有DES、 AES等；
   - 优点：算法公开、计算量小、加密速度快、加密效率高，适合加密比较大的数据。
   - 缺点：
          交易双方需要使用相同的密钥，也就无法避免密钥的传输，
          而密钥在传输过程中无法保证不被截获，因此对称加密的安全性得不到保证。
          每对用户每次使用对称加密算法时，都需要使用其他人不知道的惟一密钥，
          这会使得发收信双方所拥有的钥匙数量急剧增长，密钥管理成为双方的负担。
          对称加密算法在分布式网络系统上使用较为困难，
          主要是因为密钥管理困难，使用成本较高。

2. ⾮对称加密：密钥成对出现（且根据公钥⽆法推知私钥，根据私钥也⽆法推知公钥），
               加密解密使⽤不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），
               相对对称加密速度较慢，典型的⾮对称加密算法有RSA、 DSA等。

   - 优点：算法公开，加密和解密使用不同的钥匙，私钥不需要通过网络进行传输，安全性很高。
   - 缺点：计算量比较大，加密和解密速度相比对称加密慢很多。

### 61.4.2 Https对称加解密的过程

发送端和接收端首先要共享相同的密钥k
即通信前双方都需要知道对应的密钥才能进行通信。
发送端用共享密钥k对明文p进行加密，得到密文c，
并将得到的密文发送给接收端，接收端收到密文后，
并用其相同的共享密钥k对密文进行解密，得出明文p。

### 61.4.3 Https非对称加密过程

加密一方找到接收方的公钥e，
大部分的公钥查找工作实际上都是通过数字证书来实现的，
然后用公钥e对明文p进行加密后得到密文c，
并将得到的密文发送给接收方，接收方收到密文后，
用自己保留的私钥d进行解密，得到明文p，
用公钥加密的密文，只有拥有私钥的一方才能解密，
这样就可以解决加密的各方可以统一使用一个公钥即可。

## 61.5 摘要算法

## 61.6 数字证书

### 61.6.1 数字证书包含什么

1. 公钥；
2. 持有者信息；
3. 证书认证机构（CA）的信息；
4. CA 对这份⽂件的数字签名及使⽤的算法；
5. 证书有效期；
6. 还有⼀些其他额外信息；

### 61.6.2 作用

是⽤来认证公钥持有者的身份，以防⽌第三⽅进⾏冒充。
也就是证书就是⽤来告诉客户端，该服务端是否是合法的，因为只有证书合法，才代表服务端身份是可信的。

### 61.6.3 证书怎么来的

为了让服务端的公钥被⼤家信任，服务端的证书都是由CA（证书认证机构）签名的， CA就是⽹络世界⾥的公安局、公证中⼼，具有极⾼的可信度，
所以由它来给各个公钥签名，信任的⼀⽅签发的证书，那必然证书也是被信任的。
之所以要签名，是因为签名的作⽤可以避免中间⼈在获取证书时对证书内容的篡改。

### 61.6.4 数字证书的流程

分为了证书签名和客户端校验过程

CA 签发证书的过程的话：
   1. ⾸先 CA 会把持有者的公钥、⽤途、颁发者、有效时间等信息打成⼀个包，
      然后对这些信息进⾏ Hash 计算，得到⼀个 Hash 值；
   2. 然后 CA 会使⽤⾃⼰的私钥将该 Hash 值加密，⽣成 Certificate Signature，也就是 CA 对证书做了签名；
      最后将 Certificate Signature 添加在⽂件证书上，形成数字证书；

客户端校验服务端的数字证书的过程，
   1. ⾸先客户端会使⽤同样的 Hash 算法获取该证书的 Hash 值 H1；
   2. 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使⽤ CA 的公钥解密 Certificate
      Signature 内容，得到⼀个 Hash 值 H2 ；
   3. 最后⽐较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。

### 61.6.5 证书链

证书的验证过程中还存在⼀个证书信任链的问题，因为我们向 CA 申请的证书⼀般不是根证书签发的，⽽是由中间证书签发的，⽐如百度的证书，
会有一个三级层级关系，跟证书、中间证书、百度的证书
比如说百度证书

1. 客户端收到 baidu.com 的证书后，发现这个证书的签发者不是根证书，就⽆法根据本地已有的根证书中的公钥去验证baidu.com证书是否可信。
   于是，客户端根据 baidu.com 证书中的签发者，找到该证书的颁发机构是，比如说是google吧，然后向 CA 请求该中间证书。
2. 请求到证书后发现 google 证书是由 xxx签发的，由于 xxx 没有再上级签发机构，说明它是根证书，也就是⾃签证书。
   应⽤软件会检查此证书有否已预载于根证书清单上，如果有，则可以利⽤根证书中的公钥去验证 google 证书，如果发现验证通过，
   就认为该中间证书是可信的。google证书被信任后，可以使⽤ google 证书中的公钥去验证 baidu.com 证书的可信性，
   如果验证通过，就可以信任baidu.com 证书。

#### 61.6.5.1 为什么要有证书链

为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。

## 61.7 SSL/TLS协议

### 61.7.1 工作流程

1. 客户端向服务器索要并验证服务器的公钥。
2. 双⽅协商⽣产「会话秘钥」。
3. 双⽅采⽤「会话秘钥」进⾏加密通信。

### 61.7.2 四次握手分类

由于不同的密钥交换算法，TLS 的握⼿过程可能会有⼀些区别

1. RSA算法
2. ECDHE 算法

### 61.7.3 RSA的四次握手

#### 61.7.3.1 四次握手流程

第一次：客户端⾸先会发⼀个「Client Hello」消息
      消息⾥⾯有客户端使⽤的 TLS 版本号、⽀持的密码套件列表，以及⽣成的随机数（Client Random），这个随机
      数会被服务端保留，它是⽣成对称加密密钥的材料之⼀。

第二次：当服务端收到客户端的「Client Hello」消息后，会确认 TLS 版本号是否⽀持，和从密码套件列表中选择⼀个密码套件，
       以及⽣成随机数（Server Random）。
       接着，返回「Server Hello」消息，消息⾥⾯有服务器确认的 TLS 版本号，也给出了随机数（Server Random），
       然后从客户端的密码套件列表选择了⼀个合适的密码套件。基本的形式是「密钥交换算法 +签名算法 + 对称加密算法 + 摘要算法」
       然后，服务端为了证明⾃⼰的身份，会发送「Server Certificate」给客户端，这个消息⾥含有数字证书。
       随后，服务端发了「Server Hello Done」消息，⽬的是告诉客户端，我已经把该给你的东⻄都给你了，本次打招呼完毕。

客户端拿到了服务端的数字证书后，要校验该数字证书是真实有效

第三次握手，客户端验证完证书后，认为可信则继续往下⾛。接着，客户端就会⽣成⼀个新的随机数，⽤服务器的RSA公钥加密该随机数，
          通过「Change Cipher Key Exchange」消息传给服务端。服务端收到后，⽤RSA私钥解密，
          得到客户端发来的随机数。客户端和服务端双⽅都共享了三个随机数，分别是Client Random、Server Random、pre-master。
          于是，双⽅根据已经得到的三个随机数，⽣成会话密钥（Master Secret），它是对称密钥，⽤于对后续的HTTP请求/响应的数据加解密。
          ⽣成完会话密钥后，然后客户端发⼀个「Change Cipher Spec」，告诉服务端开始使⽤加密⽅式发送消息。

第四次握手，TLS 第四次握⼿服务器也是同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，
          如果双⽅都验证加密和解密没问题，那么握⼿正式完成。最后，就⽤「会话密钥」加解密 HTTP 请求和响应了。

#### 61.7.3.2 缺点

使⽤ RSA 密钥协商算法的最⼤问题是不⽀持前向保密。因为客户端传递随机数（⽤于⽣成对称加密密钥的条件之
⼀）给服务端时使⽤的是公钥加密的，服务端收到到后，会⽤私钥解密得到随机数。所以⼀旦服务端的私钥泄漏
了，过去被第三⽅截获的所有 TLS 通讯密⽂都会被破解。

为了解决这⼀问题，于是就有了 DH 密钥协商算法

客户端和服务端各⾃会⽣成随机数，并以此作为私钥，然后根据公开的 DH 计算公示算出各⾃的公钥，通过 TLS
握⼿双⽅交换各⾃的公钥，这样双⽅都有⾃⼰的私钥和对⽅的公钥，然后双⽅根据各⾃持有的材料算出⼀个随机
数，这个随机数的值双⽅都是⼀样的，这就可以作为后续对称加密时使⽤的密钥。
DH 密钥交换过程中，即使第三⽅截获了 TLS 握⼿阶段传递的公钥，在不知道的私钥的情况下，也是⽆法计算出
密钥的，⽽且每⼀次对称加密密钥都是实时⽣成的，实现前向保密。
但因为 DH 算法的计算效率问题，会有其他算法升级

### 61.7.4 ECDHE的四次握手

#### 61.7.4.1 四次握手流程

使⽤了ECDHE，在 TLS 第四次握⼿前，客户端就已经发送了加密的 HTTP 数据，所以， ECDHE 相⽐ RSA 握⼿过程省去了⼀个消息往返的时间

第一次握手，客户端⾸先会发⼀个Client Hello消息，消息⾥⾯有客户端使⽤的TLS版本号、⽀持的密码套件列表，以及⽣成的随机数
第二次握手，服务端收到客户端的「打招呼」，同样也要回礼，会返回「Server Hello」消息，消息⾯有服务器确认的 TLS 版本号，
          也给出了⼀个随机数 ，然后从客户端的密码套件列表选择了⼀个合适的密码套件。不过这次套件内容发生了变化：
          1. 密钥协商算法使⽤ ECDHE；
          2. 签名算法使⽤ RSA；
          3. 握⼿后的通信使⽤ AES 对称算法，密钥⻓度 256 位，分组模式是 GCM；
          4. 摘要算法使⽤ SHA384；

接着，服务端为了证明⾃⼰的身份，发送「Certificate」消息，会把证书也发给客户端。因为服务端选择了 ECDHE 密钥协商算法，
所以会在发送完证书后，发送「Server Key Exchange」消息。然后选择了名为 named_curve 的椭圆曲线，
选好了椭圆曲线相当于椭圆曲线基点 G 也定好了，这些都会公开给客户端；⽣成随机数作为服务端椭圆曲线的私钥，保留到本地；
根据基点 G 和私钥计算出服务端的椭圆曲线公钥，这个会公开给客户端。为了保证这个椭圆曲线的公钥不被第三⽅篡改，
服务端会⽤ RSA 签名算法给服务端的椭圆曲线公钥做个签名。随后，就是「Server Hello Done」消息，
服务端跟客户端表明： “这些就是我提供的信息，打招呼完毕”。
TLS 两次握⼿就已经完成了，⽬前客户端和服务端通过明⽂共享了这⼏个信息： Client Random、 Server Random 、使⽤的椭圆曲线、
椭圆曲线基点 G、服务端椭圆曲线的公钥，这⼏个信息很重要，是后续⽣成会话密钥的材料。

第三次握手，客户端收到了服务端的证书后，⾃然要校验证书是否合法，如果证书合法，那么服务端到身份就是没问题的。校验证书到过程，
          会⾛证书链逐级验证，确认证书的真实性，再⽤证书的公钥验证签名，这样就能确认服务端的身份了，确认⽆误后，就可以继续往下⾛。
          客户端会⽣成⼀个随机数作为客户端椭圆曲线的私钥，然后再根据服务端前⾯给的信息，⽣成客户端的椭圆曲线公钥，
          然后⽤「Client Key Exchange」消息发给服务端。⾄此，双⽅都有对⽅的椭圆曲线公钥、⾃⼰的椭圆曲线私钥、椭圆曲线基点 G。
          于是，双⽅都就计算出点（x，y），其中 x 坐标值双⽅都是⼀样的，x 是会话密钥，
          最终的会话密钥，就是⽤「客户端随机数 + 服务端随机数 + x（ECDHE 算法算出的共享密钥） 」三个材料⽣成的。
          `之所以这么麻烦，是因为 TLS 设计者不信任客户端或服务器「伪随机数」的可靠性，为了保证真正的完全随机，`
          `把三个不可靠的随机数混合起来，那么「随机」的程度就⾮常⾼了，⾜够让⿊客计算出最终的会话密钥，安全性更⾼。`
          算好会话密钥后，客户端会发⼀个「Change Cipher Spec」消息，告诉服务端后续改⽤对称算法加密通信。
          接着，客户端会发「Encrypted Handshake Message」消息，把之前发送的数据做⼀个摘要，再⽤对称密钥加密⼀下，
          让服务端做个验证，验证下本次⽣成的对称密钥是否可以正常使⽤。

第四次握手，最后，服务端也会有⼀个同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，
          如果双⽅都验证加密和解密没问题，那么握⼿正式完成。于是，就可以正常收发加密的 HTTP 请求和响应了。

#### 61.7.5.2 ECDHE 算法

ECDHE 算法是在 DHE 算法的基础上利⽤了 ECC 椭圆曲线特性，可以⽤更少的计算量计算出公钥，以及最终的会话密钥。

ECDHE 密钥交换算法的过程
1. 双⽅事先确定好使⽤哪种椭圆曲线，和曲线上的基点G，这两个参数都是公开的
2. 双⽅各⾃随机⽣成⼀个随机数作为私钥d，并与基点 G相乘得到公钥Q（Q = dG），此时a的公私钥为 Q1和 d1，b的公私钥为 Q2 和 d2；
3. 双⽅交换各⾃的公钥，最后⼩红计算点（x1， y1） = d1Q2，⼩明计算点（x2， y2） = d2Q1，
   由于椭圆曲线上是可以满⾜乘法交换和结合律，所以 d1Q2 = d1d2G = d2d1G = d2Q1 ，因此双⽅的 x 坐标是⼀样的，
   所以它是共享密钥，也就是会话密钥。

这个过程中，双⽅的私钥都是随机、临时⽣成的，都是不公开的，即使根据公开的信息（椭圆曲线、公钥、基点G）
也是很难计算出椭圆曲线上的离散对数（私钥）





### 61.7.5 RSA和ECDHE握⼿过程的区别

1. RSA 密钥协商算法「不⽀持」前向保密， ECDHE 密钥协商算法「⽀持」前向保密；
2. 使⽤了 RSA 密钥协商算法， TLS 完成四次握⼿后，才能进⾏应⽤数据传输，⽽对于 ECDHE 算法，
   客户端可以不⽤等服务端的最后⼀次TLS 握⼿，就可以提前发出加密的 HTTP 数据，节省了⼀个消息的往返时间；
3. 使⽤ ECDHE， 在 TLS 第 2 次握⼿中，会出现服务器端发出的「Server Key Exchange」消息，⽽ RSA 握⼿过程没有该消息；

## 61.8 https性能消耗

### 61.8.1 https在哪里有性能损耗

1. TLS 协议握⼿过程
2. 握⼿后的对称加密报⽂传输

因为TLS 协议握⼿过程；TLS 协议握⼿过程不仅增加了⽹络延时（最⻓可以花费掉 2 RTT），
⽽且握⼿过程中的⼀些步骤也会产⽣性能损耗，⽐如：
   对于 ECDHE 密钥协商算法，握⼿过程中会客户端和服务端都需要临时⽣成椭圆曲线公私钥；
   客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，⽬的是验证服务器的证书是否有被吊销；
   双⽅计算 Pre-Master，也就是对称加密密钥；

## 61.9 HTTPS优化

1. 对于硬件优化的⽅向，因为 HTTPS 是属于计算密集型，应该选择计算⼒更强的 CPU，⽽且最好选择⽀持 AES-NI特性的 CPU，
   这个特性可以在硬件级别优化 AES 对称加密算法，加快应⽤数据的加解密。

2. 对于软件优化的⽅向，如果可以，把软件升级成较新的版本，⽐如将 Linux 内核 2.X 升级成 4.X，将 openssl 1.0.1升级到 1.1.1，
   因为新版本的软件不仅会提供新的特性，⽽且还会修复⽼版本的问题。

3. 对于协议优化的⽅向：
   密钥交换算法应该选择 ECDHE 算法，⽽不⽤ RSA 算法，因为 ECDHE 算法具备前向安全性，⽽且客户端可以在第三次握⼿之后
   就发送加密应⽤数据，节省了 1 RTT。
   将 TSL1.2 升级 TSL1.3，因为 TSL1.3 的握⼿过程只需要 1 RTT，⽽且安全性更强。

4. 对于证书优化的⽅向：
   服务器应该选⽤ ECDSA 证书，⽽⾮ RSA 证书，因为在相同安全级别下， ECC 的密钥⻓度⽐ RSA 短很多，这样可以提⾼证书传输的效率；
   服务器应该开启 OCSP Stapling 功能，由服务器预先获得 OCSP 的响应，并把响应结果缓存起来，
   这样TLS 握⼿的时候就不⽤再访问 CA 服务器，减少了⽹络通信的开销，提⾼了证书验证的效率；
   对于重连 HTTPS 时，我们可以使⽤⼀些技术让客户端和服务端使⽤上⼀次 HTTPS 连接使⽤的会话密钥，直接恢复会话，
   ⽽不⽤再重新⾛完整的 TLS 握⼿过程。

5. 常⻅的会话重⽤技术有 Session ID 和 Session Ticket，⽤了会话重⽤技术，当再次重连 HTTPS 时，只需要 1 RTT就可以恢复会话。
   对于 TLS1.3 使⽤ Pre-shared Key 会话重⽤技术，只需要 0 RTT 就可以恢复会话。
   这些会话重⽤技术虽然好⽤，但是存在⼀定的安全⻛险，它们不仅不具备前向安全，⽽且有重放攻击的⻛险，所以
   应当对会话密钥设定⼀个合理的过期时间。

# 62.拆包粘包

## 62.1 粘包

### 62.1.1 概念

TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，
从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，
例如基于tcp的套接字客户端往服务端上传文件，发送时文件内容是按照一段一段的字节流发送的，
在接收方看了，根本不知道该文件的字节流从何处开始，在何处结束
所谓粘包问题主要还是因为接收方不知道消息之间的界限，不知道一次性提取多少字节的数据所造成的。

### 62.1.2 粘包的原因

1. 发送方原因
   TCP默认使用Nagle算法（主要作用：会将数据量小的，且时间间隔较短的数据一次性发给对方），
   而Nagle算法主要做两件事：
   只有上一个分组得到确认，才会发送下一个分组
   收集多个小分组，在一个确认到来时一起发送
   Nagle算法造成了发送方可能会出现粘包问题
2. 接收方原因
   TCP接收到数据包时，并不会马上交到应用层进行处理，
   或者说应用层并不会立即处理。实际上，TCP将接收到的数据包保存在接收缓存里，
   然后应用程序主动从缓存读取收到的分组。
   这样一来，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，
   多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。

### 62.1.3 粘包解决方案

（1）发送方
   对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，
   使用TCP_NODELAY选项来关闭算法。
（2）接收方
   接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。
（2）应用层
   解决办法：循环处理，应用程序从接收缓存中读取分组时，
   读完一条数据，就应该循环读取下一条数据，
   直到所有数据都被处理完成，之后开始处理每条数据的长度
   因为每条数据有固定的格式（开始符，结束符），
   但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。
   发送每条数据时，将数据的长度一并发送，
   例如规定数据的前4位是数据的长度，
   应用层在处理时可以根据长度来判断每个分组的开始和结束位置。

### 621.1.4 UDP会不会产生粘包问题呢

UDP不会发生粘包拆包现象
UDP则是面向消息传输的，是有保护消息边界的，
接收方一次只接受一条独立的信息，所以不存在粘包问题。
保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，
接收端一次只能接受一条独立的消息
比如说，有三个数据包，大小分别为2k、4k、6k，如果采用UDP发送的话，
不管接受方的接收缓存有多大，我们必须要进行至少三次以上的发送才能把数据包发送完，
但是使用TCP协议发送的话，我们只需要接受方的接收缓存有12k的大小，
就可以一次把这3个数据包全部发送完毕。

## 62.2 拆包




## 62.3 HTTP拆包粘包

一个有报文的请求到服务器时，请求头里都会有content_length，这个指定了报文的大小，
报文如果很大的时候，会通过一部分一部分的发送请求，直到结束，
当这个过程中，出现多个请求，第一个请求会带有请求头信息，前面一个请求的如果发送的报文如果没有满时，
会把后面一个请求的内容填上，这个操作就叫粘包。
这样粘包后，它会通过content_length字段的大小，来做拆包。

# 63.Cookie与Session和token

## 63.1 Cookie

### 63.1.1 概念

### 63.1.2 作用

cookie是服务路发送到用户浏览器并保存在本地的小快数据，
它会在浏览器之后向同一服务器再次发起请求时被携带上，
用于告知服务端两个请求是否来自同一浏览器

### 63.1.3 Cookie被禁⽤怎么办

最常⽤的就是利⽤ URL 重写把 Session ID 直接附加在URL路径的后⾯。

### 63.1.4 cookie被禁用了，session还能用么

能用，但没有使用cookie那么安全。cookie没有被禁用的时候，浏览器向服务器发送请求时，
会自动带上cookie。SESSIONID放在cookie里面，所以服务器可以根据SESSIONID找到相应的session文件。
cookie被禁用时，还可以通过get、post参数来向服务器提供SESSIONID。php支持通过URL 参数 来向服务器提供SESSIONID。
但是要设置php.ini文件

### 63.1.15 cookie如何放置攻击

需要在HTTP头部配上，set-cookie：httponly-。这个属性可以防止XSS,
它会禁止javascript脚本来访问cookie。

## 63.2 Session

### 63.2.1 概念

### 63.2.2 作用

### 63.2.3 HTTP是不保存状态的协议,如何保存⽤户状态

通过Session机制解决，Session的主要作⽤就是通过服务端记录⽤户的状态。

如应用场景购物⻋，当你要添加商品到购物⻋的时候，系统不知道是哪个⽤户操作的，
因为HTTP协议是⽆状态的。服务端给特定的⽤户创建特定Session之后就可以标
识这个⽤户并且跟踪这个⽤户了（⼀般情况下，服务器会在⼀定时间内保存这个Session，
过了时间限制，就会销毁这个Session）

### 62.2.4 Session用户登录状态过程

1. 用户进行登录时，用户提交包含用户名和密码的表单，放入HTTP请求报文中，
2. 服务器验证该用户名和密码，
         如果正确则把用户信息存储到Redis中，
         它在Redis中Key称为Session ID:
3. 服务器返回的响应报文的Se-Coeo首部字段包含了这个Session ID.
         客户端收到响应报文之后将该Cookie值存入浏览器中:
4. 客户编之后对间一 个服务器进行请求时会包含该Cookie值，
       服务器收到之后提取出Session ID.从Redis中取出用户信息，
       维续之前的业务操作。

### 62.2.5 如何保存session

在服务端保存Session的⽅法很多，
最常⽤的就是内存和数据库(⽐如是使⽤内存数据库redis保存)。

### 62.2.6 如何实现 Session 跟踪呢？

⼤部分情况下，我们都是通过在Cookie 中附加⼀个 Session ID 来⽅式来跟踪。

### 62.2.7 Session机制（多个session如何识别）

session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构
（也可能就是使用散列表）来保存信息。
当程序需要为某个客户端的请求创建一个session的时候，
服务器首先检查这个客户端的请求里是否已包含了一个session标识
称为session id，
如果已包含一个session id则说明以前已经为此客户端创建过session，
服务器就按照session id把这个session检索出来使用
（如果检索不到，可能会新建一个），
如果客户端请求不包含session id，则为此客户端创建一个session
并且生成一个与此session相关联的session id，
session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，
这个session id将被在本次响应中返回给客户端保存。

## 63.3 token

### 63.3.1 概念

### 63.3.2 作用

### 63.3.3 机制/流程

1. 客户端使用用户名跟密码请求登录
2. 服务端收到请求，去验证用户名与密码
3. 验证成功后，服务端会签发一个 token 并把这个 token 发送给客户端
4. 客户端收到 token 以后，会把它存储起来，比如放在 cookie 里或者 localStorage 里
5. 客户端每次向服务端请求资源的时候需要带着服务端签发的 token
6. 服务端收到请求，然后去验证客户端请求里面带着的 token ，如果验证成功，就向客户端返回请求的数据

## 63.4 区别

### 63.4.1 Cookie 和 Session 的区别

1. 安全性： Session 比 Cookie 安全，Session 是存储在服务器端的，Cookie 是存储在客户端的。
2. 存取值的类型不同：Cookie 只支持存字符串数据，想要设置其他类型的数据，
                     需要将其转换成字符串，Session 可以存任意数据类型。
3. 有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，
                Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效。
4. 存储大小不同： 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie，
                 但是当访问量过多，会占用过多的服务器资源。

### 64.4.2 token和cookie实现的区别

1. Session 是一种记录服务器和客户端会话状态的机制，使服务端有状态化，可以记录会话信息。而Token 是令牌
   访问资源接口（API）时所需要的资源凭证。Token 使服务端无状态化，不会存储会话信息。
2. Token每一个请求都有签名还能防止监听以及重放攻击，而Session就必须依赖链路层来保障通讯安全了。
3. 如果你的用户数据可能需要和第三方共享，或者允许第三方调用 API 接口，用Token 。
   如果永远只是自己的网站，自己的 App，用什么就无所谓了。

# 64.输入网址

## 64.1 输入网址牵扯到的技术

1. HTTP
2. DNS
3. 协议栈
4. TCP
5. IP
6. MAC
7. 网卡
8. 交换机
9. 路由器 

## 64.1 输入网址过程

1. 输入地址,对URL进⾏解析，从⽽⽣成发送给Web服务器的请求信息。
2. 浏览器查找域名的IP地址,因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。
   1) 浏览器会首先查看本地硬盘的hosts文件，看看其中有没有和这个域名对应的规则，
      如果有的话就直接使用hosts文件里面的ip地址。
   2) 如果在本地的hosts文件没有能够找到对应的ip地址，浏览器会发出一个DNS请求到本地DNS服务器
   3) 查询你输入的网址的DNS请求到达本地DNS服务器之后，本地DNS服务器会首先查询它的缓存记录，如果缓存中有此条记录，
      就可以直接返回结果，此过程是递归的方式进行查询。如果没有，本地DNS服务器还要向DNS根服务器进行查询。
   4) 根DNS服务器没有记录具体的域名和IP地址的对应关系，而是告诉本地DNS服务器，你可以到域服务器上去继续查询，
      并给出域服务器的地址。这种过程是迭代的过程。
   5) 本地DNS服务器继续向域服务器发出请求，比如说请求的对象是.com域服务器。
      .com域服务器收到请求之后，也不会直接返回域名和IP地址的对应关系，而是告诉本地DNS服务器，你的域名的解析服务器的地址
   6) 最后，本地DNS服务器向域名的解析服务器发出请求，这时就能收到一个域名和IP地址对应关系，
      本地DNS服务器不仅要把IP地址返回给用户电脑，还要把这个对应关系保存在缓存中，
      以备下次别的用户查询时，可以直接返回结果，加快网络访问。
3. 浏览器向web服务器发送一个HTTP请求
      通过DNS获取到IP后，就可以把HTTP的传输⼯作交给操作系统中的协议栈。协议栈的内部分为⼏个部分，分别承担不同的⼯作。
      上下关系是有⼀定的规则的，上⾯的部分会向下⾯的部分委托⼯作，下⾯的部分收到委托的⼯作并执⾏。
      应⽤程序也就是浏览器通过调⽤ Socket 库，来委托协议栈⼯作。协议栈的上半部分有两块，分别是负责收发数据的 TCP 和 UDP 协议，
      它们两会接受应⽤层的委托执⾏收发数据的操作。协议栈的下⼀半是⽤IP协议控制⽹络包收发操作，
      在互联⽹上传数据时，数据会被切分成⼀块块的⽹络包，⽽将⽹络包发送给对⽅的操作就是由 IP 负责的。
      IP 下⾯的⽹卡驱动程序负责控制⽹卡硬件，
      ⽽最下⾯的⽹卡则负责完成实际的收发操作，
      也就是对⽹线中的信号执⾏发送和接收操作。
      拿到域名对应的IP地址之后，
      浏览器会以一个随机端口向服务器的WEB程序80端口发起TCP的连接请求。
      这个连接请求到达服务器端后，进入到网卡，然后是进入到内核的TCP/IP协议栈，
      还有可能要经过防火墙的过滤，最终到达WEB程序，最终建立了TCP/IP的连接。
4. 服务器的永久重定向响应
      服务器给浏览器响应一个301永久重定向响应，这样浏览器就会访问3w了。
5. 浏览器跟踪重定向地址，
      因为现在浏览器知道了 "http://www.google.com/"才是要访问的正确地址，
      所以它会发送另一个http请求
6. 服务器处理请求
      http请求发送到了服务器，后端从在固定的端口接收到TCP报文开始，
      它会对TCP连接进行处理，对HTTP协议进行解析，
      并按照报文格式进一步封装成HTTP Request对象，供上层使用。
7. 服务器返回一个HTTP响应　
      服务器收到了我们的请求，也处理我们的请求，
      到这一步，它会把它的处理结果返回，也就是返回一个HTPP响应。
8. 浏览器显示 HTML,并请求获取嵌入在HTML的资源

## 64.2 DNS

### 64.2.1 概念

### 64.2.2 作用

### 64.2.3 机制

### 64.2.4 DNS解析超时优化

1. DNS层优化主要是服务器DNS解析增加本地缓存，牺牲DNS解析的时效性。
2. 代码层面优化主要获取本地ip地址后缓存只需要进行一次host解析即可。

## 64.3 URI和URL的区别是什么?

URI的作⽤像身份证号⼀样， URL的作⽤更像家庭住址⼀样。

## 64.4 为什么域名要分级设计

DNS 中的域名都是⽤句点来分隔的，代表了不同层次之间的界限。

域名的层级关系类似⼀个树状结构：
根 DNS 服务器
顶级域 DNS 服务器（com）
权威 DNS 服务器（server.com）

因此，客户端只要能够找到任意⼀台 DNS 服务器，
就可以通过它找到根域 DNS 服务器，然后再⼀路顺
藤摸⽠找到位于下层的某台⽬标 DNS 服务器。

## 64.5 页面跳转方式

### 64.5.1 分类

1. 重定向
2. 转发

### 64.5.2 重定向

#### 64.5.2.1 概念

#### 64.5.2.2 作用

#### 64.5.2.3 重定向原因

1. 网站调整（如改变网页目录结构）；
2. 网页被移到一个新地址；
3. 网页扩展名改变(如应用需要把.php改成.Html或.shtml)。

这种情况下，如果不做重定向，则用户收藏夹或搜索引擎数据库中
旧地址只能让访问客户得到一个404页面错误信息，访问流量白白丧失；
还有就是某些注册了多个域名的网站，
也需要通过重定向让访问这些域名的用户自动跳转到主站点等。

### 64.5.2 转发

#### 64.5.2.1 概念

#### 64.5.2.2 作用

#### 64.5.2.3 转发原因

### 64.5.3 转发与重定向的区别

拿一个例子来说，比如说，我要去盖章，重定向就是
去A部门盖章，A部门并不能解决这件事情，A部门告诉我：“我们并不能解决这件事情，但B部门可以，你去找它吧，B部门在XXXXXXXXXX。”然后我就去找B部门，B部门也解决了小宇的事情。 在这个过程中，小宇问了（请求）两次，得到了两次的答复（响应）。

转发就是我去A部门盖章，A部门并不能解决这件事情，但是B部门可以，于是A部门就帮我把文件拿到B部门那里盖好章，
在拿回来给我。在这个过程中，我一次请求得到了一次响应。


# 65. 各种协议（TCP/UDP/IP/ARP）

## 65.1 TCP

### 65.1.1 概念

IP 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。
如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。
因为 TCP 是一个工作在传输层的可靠数据传输的服务，它能确保接收端接收的网络包是无损坏、无间隔、非冗余和按序的。
一种面向连接(连接导向)的、可靠的、 基于IP的传输层协议

#### 65.1.1.1 为什么TCP面向流

TCP是基于字节流的，虽然应用层和TCP传输层之间的数据交互是大小不等的数据块，
但是TCP把这些数据块仅仅看成一连串无结构的字节流，没有边界；
另外从TCP的帧结构也可以看出，在TCP的首部没有表示数据长度的字段

### 65.1.2 TCP三次握手

#### 65.1.2.1 TCP三次握手流程

客户端–发送带有SYN标志的数据包–一次握手–服务端
服务端–发送带有SYN/ACK标志的数据包–二次握手–客户端
客户端–发送带有带有ACK标志的数据包–三次握手–服务端

##### 65.1.2.1.1 TCP为什么有SYN

接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。

##### 65.1.2.1.2 什么是 SYN 攻击？如何避免 SYN 攻击？

TCP 连接建立是需要三次握手，假设攻击者短时间伪造不同 IP 地址的 SYN 报文，
服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，
但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，
久而久之就会占满服务端的 SYN 接收队列（未连接队列），使得服务器不能为正常用户服务。

如何避免SYN攻击方式：

1. 避免 SYN 攻击方式一
   通过修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理。
   当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。控制该队列的最大值

2. 避免 SYN 攻击方式二
   正常流程下,当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「 SYN 队列」；
   接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文；
   服务端接收到 ACK 报文后，从「 SYN 队列」移除放入到「 Accept 队列」；
   应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。
   但是，如果应用程序过慢时，就会导致「 Accept 队列」被占满。
        如果不断受到 SYN 攻击，就会导致「 SYN 队列」被占满。
   当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」；
   计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端，
   服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」。
   最后应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。

##### 65.1.2.1.3 TCP除了SYN，为什么还要 ACK

双⽅通信⽆误必须是两者互相发送信息都⽆误。传了 SYN，证明发送⽅到接收⽅的通道没有问题，但是
接收⽅到发送⽅的通道还需要 ACK 信号来进⾏验证。

##### 65.1.2.1.4 为什么客户端和服务端的初始序列号ISN是不相同的？

如果⼀个已经失效的连接被重⽤了，但是该旧连接的历史报⽂还残留在⽹络中，如果序列号相同，
那么就⽆法分辨出该报⽂是不是历史报⽂，如果历史报⽂被新的连接接收了，则会产⽣数据错乱。
所以，每次建⽴连接前重新初始化⼀个序列号主要是为了通信双⽅能够根据序号将不属于本连接的报⽂段丢弃。
另⼀⽅⾯是为了安全性，防⽌⿊客伪造的相同序列号的 TCP 报⽂被对⽅接收。

#### 65.1.2.2 TCP为什么要三次握⼿

1. 三次握⼿才⾸要原因是为了防⽌旧的重复连接初始化造成混乱
   比如说，⽹络环境是错综复杂的，往往并不是如我们期望的⼀样，先发送的数据包，就先到达⽬标主机，反⽽可能很混乱，
   可能会由于⽹络拥堵等乱七⼋糟的原因，会使得旧的数据包，先到达⽬标主机，
   比如说客户端连续发送多次 SYN 建⽴连接的报⽂，在⽹络拥堵情况下：
        ⼀个「旧 SYN 报⽂」⽐「最新的 SYN 」 报⽂早到达了服务端；
        那么此时服务端就会回⼀个 SYN + ACK 报⽂给客户端；
        客户端收到后可以根据⾃身的上下⽂，判断这是⼀个历史连接（序列号过期或超时），那么客户端就会发送RST报⽂
        给服务端，表示中⽌这⼀次连接。
   这样，如果是历史连接（序列号过期或超时），则第三次握⼿发送的报⽂是 RST 报⽂，以此中⽌历史连接；
        如果不是历史连接，则第三次发送的报⽂是 ACK 报⽂，通信双⽅就会成功建⽴连接；

2. 三次握⼿才可以 同步 双⽅的 初始 序列号
   序列号是作为TCP可靠传输的⼀个关键因素，它的作⽤其实是：

   * 接收⽅可以去除重复的数据；
   * 接收⽅可以根据数据包的序列号按序接收；
   * 可以标识发送出去的数据包中，哪些是已经被对⽅收到的；

   序列号在 TCP 连接中占据着⾮常重要的作⽤，所以当客户端发送携带「初始序列号」的 SYN 报⽂的时候，
   需要服务端回⼀个 ACK 应答报⽂，表示客户端的 SYN 报⽂已被服务端成功接收，那当服务端发送「初始序列号」
   给客户端的时候，依然也要得到客户端的应答回应， 这样⼀来⼀回，才能确保双⽅的初始序列号能被可靠的同步。
   
   - 四次握⼿其实也能够同步双⽅的初始化序号，但由于客户端传输ACK和SYN可以优化成⼀步，所以就成了「三次握⼿」
   - 两次握⼿只保证了⼀⽅的初始序列号能被对⽅成功接收，没办法保证双⽅的初始序列号都能被确认接收。

3. 三次握⼿才可以避免资源浪费
  
  如果只有「两次握⼿」，当客户端的 SYN 请求连接在⽹络中阻塞，
  客户端没有接收到 ACK 报⽂，重复发送多次 SYN 报⽂，
  那么服务器在收到请求后就会建⽴多个冗余的⽆效链接，造成不必要的资源浪费。
  
#### 65.1.2.3 如何对三次握手进行性能优化

1. 三次握⼿建⽴连接的⾸要⽬的是「同步序列号」。只有同步了序列号才有可靠传输，所以当客户端发起 SYN 包时，
   可以通过tcp_syn_retries 控制其重传的次数，⽐如内⽹通讯不畅时，就可以适当调低重试次数，尽快把错误暴露给应⽤程序
2. 当服务端SYN半连接队列溢出后，会导致后续连接被丢弃，可以通过backlog等参数来调整 SYN 半连接队列的⼤⼩。
3. TCP Fast Open 功能可以绕过三次握⼿，使得 HTTP 请求减少了 1 个 RTT 的时间，所以也是一种性能优化方案

#### 65.1.2.4 如何绕过三次握手发送数据

TCP Fast Open 功能可以绕过三次握⼿，使得 HTTP 请求减少了1个RTT的时间，
第⼀次发起 HTTP GET请求的时候，还是需要正常的三次握⼿流程。
之后发起 HTTP GET请求的时候，可以绕过三次握⼿，这就减少了握⼿带来的 1 个 RTT 的时间消耗。

#### 65.1.2.5 TCP Fast Open的过程

I、客户端⾸次建⽴连接时的过程：
   1. 客户端发送SYN报⽂，该报⽂包含Fast Open选项，且该选项的Cookie为空；
   2. ⽀持 TCP Fast Open 的服务器⽣成 Cookie，并将其置于 SYN-ACK 数据包中的Fast Open选项以发回客户端；
   3. 客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。

II、如果客户端再次向服务器建⽴连接时的过程：
   1. 客户端发送 SYN 报⽂，该报⽂包含「数据」以及此前记录的 Cookie；
   2. ⽀持 TCP Fast Open 的服务器会对收到 Cookie 进⾏校验：如果 Cookie 有效，服务器将在 SYNACK 报⽂中对 SYN 和
      「数据」进⾏确认，服务器随后将「数据」递送⾄相应的应⽤程序；如果Cookie ⽆效，服务器将丢弃 SYN 报⽂中包含的
      「数据」，且其随后发出的 SYN-ACK 报⽂将只确认 SYN 的对应序列号；
   3. 如果服务器接受了 SYN 报⽂中的「数据」，服务器可在握⼿完成之前发送「数据」， 这就减少了握⼿带来的 1 个 RTT 的
      时间消耗；
   4. 客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报⽂中发送的「数据」没有被确
      认，则客户端将重新发送「数据」；

### 65.1.3 TCP的四次挥手

#### 65.1.3.1 TCP四次挥手流程

客户端-发送一个FIN，用来关闭客户端到服务器的数据传送
服务器-收到这个FIN，它发回一个ACK，确认序号为收到的序号加1 。和SYN一样，一个FIN将占用一个序号
服务器-关闭与客户端的连接，发送一个FIN给客户端
客户端-发回ACK报文确认，并将确认序号设置为收到序号加1

##### 65.1.3.1.1 客户端主动调⽤了 close ，会发⽣什么？

1. 客户端调⽤ close ，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报⽂，进⼊ FIN_WAIT_1状态；
2. 服务端接收到了 FIN 报⽂， TCP 协议栈会为 FIN 包插⼊⼀个⽂件结束符 EOF 到接收缓冲区中，应⽤程序
   可以通过 read 调⽤来感知这个 FIN 包。这个 EOF 会被放在已排队等候的其他已接收的数据之后，这就
   意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再⽆额外数据到达。此时，服务端进⼊CLOSE_WAIT 状态；
3. 接着，当处理完数据后，⾃然就会读到 EOF ，于是也调⽤ close 关闭它的套接字，这会使得客户端会发出⼀个 FIN 包，
   之后处于 LAST_ACK 状态；
4. 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进⼊ TIME_WAIT 状态；
5. 服务端收到 ACK 确认包后，就进⼊了最后的 CLOSE 状态；
6. 客户端经过 2MSL 时间之后，也进⼊ CLOSE 状态；

#### 65.1.3.2 TCP为什么要四次挥手

四次挥手主要是从FIN过程进行分析：
1. 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。
2. 服务器收到客户端的 FIN 报⽂时，先回⼀个 ACK 应答报⽂，⽽服务端可能还有数据需要处理和发送，
   等服务端不再发送数据时，才发送 FIN 报⽂给客户端来表示同意现在关闭连接。

服务端通常需要等待完成数据的发送和处理，所以服务端的ACK和FIN⼀般都会分开发送，从⽽⽐三次握⼿导致多了⼀次

#### 65.1.3.3 如何对四次挥手进行优化

1. 主动发起FIN报⽂断开连接的⼀⽅，如果迟迟没收到对⽅的 ACK 回复，则会重传 FIN 报⽂，重传的次数由 
   tcp_orphan_retries 参数决定。

当主动⽅收到 ACK 报⽂后，连接就进⼊ FIN_WAIT2 状态，根据关闭的⽅式不同，优化的⽅式也不同：

* 如果是 close 函数关闭的连接，那么它就是孤⼉连接。如果在系统设置的时间内 （tcp_fin_timeout） 没有收到对⽅的 
  FIN 报⽂，连接就直接关闭。同时，为了应对孤⼉连接占⽤太多的资源， tcp_max_orphans定义了最⼤孤⼉连接的数量，超
  过时连接就会直接释放。

* 反之是 shutdown 函数关闭的连接，则不受此参数限制；
  
2. 当主动⽅接收到 FIN 报⽂，并返回 ACK 后，主动⽅的连接进⼊ TIME_WAIT 状态。为了防⽌ TIME_WAIT 状态占⽤太多的
   资源， tcp_max_tw_buckets 定义了最⼤数量，超过时连接也会直接释放。

3. 被动方关闭的连接，它在回复 ACK 后就进⼊了 CLOSE_WAIT 状态，等待进程调⽤ close函数关闭连接。因此，出现⼤量 
   CLOSE_WAIT 状态的连接时，应当从应⽤程序中找问题。
   
4. 当被动⽅发送 FIN 报⽂后，连接就进⼊ LAST_ACK 状态，在未等到 ACK 时，会在tcp_orphan_retries 参数的控制下重
   发 FIN 报⽂。

#### 65.1.3.4 第四次挥手后为什么不立即断开？

主要就是
1.为了确保第四次挥手的ACK能被服务器接收到
2.确保所有的老链接都在网络中消失。

要等待2MSL后才断开链接

假如第四次挥手失败了，因为丢失而未到达服务器，这样，服务器会一直收不到客户端的回应，
也就无法得知客户端是否收到了即将要断开连接的请求。
客户端此刻还蒙在鼓里，还在等待服务器继续发送消息。服务器不能判断客户端是否收到，
本身就是一个BUG，于是才有的等待2MSL的情况。为了保证客户端最后一次挥手的报文能够到达服务器，
若第4次挥手的报文段丢失了，服务器就会超时重传第3次挥手的报文段，
所以客户端此时不是直接进入CLOSED，而是保持TIME_WAIT（等待2MSL就是TIME_WAIT）。
当客户端再次受到服务器因为超时重传而发送的第3次挥手的请求时，
客户端就会重新给服务器发送第4次挥手的报文（保证服务器能够受到客户端的回应报文）。
最后，客户端、服务器才真正断开连接。等待2MSL就是为了确保服务器能够受到客户端最后的回应。
2.如果客户端直接CLOSED，然后又再次向服务器发起一个新连接，
谁也不能保证新发起的连接和刚关闭的连接的端口号是不同的，
有可能新、老连接的端口号就是一样的。
假设新、老连接端口号一致，若老连接的一些数据仍滞留在网络中，
这些滞留数据在新连接建立后才到达服务器，鉴于前后端口号一致，
TCP协议就默认这些数据属于新连接，于是数据就这样乱成一锅粥了。
所以TCP连接还要在TIME_WAIT状态下等待2MSL，确保所有老连接的数据都在网络中消失！

#### 65.1.3.5 参数

##### 65.1.3.5.1 TIME_WAIT

###### 65.1.3.5.1.1 概念

###### 65.1.3.5.1.2 为什么TIME_WAIT等待的时间是2MSL？

⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包被接收⽅处理后⼜会向对⽅发送响应，
所以⼀来⼀回需要等待2倍的时间。

###### 65.1.3.5.1.3 为什么需要TIME_WAIT状态？

1. 防⽌具有相同端口的的「旧」数据包被收到；
   
   比如说服务端在关闭连接之前发送一个报⽂，但是被⽹络延迟了。这时有相同端⼝的TCP连接被复⽤后，被延迟的报文抵达
   了客户端，那么客户端是有可能正常接收这个过期的报⽂，这就会产⽣数据错乱等严重的问题。而使用TIME_WAIT这个时
   间，⾜以让两个⽅向上的数据包都被丢弃

2. 保证连接正确关闭

  比如客户端四次挥⼿的最后⼀个ACK报⽂如果在⽹络中被丢失了，此时如果客户端TIME-WAIT 过短或没有，则就直接进⼊
  了 CLOSED 状态了，那么服务端则会⼀直处在 LASE_ACK状态。当客户端发起建⽴连接的 SYN 请求报⽂后，服务端会发送 
  RST 报⽂给客户端，连接建⽴的过程就会被终⽌。如果有这个时间，就会正常关闭，即使没有收到ACK报文，我也有时间重发
  并关闭

###### 65.1.3.5.1.4 TIME_WAIT 过多有什么危害？

1. 内存资源占⽤；

2. 对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝；被占满就会导致⽆法创建新的连接。

###### 65.1.3.5.1.5 如何优化 TIME_WAIT？

优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，
要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的。

##### 65.1.3.5.2 CLOSE-WAIT

###### 65.1.3.5.2.1 概念

###### 65.1.3.5.2.2 如果一直处于CLOSE-WAIT是什么原因导致的

在对方关闭连接之后服务器程序自己没有进一步发出ack信号。
换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，
于是这个资源就一直被程序占着。
个人觉得这种情况，通过服务器内核参数也没办法解决，
服务器对于程序抢占的资源没有主动回收的权利，除非终止程序运行。

##### 65.1.3.5.2 CLOSE_WAIT和TIME_WAIT的区别

比如说服务器A是一台爬虫服务器，
它使用简单的HttpClient去请求资源服务器B上面的apache获取文件资源，
正常情况下，如果请求成功，那么在抓取完资源后，
服务器A会主动发出关闭连接的请求，这个时候就是主动关闭连接，
服务器A的连接状态我们可以看到是TIME_WAIT。
如果一旦发生异常呢？假设请求的资源服务器B上并不存在，
那么这个时候就会由服务器B发出关闭连接的请求，
服务器A就是被动的关闭了连接，
如果服务器A被动关闭连接之后程序员忘了让HttpClient释放连接，
那就会造成CLOSE_WAIT的状态了。

### 65.1.4 TCP传输数据不一致

#### 65.1.4.1 TCP传输不一致

又可能是数据包到达时间的问题，比如你接收到数据包A，然后给服务器发送一个确认包，表面上看很合理，
但是你接收到的A不见得是服务器发给你的第一个包，你对A的确认信息也不见得服务器可以第一时间接收到，
这样就会造成一个时间差，这时就会出现接收N个A的情况，如果你不对这些数据包的顺序做出处理的话就会出现你这个情况

#### 65.1.4.2 传输数据优化

TCP 会保证每⼀个报⽂都能够抵达对⽅，报⽂发出去后，必须接收到对⽅返回的确认报⽂ ACK，
如果迟迟未收到，就会超时重发该报⽂，直到收到对⽅的 ACK 为⽌。
所以， TCP 报⽂发出去后，并不会⽴⻢从内存中删除，
因为重传时还需要⽤到它。这种⽅式的缺点是效率⽐较低的
可以采用并⾏批量发送报⽂，再批量确认报⽂即可，
但是当接收⽅硬件不如发送⽅，或者系统繁忙、资源紧张时，
是⽆法瞬间处理这么多报⽂的。于是，这些报⽂只能被丢掉，
使得⽹络效率⾮常低。
为了解决这种现象发⽣，
TCP 提供⼀种机制可以让「发送⽅」根据「接收⽅」的实际接收能⼒控制发送的数据量
因为⽹络的传输能⼒是有限的，当发送⽅依据发送窗⼝，
发送超过⽹络处理能⼒的报⽂时，路由器会直接丢弃这些报⽂。
影响了传输速度，发送缓冲区的⼤⼩最好是往带宽时延积靠近

### 65.1.5 TCP保活机制

如果已经建⽴了连接，但是客户端突然出现故障了

TCP 有⼀个机制是保活机制。

定义在一个时间段内，如果没有任何连接相关的活动， TCP 保活机制会开始作⽤，每隔⼀个时间间隔，发送⼀个探测报⽂，该

探测报⽂包含的数据⾮常少，如果连续⼏个探测报⽂都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息

通知给上层应⽤程序。

### 65.1.6 什么是TCP连接？

⽤于保证可靠性和流量控制维护的某些状态信息，
包括Socket、序列号和窗⼝⼤⼩称为连接。

#### 65.1.6.1 如何唯一确定一个 TCP 连接呢？

TCP 四元组可以唯一的确定一个连接，四元组包括：
   - 源地址
   - 源端口
   - 目的地址
   - 目的端口
源地址和目的地址的字段是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。
源端口和目的端口的字段（16位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。

#### 65.1.6.2 TCP的最大连接数是多少？

服务器通常固定在某个本地端口上监听，等待客户端的连接请求。
因此，客户端 IP 和 端口是可变的，其理论值计算公式应该是:
客户端的IP数 x 客户端的端口数

#### 65.1.6.3 TCP连接数为什么远不能达到理论上限

首先主要是文件描述符限制，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目；
另一个是内存限制，每个 TCP 连接都要占用一定内存，操作系统是有限的。

### 65.1.7 初始序列号

#### 65.1.7.1 为什么客户端和服务端的初始序列号 ISN 是不相同的？

因为网络中的报文会延迟、会复制重发、也有可能丢失，
这样会造成的不同连接之间产生互相影响，所以为了避免互相影响，客户端和服务端的初始序列号是随机且不同的。

### 65.1.8 Mss

#### 65.1.8.1 概念

除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度；
MTU：一个网络包的最大长度，以太网中一般为 1500 字节；

#### 65.1.8.2 IP层会分片，为什么TCP层还需要MSS呢？

当 IP 层有一个超过 MTU 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，
把数据分片成若干片，保证每一个分片都小于 MTU。
把一份 IP 数据报进行分片以后，由目标主机的 IP 层来进行重新组装后，在交给上一层 TCP 传输层。
但这存在隐患的，那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传。
因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。
当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，则不会响应 ACK 给对方，那么发送方的 TCP 在超时后，
就会重发「整个 TCP 报文（头部 + 数据）」。
所以 IP 层进行分片传输，是非常没有效率的。
所以，为了达到最佳的传输效率， TCP 协议在建立连接的时候通常要协商双方的 MSS 值，
当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。

### 65.1.9 TCP首部字段

1） 源端口和目的端口，各占2个字节，分别写入源端口和目的端口。

2） 序号，占4字节。序号范围是【0，2^32 - 1】，共2^32个序号。
    序号增加到2^32-1后，下一个序号就又回到0。
    TCP是面向字节流的。在一个TCP连接中传送的字节流中的每一个字节都按顺序编号。
    整个要传送的字节流的起始序号必须在连接建立时设置。
    首部中的序号字段值则是指的是本报文段所发送的数据的第一个字节的序号。
    例如，一报文段的序号是301，而接待的数据共有100字节。
    这就表明：本报文段的数据的第一个字节的序号是301，
    最后一个字节的序号是400。显然，下一个报文段（如果还有的话）的数据序号应当从401开始，
    即下一个报文段的序号字段值应为401。这个字段的序号也叫“报文段序号”。

3） 确认号，占4字节，是期望收到对方下一个报文段的第一个数据字节的序号。
    例如，B正确收到了A发送过来的一个报文段，其序号字段值是501，
    而数据长度是200字节（序号501~700），这表明B正确收到了A发送的到序号700为止的数据。
    因此，B期望收到A的下一个数据序号是701，
    于是B在发送给A的确认报文段中把确认号置为701。

4） 数据偏移，占4位，它指出TCP报文段的数据起始处距离TCP报文段的起始处有多远。
   这个字段实际上是指出TCP报文段的首部长度。由于首部中还有长度不确定的选项字段，
   因此数据偏移字段是必要的，

5） 保留，占6位，保留为今后使用，但目前应置为0 。

还有几个控制位，用来说明报文段的性质。
6） 紧急URG，当URG=1时，表明紧急指针字段有效。
    它告诉系统此报文段中有紧急数据，应尽快发送（相当于高优先级的数据），
    而不要按原来的排队顺序来传送。
    例如，已经发送了很长的一个程序要在远地的主机上运行。
    但后来发现了一些问题，需要取消该程序的运行，因此用户从键盘发出中断命令。
    如果不使用紧急数据，那么这两个字符将存储在接收TCP的缓存末尾。
    只有在所有的数据被处理完毕后这两个字符才被交付接收方的应用进程。
    这样做就浪费了很多时间。

7） 确认ACK，仅当ACK = 1时确认号字段才有效，
    当ACK = 0时确认号无效。TCP规定，在连接建立后所有的传送的报文段都必须把ACK置为1。

8） 推送 PSH，当两个应用进程进行交互式的通信时，
    有时在一端的应用进程希望在键入一个命令后立即就能收到对方的响应。
    在这种情况下，TCP就可以使用推送（push）操作。
    这时，发送方TCP把PSH置为1，并立即创建一个报文段发送出去。
    接收方TCP收到PSH=1的报文段，就尽快地（即“推送”向前）交付接收应用进程。
    而不用再等到整个缓存都填满了后再向上交付。

9） 复位RST，当RST=1时，表名TCP连接中出现了严重错误（如由于主机崩溃或其他原因），
    必须释放连接，然后再重新建立传输连接。
    RST置为1还用来拒绝一个非法的报文段或拒绝打开一个连接。

10） 同步SYN，在连接建立时用来同步序号。当SYN=1而ACK=0时，
     表明这是一个连接请求报文段。对方若同意建立连接，
     则应在响应的报文段中使SYN=1和ACK=1，
     因此SYN置为1就表示这是一个连接请求或连接接受报文。

11） 终止FIN，用来释放一个连接。当FIN=1时，
     表明此报文段的发送发的数据已发送完毕，并要求释放运输连接。

12） 窗口，占2字节。窗口值是【0，2^16-1】之间的整数。
     窗口指的是发送本报文段的一方的接受窗口（而不是自己的发送窗口）。
     窗口值告诉对方：从本报文段首部中的确认号算起，
     接收方目前允许对方发送的数据量（以字节为单位）。
     之所以要有这个限制，是因为接收方的数据缓存空间是有限的。
     总之，窗口值作为接收方让发送方设置其发送窗口的依据。
     例如，发送了一个报文段，其确认号是701，窗口字段是1000.
     这就是告诉对方：“从701算起，我（即发送方报文段的一方）
     的接收缓存空间还可接受1000个字节数据（字节序号是701~1700），

13） 检验和，占2字节。检验和字段检验的范围包括首部和数据这两部分。
     和UDP用户数据报一样，在计算检验和时，
     要在TCP报文段的前面加上12字节的伪首部。
     伪首部的格式和UDP用户数据报的伪首部一样。
     但应把伪首部第4个字段中的17改为6（TCP的协议号是6）；
     把第5字段中的UDP中的长度改为TCP长度。接收方收到此报文段后，
     仍要加上这个伪首部来计算检验和。若使用TPv6,则相应的伪首部也要改变。

14） 紧急指针，占2字节。紧急指针仅在URG=1时才有意义，
     它指出本报文段中的紧急数据的字节数（紧急数据结束后就是普通数据）。
     因此，在紧急指针指出了紧急数据的末尾在报文段中的位置。
     当所有紧急数据都处理完时，TCP就告诉应用程序恢复到正常操作。
     值得注意的是，即使窗口为0时也可以发送紧急数据。

15） 选项，长度可变，最长可达4字节。当没有使用“选项”时，TCP的首部长度是20字节。

### 65.1.10 TCP作用

保证数据通信的完整性和可靠性，防止丢包,因为IP 协议只是一个地址协议，并不保证数据包的完整。
如果路由器丢包（比如缓存满了，新进来的数据包就会丢失），
就需要发现丢了哪一个包，以及如何重新发送这个包。这就要依靠 TCP 协议。

### 65.1.11 TCP数据包的大小

TCP 负载实际为1400字节左右。
以太网数据包（packet）的大小是固定的，1522字节。其中， 
1500 字节是负载（payload），22字节是头信息（head）。
IP 数据包在以太网数据包的负载里面，它也有自己的头信息，
最少需要20字节，所以 IP 数据包的负载最多为1480字节。
TCP 数据包在 IP 数据包的负载里面。
它的头信息最少也需要20字节，
因此 TCP 数据包的最大负载是 1480 - 20 = 1460 字节。
由于 IP 和 TCP 协议往往有额外的头信息，
所以 TCP 负载实际为1400字节左右。
   * 注：可不可以压缩加快速度---》HTTP/2进行了优化

### 65.1.12 TCP 数据包的编号（SEQ）

一个包1400字节，那么一次性发送大量数据，
就必须分成多个包
发送的时候，TCP 协议为每个包编号，
以便接收的一方按照顺序还原。万一发生丢包，
也可以知道丢失的是哪一个包。
第一个包的编号是一个随机数。
假设为1号包。假定这个包的负载长度是100字节，
那么可以推算出下一个包的编号应该是101。
每个数据包都可以得到两个编号：
自身的编号，以及下一个包的编号。
接收方由此知道，应该按照什么顺序将它们还原成原始文件。

### 65.1.13 TCP 数据包的组装

收到TCP数据包以后，组装还原是操作系统完成的。应用程序不会直接处理 TCP 数据包。
对于应用程序来说，不用关心数据通信的细节。除非线路异常，收到的总是完整的数据。
应用程序需要的数据放在TCP数据包里面，有自己的格式（比如HTTP协议）。
TCP 并没有提供任何机制，表示原始文件的大小，这由应用层的协议来规定。
比如，HTTP 协议就有一个头信息Content-Length，表示信息体的大小。对于操作系统来说，
就是持续地接收 TCP 数据包，将它们按照顺序组装好，一个包都不少。
操作系统不会去处理 TCP 数据包里面的数据。一旦组装好 TCP 数据包，就把它们转交给应用程序。
TCP 数据包里面有一个端口（port）参数，就是用来指定转交给监听该端口的应用程序。

### 65.1.14 可靠性传输

[详见](#67可靠性传输)

### 65.1.15 TCP半/全连接队列

#### 65.1.15.1 什么是TCP半/全连接队列

在 TCP 三次握⼿的时候， Linux 内核会维护两个队列，分别是：
半连接队列，也称 SYN 队列；
全连接队列，也称 accepet 队列；

服务端收到客户端发起的 SYN 请求后， 内核会把该连接存储到半连接队列，并向客户端响应 SYN+ACK，
接着客户端会返回 ACK，服务端收到第三次握⼿的 ACK 后， 内核会把连接从半连接队列移除，然后创建新的完全的连接，
并将其添加到 accept 队列，等待进程调⽤ accept 函数时把连接取出来。


## 65.2 UDP

### 65.2.1 概念

#### 65.2.1.1 为什么UDP头部没有⾸部⻓度字段

TCP 有可变⻓的「选项」字段，⽽ UDP 头部⻓度则是不会变化的，⽆需多⼀个字段去记录 UDP 的⾸部⻓度。

## 65.3 IP

### 65.3.1 分类

分为了A 类、 B 类、 C 类、 D 类、 E 类。


## 65.4 DNS

### 65.4.1 概念

我们在上⽹的时候，通常使⽤的⽅式是域名，⽽不是 IP 地址，因为域名⽅便⼈类记忆。
那么实现这⼀技术的就是 DNS 域名解析， DNS 可以将域名⽹址⾃动转换为具体的 IP 地址。

### 65.4.2 域名解析工作原理

浏览器⾸先看⼀下⾃⼰的缓存⾥有没有，如果没有就向操作系统的缓存要，还没有就检查本机域名解析⽂件hosts ，
如果还是没有，就会 DNS 服务器进⾏查询，
1. 客户端⾸先会发出⼀个 DNS 请求，问 www.server.com 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端
   的 TCP/IP 设置中填写的 DNS 服务器地址）。
2. 本地域名服务器收到客户端的请求后，如果缓存⾥的表格能找到 www.server.com，则它直接返回 IP 地址。
   如果没有，本地 DNS 会去问它的根域名服务器：能告诉我 www.server.com 的 IP 地址吗？” 根域名
   服务器是最⾼层次的，它不直接⽤于域名解析，但能指明⼀条道路。
3. 根 DNS 收到来⾃本地 DNS 的请求后，发现后置是 .com，说： “www.server.com 这个域名归 .com 区域管
   理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。 ”
4. 本地 DNS 收到顶级域名服务器的地址后，发起请求问，能告诉我 www.server.com 的 IP 地址吗？ ”
5. 顶级域名服务器说： “我给你负责 www.server.com 区域的权威 DNS 服务器的地址，你去问它应该能问到”。
6. 本地 DNS 于是转向问权威 DNS 服务器： “⽼三， www.server.com对应的IP是啥呀？ ” server.com 的权威
   DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。
7. 权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。
8. 本地 DNS 再将 IP 地址返回客户端，客户端和⽬标建⽴连接。

## 65.5 ARP

### 65.1 概念

在OSI模型有七层，TCP在第4层传输层，IP在第3层网络层，而ARP在第2层数据链路层。
因为高层对低层是有强依赖的，所以TCP的建立前要进行ARP的请求和应答。
ARP高速缓存表在IP层使用。如果每次建立TCP连接都发送ARP请求，会降低效率，因此在主机、交换机、路由器上都会有ARP缓存表。
建立TCP连接时先查询ARP缓存表，如果有效，直接读取ARP表项的内容进行第二层数据包的发送；
只有表失效时才进行ARP请求和应答进行MAC地址的获取，以建立TCP连接。

### 66.2 工作流程

1. 首先，每个主机都会在自己的ARP缓冲区中建立一个ARP列表，
   以表示IP地址和MAC地址之间的对应关系。

2. 当源主机要发送数据时，首先检查ARP列表中是否有对应IP地址的目的主机的MAC地址，
   如果有，则直接发送数据，如果没有，就向本网段的所有主机发送ARP数据包，
   该数据包包括的内容有：源主机 IP地址，源主机MAC地址，目的主机的IP 地址。

3. 当本网络的所有主机收到该ARP数据包时，首先检查数据包中的IP地址是否是自己的IP地址，
   如果不是，则忽略该数据包，如果是，则首先从数据包中取出源主机的IP和MAC地址写入到ARP列表中，
   如果已经存在，则覆盖，然后将自己的MAC地址写入ARP响应包中，告诉源主机自己是它想要找的MAC地址。

4. 源主机收到ARP响应包后。将目的主机的IP和MAC地址写入ARP列表，并利用此信息发送数据。
   如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。广播发送ARP请求，单播发送ARP响应。


## 65.6 DHCP

### 65.6.1 概念

比如说笔记本通常都是通过 DHCP 动态获取 IP 地址，⼤⼤省去了配 IP 信息繁琐的过程。

### 65.6.2 流程

1. 客户端⾸先发起 DHCP 发现报⽂（DHCP DISCOVER） 的 IP 数据报，由于客户端没有 IP 地址，也不知道
   DHCP 服务器的地址，所以使⽤的是 UDP ⼴播通信，其使⽤的⼴播⽬的地址是 255.255.255.255（端⼝67） 
   并且使⽤ 0.0.0.0（端⼝ 68） 作为源 IP 地址。 DHCP 客户端将该 IP 数据报传递给链路层，链路层然后
   将帧⼴播到所有的⽹络中设备。
2. DHCP 服务器收到 DHCP 发现报⽂时，⽤ DHCP 提供报⽂（DHCP OFFER） 向客户端做出响应。该报⽂仍
   然使⽤ IP ⼴播地址 255.255.255.255，该报⽂信息携带服务器提供可租约的 IP 地址、⼦⽹掩码、默认⽹关、
   DNS 服务器以及 IP 地址租⽤期。
3. 客户端收到⼀个或多个服务器的 DHCP 提供报⽂后，从中选择⼀个服务器，并向选中的服务器发送 DHCP 请求报⽂
   DHCP REQUEST进⾏响应，回显配置的参数。
4. 最后，服务端⽤ DHCP ACK 报⽂对 DHCP 请求报⽂进⾏响应，应答所要求的参数。


## 65.3 区别

### 65.3.1 TCP与UDP区别

![avatar](http://qd6kny79g.bkt.clouddn.com/03-TCP.jpg)

1. `连接方面来看`
   TCP 是面向连接的传输层协议，传输数据前先要建立连接。
   UDP 是不需要连接，即刻传输数据。
2. `服务对象`
   TCP 是一对一的两点服务，即一条连接只有两个端点。
   UDP 支持一对一、一对多、多对多的交互通信
3. `可靠性`
   TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。
   UDP 是尽最大努力交付，不保证可靠交付数据。
4. `拥塞控制、流量控制`
   TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。
   UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。
5. `首部开销`
   TCP 首部长度较长，会有一定的开销，
      首部在没有使用「选项」字段时是 20 个字节，
      如果使用了「选项」字段则会变长的。
   UDP 首部只有 8 个字节，并且是固定不变的，开销较小。
6. `传输⽅式`
   TCP 是流式传输，没有边界，但保证顺序和可靠。
   UDP 是⼀个包⼀个包的发送，是有边界的，但可能会丢包和乱序。
7. `分⽚不同`
   TCP 的数据⼤⼩如果⼤于 MSS ⼤⼩，则会在传输层进⾏分⽚，⽬标主机收到后，也同样在传输层组装 TCP数据包，
   如果中途丢失了⼀个分⽚，只需要传输丢失的这个分⽚。
   UDP 的数据⼤⼩如果⼤于 MTU ⼤⼩，则会在 IP 层进⾏分⽚，⽬标主机收到后，在 IP 层组装完数据，
   接着再传给传输层，但是如果中途丢了⼀个分⽚，在实现可靠传输的 UDP 时则就需要重传所有的数据包，
   这样传输效率⾮常差，所以通常 UDP 的报⽂应该⼩于 MTU

### 65.3.2 TCP与IP的区别

IP层接收由更低层（网络接口层例如以太网设备驱动程序）发来的数据包，并把该数据包发送到更高层—TCP层；
IP层也把从TCP接收来的数据包传送到更低层。
我认为TCP和IP的关系是：IP提供基本的数据传送，而高层的TCP对这些数据包做进一步加工，如提供端口号等等。

## 65.4 应用场景

### 65.4.1 TCP应用场景

由于TCP是面向连接，能保证数据的可靠性交付，因此经常用于：
  * FTP 文件传输
  * HTTP / HTTPS
接受邮件、远程登录

### 65.4.2 UDP应用场景

由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：
  * 包总量较少的通信，如 DNS 、SNMP 等
  * 视频、音频等多媒体通信
  * 广播通信
QQ聊天、在线视频、网络语音电话

### 65.4.3 IP应用场景


### 65.4.4 ARP应用场景




# 67.可靠性传输

## 67.1 TCP可靠性传输方案

1. 确认应答+序列号：TCP给发送的每⼀个包进⾏编号，
                  接收⽅对数据包进⾏排序，把有序数据传送给应⽤层。
2. 校验和：TCP 将保持它⾸部和数据的检验和。
          ⽬的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报⽂段和不确认收到此报⽂段。
3. 流量控制：TCP 连接的每⼀⽅都有固定⼤⼩的缓冲空间， 
           TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收⽅来不及处理发送⽅的数据，能提示发送⽅降低发送的速率，
           防⽌包丢失。 TCP 使⽤的流量控制协议是可变⼤⼩的滑动窗⼝协议。（TCP 利⽤滑动窗⼝实现流量控制）
4. 拥塞控制：当⽹络拥塞时，减少数据的发送。
5. ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完⼀个分组就停⽌发送，等待对⽅确认。在收到确认后再发下⼀个分组。
6. 超时重传： 当 TCP 发出⼀个段后，它启动⼀个定时器，
            等待⽬的端确认收到这个报⽂段。如果不能及时收到⼀个确认，将重发这个报⽂段。

## 67.2 重传机制

### 67.2.1 常见的重传机制

1. 超时重传
2. 快速重传
3. SACK
4. D-SACK

### 67.2.2 超时重传

#### 67.2.2.1 概念

在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据。

#### 67.2.2.2 什么时候会发生超时重传

1. 数据包丢失
2. 确认应答丢失

#### 67.2.2.3 超时重传存在的问题

如果超时重发的数据，再次超时的时候，又需要重传的时候，
TCP 的策略是超时间隔加倍。这样超时周期可能相对较长

### 67.2.3 快速重传

#### 67.2.3.1 概念

它不以时间为驱动，而是以数据驱动重传

比如说，发送方发出了 1，2，3，4，5 份数据：

第一份 Seq1 先送到了，于是就 Ack 回 2；
结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；
后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；
发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。
最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。

所以说，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段

#### 67.2.3.2 快速重传问题

快速重传机制只解决了一个问题，就是超时时间的问题，
但是它依然面临着另外一个问题。就是重传的时候，
是重传之前的一个，还是重传所有的问题。

### 67.2.4 SACK方法

#### 67.2.4.1 概念

这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，
它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，
哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。
比如说发送方收到了三次同样的 ACK 确认报文，
于是就会触发快速重发机制，
通过 SACK 信息发现只有某段数据丢失，
则重发时，就只选择了这个 TCP 段进行重复。

### 67.2.5 D-SACK

#### 67.2.5.1 概念

主要是使用了 SACK 来告诉「发送方」有哪些数据被重复接收了

比如说ack丢包了
「接收⽅」发给「发送⽅」的两个 ACK 确认应答都丢失了，所以发送⽅超时后，重传第⼀个数据包（3000 ~3499）
于是「接收⽅」发现数据是重复收到的，于是回了⼀个 SACK = 3000~3500，告诉「发送⽅」 3000~3500
的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个
SACK 就代表着 D-SACK 。这样「发送⽅」就知道了，数据没有丢，是「接收⽅」的 ACK 确认报⽂丢了。

#### 67.2.5.2 D-SACK好处

1. 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了;
2. 可以知道是不是「发送方」的数据包被网络延迟了;
3. 可以知道网络中是不是把「发送方」的数据包给复制了;

## 67.3 滑动窗口

### 22-1：引入窗口概念的原因

TCP 是每发送一个数据，都要进行一次确认应答。
当上一个数据包收到了应答了，
再发送下一个，效率比较低，如果数据包的往返时间越长，通信的效率就越低

#### 22-2：什么是窗口

在往返时间较长的情况下，它也不会降低网络通信的效率，这个就是窗口
窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值，窗口的实现实际上是操作系统开辟的一个缓存空间，
发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据
就可以从缓存区清除。比如说ACK 100 确认应答报文丢失，也没关系，因为可以通过下一个确认应答进行确认，
只要发送方收到了ACK 200确认应答，就意味着 200 之前的所有数据「接收方」都收到了。

#### 22-3：窗口大小由哪一方决定？

通常窗口的大小是由接收方的窗口大小来决定的

这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。
于是发送端就可以根据这个接收端的处理能力来发送数据，
而不会导致接收端处理不过来。
发送方发送的数据大小不能超过接收方的窗口接收数据大小，
否则接收方就无法正常接收到数据。

#### 22-4：发送方的窗口

一共分为了四部分

1. 是已发送并收到 ACK确认的数据
2. 是已发送但未收到 ACK确认的数据
3. 是未发送但总大小在接收方处理范围内（接收方还有空间）
4. 是未发送但总大小超过接收方处理范围（接收方没有空间）

p160-163未整理

#### 22-9：TCP 是如何解决窗口关闭时，潜在的死锁现象呢？

为了解决这个问题，TCP 为每个连接设有一个持续定时器，
只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。
如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，
而对方在确认这个探测报文时，给出自己现在的接收窗口大小。
如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；
如果接收窗口不是 0，那么死锁的局面就可以被打破了。

## 67.4 流量控制

### 67.4.1 概念

发送⽅不能⽆脑的发数据给接收⽅，要考虑接收⽅处理能⼒。如果⼀直⽆脑的发数据给对⽅，但对⽅处理不过来，那
么就会导致触发重发机制，从⽽导致⽹络流量的⽆端的浪费。
为了解决这种现象发⽣， TCP 提供⼀种机制可以让「发送⽅」根据「接收⽅」的实际接收能⼒控制发送的数据量，

比如说，客户端是接收⽅，服务端是发送⽅，假设接收窗⼝和发送窗⼝相同，都为 200
       假设两个设备在整个传输过程中都保持相同的窗⼝⼤⼩，不受外界影响
1. 客户端向服务端发送请求数据报⽂。
2. 服务端收到请求报⽂后，发送确认报⽂和80字节的数据，于是可⽤窗⼝Usable减少为120字节，
   同时SND.NXT 指针也向右偏移80字节后，指向321， 这意味着下次发送数据的时候，序列号是321。
3. 客户端收到 80 字节数据后，于是接收窗⼝往右移动 80 字节， RCV.NXT 也就指向 321，
   这意味着客户端期望的下⼀个报⽂的序列号是 321，接着发送确认报⽂给服务端。
4. 服务端再次发送了 120 字节数据，于是可⽤窗⼝耗尽为 0，服务端⽆法再继续发送数据。
5. 客户端收到 120 字节的数据后，于是接收窗⼝往右移动 120 字节， RCV.NXT 也就指向 441，接着发送确认报⽂给服务端。
6. 服务端收到对 80 字节数据的确认报⽂后， SND.UNA 指针往右偏移后指向 321，于是可⽤窗⼝ Usable增⼤到 80。
7. 服务端收到对 120 字节数据的确认报⽂后， SND.UNA 指针往右偏移后指向 441，于是可⽤窗⼝ Usable增⼤到 200。
8. 服务端可以继续发送了，于是发送了 160 字节的数据后， SND.NXT 指向 601，于是可⽤窗⼝ Usable 减少到 40。
9. 客户端收到 160 字节后，接收窗⼝往右移动了 160 字节， RCV.NXT 也就是指向了 601，接着发送确认报⽂给服务端。
10. 服务端收到对 160 字节数据的确认报⽂后，发送窗⼝往右移动了 160 字节，于是 SND.UNA 指针偏移了160 后指向 601，
    可⽤窗⼝ Usable 也就增⼤⾄了 200。

## 67.5 拥塞控制

### 67.5.1 为什么要有拥塞控制呀，不是有流量控制了吗？

流量控制是避免「发送方」的数据填满「接收方」的缓存，
但是并不知道网络的中发生了什么
在网络出现拥堵时，如果继续发送大量数据包，
可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，
但是一重传就会导致网络的负担更重，
于是会导致更大的延迟以及更多的丢包，
这个情况就会进入恶性循环被不断地放大
为了避免「发送方」的数据填满整个网络，有了拥塞控制

### 67.5.2 什么是拥塞控制

在某段时间，若对⽹络中某⼀资源的需求超过了该资源所能提供的可⽤部分，
⽹络的性能就要变坏。这种情况就叫拥塞。
拥塞控制就是为了防⽌过多的数据注⼊到⽹络中，
这样就可以使⽹络中的路由器或链路不致过载。
拥塞控制所要做的都有⼀个前提，就是⽹络能够承受现有的⽹络负荷。

### 67.5.3 什么是拥塞窗口？和发送窗口有什么关系呢？

拥塞窗口是发送方维护的一个的状态变量，
它会根据网络的拥塞程度动态变化的。
发送窗口和接收窗口是约等于的关系，
那么由于加入了拥塞窗口的概念后，
此时发送窗口的值是是拥塞窗口和接收窗口中的最小值。
拥塞窗口 cwnd 变化的规则：
只要网络中没有出现拥塞， cwnd 就会增大；
但网络中出现了拥塞， cwnd 就减少；

### 67.5.4 那么怎么知道当前网络是否出现了拥塞呢？

只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了用拥塞。

### 67.5.5 拥塞控制算法

TCP的拥塞控制采⽤了四种算法：

- 慢启动
- 拥塞避免
- 拥塞发生
- 快速恢复

#### 67.5.5.1 慢启动

##### 67.5.5.1.1 规则

当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1

比如说，假定拥塞窗⼝ cwnd 和发送窗⼝ swnd 相等，
1. 连接建⽴完成后，⼀开始初始化 cwnd = 1 ，表示可以传⼀个 MSS ⼤⼩的数据。
2. 当收到⼀个 ACK 确认应答后， cwnd 增加 1，于是⼀次能够发送 2 个
3. 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以⽐之前多发2 个，所以这⼀次能够发送 4 个
4. 当这4个的ACK确认到来的时候，每个确认cwnd增加1，4个确认cwnd增加4，于是就可以⽐之前多发4个，所以这⼀次能够发送8个。

###### 67.5.5.1.1.1 慢启动涨到什么时候是个头呢？

有⼀个叫慢启动⻔限 ssthresh （slow start threshold）状态变量。
   当 cwnd < ssthresh 时，使⽤慢启动算法。
   当 cwnd >= ssthresh 时，就会使⽤「拥塞避免算法」。

#### 67.5.5.2 拥塞避免算法

##### 67.5.5.2.1 规则

每当收到⼀个 ACK 时， cwnd 增加 1/cwnd。

###### 67.5.5.2.1.1 重传机制何时结束

当触发了重传机制，也就进入了「拥塞发生算法」。

#### 67.5.5.3 拥塞发生

##### 67.5.5.3.1 分类

由于重传机制不同，所以分为了超时重传和快速重传

##### 67.5.5.3.2 超时重传的拥塞发⽣算法

ssthresh 设为 cwnd/2 ，
cwnd 重置为 1

##### 67.5.5.3.3 快速重传的拥塞发⽣算法

cwnd = cwnd/2 ，也就是设置为原来的⼀半;
ssthresh = cwnd ;
进⼊快速恢复算法

##### 67.5.5.3.3 快速恢复算法

你还能收到 3 个重复 ACK 说明⽹络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。
正如前⾯所说，进⼊快速恢复之前， cwnd 和 ssthresh 已被更新了：
cwnd = cwnd/2 ，也就是设置为原来的⼀半;
ssthresh = cwnd ;

###### 67.5.5.3.3.2 规则

1. 拥塞窗⼝ cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了）；
2. 重传丢失的数据包；
3. 如果再收到重复的 ACK，那么 cwnd 增加 1；
4. 如果收到新数据的 ACK 后，把 cwnd 设置为第⼀步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，
   说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进⼊拥塞避免状态；

## 67.6 ARQ协议

### 24-1：什么是ARQ协议

ARQ协议是⾃动重传请求，
他是OSI模型中数据链路层和传输层的错误纠正协议之⼀。
它通过使⽤确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。
如果发送⽅在发送后⼀段时间之内没有收到确认帧，它通常会重新发送。
ARQ包括停⽌等待ARQ协议和连续ARQ协议。

### 24-2：什么是停⽌等待ARQ协议

停⽌等待协议是为了实现可靠传输的，它的基本原理就是每发完⼀个分组就停⽌发送，
等待对⽅确认（回复ACK）。如果过了⼀段时间（超时时间后），
还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，
直到收到确认后再发下⼀个分组；在停⽌等待协议中，
若接收⽅收到重复分组，就丢弃该分组，但同时还要发送确认；

1. 优点： 简单
2. 缺点： 信道利⽤率低，等待时间⻓

### 24-3: 什么是连续ARQ协议

连续 ARQ 协议可提⾼信道利⽤率。发送⽅维持⼀个发送窗⼝，
凡位于发送窗⼝内的分组可以连续发送出去，
⽽不需要等待对⽅确认。
接收⽅⼀般采⽤累计确认，对按序到达的最后⼀个分组发送确认，
表明到这个分组为⽌的所有分组都已经正确收到了。

1. 优点： 信道利⽤率⾼，容易实现，即使确认丢失，也不必重传。
2. 缺点： 不能向发送⽅反映出接收⽅已经正确收到的所有分组的信息。
   * ⽐如：发送⽅发送了5条消息，中间第三条丢失（3号），这时接收⽅只能对前两个发送确认。
           发送⽅⽆法知道后三个分组的下落，⽽只好把后三个全部重传⼀次。
           这也叫 Go-Back-N（回退 N），
           表示需要退回来重传已经发送过的N 个消息。

## 67.7 UDP如何做可靠传输

由于UDP在传输层无法保证数据的可靠传输，只能通过应用层来实现了。
实现的方式可以参照tcp可靠性传输的方式，只是实现不在传输层，实现转移到了应用层。
实现确认机制、重传机制、窗口确认机制。

# 68.Scoket

## 68.1 概念

SOCKET 应该算不上是个协议，应该是应用层与传输层间的一个抽象层，是个编程接口。

## 68.2 Socket编程

服务端和客户端初始化 socket，得到文件描述符；
服务端调用 bind，将绑定在 IP 地址和端口;
服务端调用 listen，进行监听；
服务端调用 accept，等待客户端连接；
客户端调用 connect，向服务器端的地址和端口发起连接请求；
服务端 accept 返回用于传输的 socket 的文件描述符；
客户端调用 write 写入数据；服务端调用 read 读取数据；
客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，
就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。

## socket例子

![avatr](https://www.cnblogs.com/tq03/p/3522028.html)



# ------操作系统--------------------------------------------

# 1. 内存管理-虚拟内存

## 1-1：什么是虚拟地址

我们可以把进程所使用的地址「隔离」开来，
让操作系统为每个进程分配独立的一套「虚拟地址」，
自己使用自己的地址就行，互不干涉。

## 1-2：操作系统是如何管理虚拟地址与物理地址之间的关系？

主要有两种方式，分别是

1. 内存分段
2. 内存分页

## 1-3：什么是虚拟内存

虚拟内存是一种存储模式，
通过这种模式能让我们有种感觉，
我们的内存本身能够处理远比内存大的多的数据或者文件。

### 1-3-1：虚拟内存的优缺点

`优点`
（1）可以使用有限的内存资源，
     处理比实际内存更大的文件或者数据
（2）更加高效的内存利用
（3）在有限的内存资源内，
    让系统运行更多的程序实例，
    因为每个程序都是按需取。
`缺点`
（1）如果内存严重不足，
     而处理超级大的文件时，
     会频繁引起内存和磁盘进行swap，从而降低系统性能。
（2）在多个应用程序之间切换会花费更多的时间
（3）虚拟内存本质上是充分了磁盘空间，
     但同时变相的提供用户使用的实际磁盘空间也会变小。

### 1-3-2：为什么虚拟内存可以大于物理内存

应该是是windows推出时候的一个技术措施，
用于缓解当时缓慢的硬盘速度，
由于内存比硬盘快很多，所以造成了大量的由硬盘读取的数据无法一下子读取，
所以就产生了这个虚拟内存技术，在空闲时候
预读
数据到虚拟内存上，其实虚拟内存也是硬盘空间，
但是由于数据已经经过处理所以比起单纯读取硬盘要快很多。
由于是预读数据，所以存在错误预读的情况，于是虚拟内存的容量会大于
物理内存
容量很多才能追上。


# 2.内存管理-内存分段

## 2-1：什么是内存分段

由于程序是由若干个逻辑分段组成的，
比如说是由代码分段、数据分段、栈段、堆段组成。
不同的段是有不同的属性的，
所以就用分段（Segmentation）的形式把这些段分离出来。

## 2-2：分段机制下，虚拟地址和物理地址是如何映射的？

分段机制下的虚拟地址由两部分组成，段选择子和段内偏移量

段选择子就保存在段寄存器里面。段选择子里面最重要的是段号，

用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。

虚拟地址中的段内偏移量应该位于 0 和段界限之间，

如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

也就是说虚拟地址是通过段表与物理地址进行映射的，

分段机制会把程序的虚拟地址分成 4 个段，

每个段在段表中有一个项，

在这一项找到段的基地址，

再加上偏移量，于是就能找到物理内存中的地址

## 2-3：访问某段偏移量xxx的虚拟地址

如访问段 3 中偏移量 500 的虚拟地址，

我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。

## 2-4：内存分段缺陷

1. 内存碎片的问题。

   比如说有 1G 的物理内存，用户执行了多个程序，其中：
         QQ占用了512MB内存，浏览器占用了128MB内存，音乐占用了256 MB内存。
这个时候，如果我们关闭了浏览器，则空闲内存还有256MB。
如果这个256MB不是连续的，被分成了两段128MB内存，这就会导致没有空间再打开一个200MB的程序。
这样就会在两处产生内存碎片问题：
    1）外部内存碎片，也就是产生了多个不连续的小物理内存，
                     导致新的程序无法被装载；
    2）内部内存碎片，程序所有的内存都被装载到了物理内存，
                     但是这个程序有部分的内存可能并不是很常使用
                     这也会导致内存的浪费；

2. 内存交换的效率低的问题

   对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，
   产生了内存碎片，那不得不重新 Swap 内存区域，这个过程会产生性能瓶颈。
   因为硬盘的访问速度要比内存慢太多了，每一次内存交换，
   我们都需要把一大段连续的内存数据写到硬盘上。
   所以，如果内存交换的时候，
   交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。




## 2-5：如何解决内存分段的缺陷

1. 解决外部内存碎片的问题就是内存交换。

   比如说可以把音乐程序占用的那256MB内存写到硬盘上，
   然后再从硬盘上读回来到内存里。
   不过再读回的时候，我们不能装载回原来的位置，
   而是紧紧跟着那已经被占用了的 512MB 内存后面。
   这样就能空缺出连续的 256MB 空间，
   于是新的 200MB 程序就可以装载进来。

2. 使用内存分页来解决

# 3. 内存管理-内存分页

## 3-1：为什么有内存分页（内存分页定义）

虽然分段的好处就是能产生连续的内存空间，
但是会出现内存碎片和内存交换的空间太大的问题。
要解决这些问题，那么就要想出能少出现一些内存碎片的办法。
另外，当需要进行内存交换的时候，
让需要交换写入或者从磁盘装载的数据更少一点，
这样就可以解决问题了。这个办法，也就是内存分页（Paging）。
分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。
这样一个连续并且尺寸固定的内存空间，这个也就是页。
在 Linux 下，每一页的大小为 4KB。虚拟地址与物理地址之间通过页表来映射

页表实际上存储在 CPU 的内存管理单元（MMU）中，
于是 CPU 就可以直接通过 MMU，找出要实际要访问的物理内存地址。
而当进程访问的虚拟地址在页表中查不到时，
系统会产生一个缺页异常，
进入系统内核空间分配物理内存、更新进程页表，
最后再返回用户空间，恢复进程的运行。

## 3-2：分页是怎么解决分段的内存碎片、内存交换效率低的问题？

因为采用了分页，那么释放的内存都是以页为单位释放的，
也就不会产生无法给进程使用的小内存。
如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」
的内存页面给释放掉，也就是暂时写在硬盘上，也是换出（Swap Out）。
一旦需要的时候，再加载进来，称为换入（Swap In）。
所以，一次性写入磁盘的也只有少数的一个页或者几个页，
不会花太多时间，内存交换的效率就相对比较高。
分页的方式使得我们在加载程序的时候，
不再需要一次性都把程序加载到物理内存中。
我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，
并不真的把页加载到物理内存里，而是只有在程序运行中，
需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。

## 3-3：分页机制下，虚拟地址和物理地址是如何映射的？

在分页机制下，虚拟地址分为两部分，
页号和页内偏移。页号作为页表的索引，
页表包含物理页每页所在物理内存的基地址，
这个基地址与页内偏移的组合就形成了物理内存地址

对于一个内存地址转换，其实就是这样三个步骤：

1. 第一步把虚拟内存地址，切分成页号和偏移量；

2. 第二步根据页号，从页表里面，查询对应的物理页号；

3. 第三步直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

## 3-4：简单的分页有什么缺陷吗？

有空间上的缺陷。

因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。
比如说在32位的环境下，虚拟地址空间共有4GB，如果说一个页的大小是4KB（2^12），
那么就需要大约几百万 （2^20）个页，
每个「页表项」需要 4 个字节大小来存储，
那么整个 4GB 空间的映射就需要有 4MB 的内存来存储页表。
这 4MB 大小的页表，看起来也不是很大。
但是因为每个进程都是有自己的虚拟地址空间的，也是有自己的页表的。
那么，100 个进程的话，就需要 400MB 的内存来存储页表，
这是非常大的内存了，

## 3-5：简单的分页缺陷的解决方案

要解决简单的分页问题，就需要采用多级页表（Multi-Level Page Table）的解决方案。

因为对于单页表的实现方式，在32位和页大小4KB的环境下，
一个进程的页表需要装下100多万个「页表项」，
并且每个页表项是占用4字节大小的，
于是相当于每个页表需占用4MB大小的空间。
我们把这个100多万个「页表项」的单级页表再分页，
将页表（一级页表）分为 1024 个页表（二级页表），
每个表（二级页表）中包含 1024 个「页表项」，形成二级分页。

## 3-6：分了二级表，内存不是变大了呢

每个进程都有 4GB 的虚拟地址空间，
而显然对于大多数程序来说，其使用到的空间远未达到 4GB，
因为会存在部分对应的页表项都是空的，根本没有分配，
对于已分配的页表项，如果存在最近一定时间未访问的页表，
在物理内存紧张的情况下，操作系统会将页面换出到硬盘，
也就是说不会占用物理内存。
如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，
但如果某个一级页表的页表项没有被用到，
也就不需要创建这个页表项对应的二级页表了，
也就是可以在需要时才创建二级页表。

## 3-7：为什么不分级的页表就做不到这样节约内存呢？

因为保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。
假如虚拟地址在页表中找不到对应的页表项，
计算机系统就不能工作了。
所以页表一定要覆盖全部虚拟地址空间，
而不分级的页表就需要有 100 多万个页表项来映射，
而二级分页则只需要1024个页表项
（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。
我们可以把二级分页再推广到多级页表，
就会发现页表占用的内存空间更少了，
这样其实对于64位的系统来说，两级分页肯定不够了，就变成了四级目录，

1. 全局页目录项 PGD（Page Global Directory）；
2. 上层页目录项 PUD（Page Upper Directory）；
3. 中间页目录项 PMD（Page Middle Directory）；
4. 页表项 PTE（Page Table Entry）；

## 3-8：多级页表的缺陷以及解决方案

多级页表虽然解决了空间上的问题，
但是虚拟地址到物理地址的转换就多了几道转换的工序，
这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。
程序是有局部性的，即在一段时间内，
整个程序的执行仅限于程序中的某一部分。
相应地，执行所访问的存储空间也局限于某个内存区域。
然后，在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，
这个 Cache 就是 TLB（Translation Lookaside Buffer） ，
通常称为页表缓存、转址旁路缓存、快表等
在 CPU 芯片里面，封装了内存管理单元（Memory Management Unit）芯片，
它用来完成地址转换和 TLB 的访问与交互。
有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。
TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。

# 4.内存管理-段页式内存管理

## 4-1：什么是段页式内存管理

因为内存分段和内存分页并不是对立的，
它们是可以组合起来在同一个系统中使用的，
那么组合起来后，通常称为段页式内存管理

## 4-2：段页式内存管理实现的方式

先将程序划分为多个有逻辑意义的段
接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；
这样，地址结构就由段号、段内页号和页内位移三部分组成。
用于段页式地址变换的数据结构是每一个程序一张段表，
每个段又建立一张页表，段表中的地址是页表的起始地址，
而页表中的地址则为某页的物理页号
段页式地址变换中要得到物理地址须经过三次内存访问：
第一次访问段表，得到页表起始地址；
第二次访问页表，得到物理页号；
第三次将物理页号与页内位移组合，得到物理地址。

# 5. 内存管理-linux内存管理

## 5-1：Linux 操作系统采用了哪种方式来管理内存

Linux 内存主要采用的是页式内存管理，
但同时也不可避免地涉及了段机制

Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），
也就是所有的段的起始地址都是一样的。
在Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，
所面对的地址空间都是线性地址空间（虚拟地址），
这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护

## 5-2：Linux 的虚拟地址空间是如何分布的？

在 Linux 操作系统中，
虚拟地址空间的内部又被分为内核空间和用户空间两部分，

## 5-3：内核空间与用户空间的区别

1. 进程在用户态时，只能访问用户空间内存；

2. 只有进入内核态后，才可以访问内核空间的内存；

虽然每个进程都各自有独立的虚拟内存，
但是每个虚拟内存中的内核地址，
其实关联的都是相同的物理内存。
这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

## 5-4：用户空间分布

有 7 种不同的内存段：

1. 程序文件段，包括二进制可执行代码；

2. 已初始化数据段，包括静态常量；

3. 未初始化数据段，包括未初始化的静态变量；

4. 堆段，包括动态分配的内存，从低地址开始向上增长；

5. 文件映射段，包括动态库、共享内存等，
   从低地址开始向上增长（跟硬件和内核版本有关）

6. 栈段，包括局部变量和函数调用的上下文等。
   栈的大小是固定的，一般是 8 MB。
   当然系统也提供了参数，以便我们自定义大小；


# 6.用户态和内核态

内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。

用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。

## 6-1：为什么要有用户态和内核态？

由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 
或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级 -- 用户态和内核态。

## 6-2：用户态与内核态的切换

用户态程序将一些数据值放在寄存器中, 或者使用参数创建一个堆栈(stack frame), 以此表明需要操作系统提供的服务.
用户态程序执行陷阱指令
CPU切换到内核态, 并跳到位于内存指定位置的指令, 这些指令是操作系统的一部分, 他们具有内存保护, 不可被用户态程序访问
这些指令称之为陷阱(trap)或者系统调用处理器(system call handler). 他们会读取程序放入内存的数据参数, 并执行程序请求的服务
系统调用完成后, 操作系统会重置CPU为用户态并返回系统调用的结果

## 6-3：用户态和内核态的概念区别

这两种状态的主要差别是

处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的
处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的。

内核态与用户态是操作系统的两种运行级别，当程序运行在3级特权级上时，
就可以称之为运行在用户态。因为这是最低特权级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态；

当程序运行在0级特权级上时，就可以称之为运行在内核态。

运行在用户态下的程序不能直接访问操作系统内核数据结构和程序。
当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态（比如操作硬件）。






# 7.内存管理-页面置换算法

## 7-1：页面置换算法

1. OPT ⻚⾯置换算法（最佳⻚⾯置换算法） ：
   
   最佳(Optimal, OPT)置换算法所选择的被淘汰⻚⾯将是以后永不使⽤的，
   或者是在最⻓时间内不再被访问的⻚⾯,这样可以保证获得最低的缺⻚率。
   但由于⼈们⽬前⽆法预知进程在内存下的若千⻚⾯中哪个是未来最⻓时间内不再被访问的，
   因⽽该算法⽆法实现。⼀般作为衡量其他置换算法的⽅法。

2. FIFO（First In First Out） ⻚⾯置换算法（先进先出⻚⾯置换算法） : 
   
   总是淘汰最先进⼊内存的⻚⾯，即选择在内存中驻留时间最久的⻚⾯进⾏淘汰。

3. LRU （Least Currently Used）⻚⾯置换算法（最近最久未使⽤⻚⾯置换算法） ： 
   
   LRU算法赋予每个⻚⾯⼀个访问字段，⽤来记录⼀个⻚⾯⾃上次被访问以来所经历的时间 T，
   当须淘汰⼀个⻚⾯时，选择现有⻚⾯中其 T 值最⼤的，即最近最久未使⽤的⻚⾯予以淘汰。

4. LFU （Least Frequently Used）⻚⾯置换算法（最少使⽤⻚⾯置换算法） : 
   
   该置换算法选择在之前时期使⽤最少的⻚⾯作为淘汰⻚。

## 6-2：⻚⾯置换算法的作⽤?

地址映射过程中，若在⻚⾯中发现所要访问的⻚⾯不在内存中，则发⽣缺⻚中断。
缺⻚中断 就是要访问的⻚不在主存，需要操作系统将其调⼊主存后再进⾏访问。 在这个时候，被
内存映射的⽂件实际上成了⼀个分⻚交换⽂件。
当发⽣缺⻚中断时，如果当前内存中并没有空闲的⻚⾯，操作系统就必须在内存选择⼀个⻚⾯将其移出
内存，以便为即将调⼊的⻚⾯让出空间。⽤来选择淘汰哪⼀⻚的规则叫做⻚⾯置换算法，我们可以把⻚
⾯置换算法看成是淘汰⻚⾯的规则。

## 6-3：手写LRU缓存

主要思路就是

1.可以使用最基础的单向链表处理
2.使用双向链表,可以加入hash表做优化
3.最简单的实现是使用JDK中自带的LinkedHashMap,
  需要重写removeEldestEntry()方法,
  这是LinkedHashMap提供的一个删除最老条目的方法;

```java
class LRUNode{
    private String key;
    private String value;
    private LRUNode pre;
    private LRUNode next;

    public LRUNode(String key, String value) {
        this.key = key;
        this.value = value;
    }
}


private volatile LRUNode head;
private volatile Integer lenght = 0;

public LRUcache(Integer lenght){
    this.lenght = lenght;
}

public  boolean put(String key,String value){

    LRUNode cur = this.head;
    Integer curLenght = 1;
    while (null != cur && cur.next != null){
        curLenght++;
        if(cur.key.equals(key)){
            //包含 修改其前后node的指向
            cur.pre.next = cur.next;
            cur.next.pre = cur.pre;

            cur.pre = null;
            cur.next = head;
            head.pre = cur;
            return true;
        }else {
            cur = cur.next;
        }
    }
    //没有找到
    LRUNode newNode = new LRUNode(key,value);
    if(curLenght >= lenght){
        //删除最后一个对象
        cur.pre.next = null;
        cur.pre = null;
        newNode.next = head;
        head.pre = newNode;

    }else{
        if(head != null){
            head.pre = newNode;
            newNode.next = head;
        }
    }
    head = newNode;
    return true;
}

public LRUNode get(String key){
    if(key.isEmpty()){
        throw new RuntimeException("key not is find");
    }
    LRUNode cur = head;
    while (lenght != 0 && cur != null){
        if(cur.key.equals(key)){
            if(null != cur.pre && null != cur.next){
                cur.pre.next = cur.next;
                cur.next.pre = cur.pre;

            }else if(null != cur.pre){
                cur.pre.next = null;
            }

            cur.pre = null;
            cur.next = head;
            head.pre = cur;
            head = cur;
            return cur;
        }
        cur = cur.next;
    }
    throw new NullPointerException("this key not find");
}
```

# 8.调度算法


## 1. 进程调度算法

### -1-1：什么时候会发⽣ CPU 调度呢

1. 当进程从运⾏状态转到等待状态；
2. 当进程从运⾏状态转到就绪状态；
3. 当进程从等待状态转到就绪状态；
4. 当进程从运⾏状态转到终⽌状态；

### -1-2：调度算法

1. 先来先服务调度算法
2. 最短作业优先调度算法
3. 高响应比优先调度算法
4. 时间片轮转调度算法
5. 最高优先级调度算法
6. 多级反馈队列调度算法

7. `非抢占式的先来先服务（First Come First Severd, FCFS）算法`
   先来后到，每次从就绪队列选择最先进入队列的进程，
   然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。
   这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。
   FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。
8. `最短作业优先（Shortest Job First, SJF）调度算法 `
   它会优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。
   这显然对长作业不利，很容易造成一种极端现象。
   比如，一个长作业在就绪队列等待运行，
   而这个就绪队列有非常多的短作业，
   那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。
9. `高响应比优先 （Highest Response Ratio Next, HRRN）调度算法`
   主要是权衡了短作业和长作业。
   每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，  
   响应比优先级的计算公式：
   优先权=（等待时间+要求服务时间）/要求服务时间
   - 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，
      「响应比」就越高，这样短作业的进程容易被选中运行；
   - 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，
   这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，
   当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；
10. `时间片轮转（Round Robin, RR）调度算法`
   每个进程被分配一个时间段，称为时间片（Quantum），即允许该进程在该时间段中运行。
   如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；
   如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；
   另外，时间片的长度就是一个很关键的点：
   - 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
   - 如果设得太长又可能引起对短作业进程的响应时间变长；
   注：通常时间片设为 20ms~50ms 通常是一个比较合理的折中值。
11. `最高优先级调度算法`
   希望调度程序能从就绪队列中选择最高优先级的进程进行运行，
   进程的优先级可以分为，静态优先级或动态优先级：
   - 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
   - 动态优先级：根据进程的动态变化调整优先级， 
   比如如果进程运行时间增加，则降低其优先级，
   如果进程等待时间（就绪队列的等待时间）增加， 
   则升高其优先级，也就是随着时间的推移增加等待进程的优先级。
   该算法也有两种处理优先级高的方法，非抢占式和抢占式：
   非抢占式：当就绪队列中出现优先级高的进程，
   运行完当前进程，再选择优先级高的进程。
   抢占式：当就绪队列中出现优先级高的进程，
   当前进程挂起，调度优先级高的进程运行。
   但是依然有缺点，可能会导致低优先级的进程永远不会运行。
12. `多级反馈队列（Multilevel Feedback Queue）调度算法`
   「多级」表示有多个队列，每个队列优先级从高到低，
   同时优先级越高时间片越短。
   「反馈」表示如果有新的进程加入优先级高的队列时，
   立刻停止当前正在运行的进程，转而去运行优先级高的队列；
   设置了多个队列，赋予每个队列不同的优先级，
   每个队列优先级从高到低，同时优先级越高时间片越短；
   新的进程会被放入到第一级队列的末尾，
   按先来先服务的原则排队等待被调度，
   如果在第一级队列规定的时间片没运行完成，
   则将其转入到第二级队列的末尾，以此类推，直至完成；
   当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。
   如果进程运行时，有新进程进入较高优先级的队列，
   则停止当前运行的进程并将其移入到原队列末尾，
   接着让较高优先级的进程运行；可以发现，
   对于短作业可能可以在第一级队列很快被处理完。
   对于长作业，如果在第一级队列处理不完，
   可以移入下次队列等待被执行，
   虽然等待的时间变长了，但是运行时间也会更长了，
   所以该算法很好的兼顾了长短作业，同时有较好的响应时间。
   ⾏；

## 2.页面置换算法

### -2-1：缺页

当 CPU 访问的⻚⾯不在物理内存时，便会产⽣⼀个缺⻚中断，请求操作系统将所缺⻚调⼊到物理内存。那
它与⼀般中断的主要区别在于：

1. 缺⻚中断在指令执⾏「期间」产⽣和处理中断信号，
   ⽽⼀般中断在⼀条指令执⾏「完成」后检查和处理中断信号。

2. 缺⻚中断返回到该指令的开始重新执⾏「该指令」，
   ⽽⼀般中断返回回到该指令的「下⼀个指令」执⾏。

### -2-2：缺页中断流程

1. 在 CPU ⾥访问⼀条 Load M 指令，然后 CPU 会去找 M 所对应的⻚表项。
2. 如果该⻚表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「⽆效
的」，则 CPU 则会发送缺⻚中断请求。
3. 操作系统收到了缺⻚中断，则会执⾏缺⻚中断处理函数，先会查找该⻚⾯在磁盘中的⻚⾯的位置。
4. 找到磁盘中对应的⻚⾯后，需要把该⻚⾯换⼊到物理内存中，但是在换⼊前，需要在物理内存中找空
闲⻚，如果找到空闲⻚，就把⻚⾯换⼊到物理内存中。
     找不到空闲⻚的话，就说明此时内存已满了，这时候，就需要「⻚⾯置换算法」选择⼀个物理⻚，如果该
     物理⻚有被修改过（脏⻚），则把它换出到磁盘，然后把该被置换出去的⻚表项的状态改成「⽆效的」，
     最后把正在访问的⻚⾯装⼊到这个物理⻚中。
5. ⻚⾯从磁盘换⼊到物理内存完成后，则把⻚表项中的状态位修改为「有效的」。
6. 最后，CPU 重新执⾏导致缺⻚异常的指令。
   
### -2-3：页面置换算法

1. 最优页面置换算法
当一个缺页中断发生时，对于保存在内存中的每一个逻辑页面，
计算在它的下一次访问之前，还需要等待多长时间，
从中选择时间最长的那个，作为被置换的页面
这是一种理想的页面置换算法，
在实际系统中是无法实现的，
因为操作系统无从知道每一个页面要等待多长时间
以后才会被访问可以作为其它算法性能评价的依据
2. 先进先出页面置换算法(First-In First-Out，FIFO)
选择在内存中驻留时间最长的页面并淘汰它
OS维护一个链表，记录了所有页面位于内存当中的逻辑页面，
从链表的排列顺序来看，链首页面的驻留时间最长，
页尾页面的驻留时间最短，当发生一个缺页中断时，
把链首页面淘汰出去，并把新的页面添加到链表的末尾
3. 最近最少使用算法(Least Recently Used，LRU)
当一个缺页中断发生时，选择最久未使用的那个页面，并淘汰它
此算法是最优页面置换算的一个近似，其依据是程序的局部性原理，
即在最近一小段时间内，如果某些页面被频繁的访问，
那么在将来的一段时间内，它们还可能会再次被频繁的访问；
反之，如果在过去某些页面长时间未被访问，
那么在将来他们还可能会长时间得不到访问
4. 时钟页面置换算法
需要用到页表项当中的访问位，
当一个页面被装入内存中时，
把该位初始化为0，
然后如果这个页面被访问(读/写)，
则把该位置1
把各个页面组织成环形链表(类似钟表面)，
把指针指向最老的页面(最先进来的)
当发生一个缺页中断时，考察指针所指向的最老页面，
若它的访问位为0，立即淘汰；若访问位为1，
则把该位置0，然后指针往下移动一个，
如此下去，直到找到被淘汰的页面，
然后把指针移动到它的下一个
5. 二次机会算法
此方法与时钟页面置换算法有些类似，
只是二次机会算法考察的是页表项中的 
access 和 dirty 两个位
还是将各个页面组织成环形链表，
当发生缺页中断时，
考察 access 和 dirty两个位，
进行第一轮扫描若找到两个位都是0的页，
直接淘汰；第一轮没有淘汰页，
第二轮扫描 (access == 0 && dirty == 1) 的页，
找到直接淘汰掉；第二轮扫描没有淘汰页，
第三轮扫描将 access 位全部置0，再进行前两轮扫描
6. 最不经常用算法(Least Frequently Used，LFU)
当缺页中断发生时，选择访问次数最少的那个页面，并淘汰之

## 3.磁盘调度算法

磁盘调度算法的⽬的很简单，就是为了提⾼磁盘的访问性能，⼀般是通过优化磁盘的访问请求顺序来做到
的。

## -3-1：常用算法

1. 先来先服务算法

先到来的请求，先被服务。

2. 最短寻道时间优先算法

优先选择从当前磁头位置所需寻道时
间最短的请求

3. 扫描算法算法

可以规定：磁头在⼀个⽅向上移动，访问所有未完成的请求，直到磁头到达该⽅向上
的最后的磁道，才调换⽅向，

4. 循环扫描算法

只有磁头朝某个特定⽅向移动时，才处理磁道访问请求，⽽返
回时直接快速移动⾄最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请
求，该算法的特点，就是磁道只响应⼀个⽅向上的请求。

5. LOOK 与 C-LOOK 算法

## 单核cpu需要考虑线程安全

因为单核cpu仍然存在线程切换，在执行非原子操作的时候，仍然存在线程问题。

# 7.文件系统组成

## 7-1：什么是文件系统

文件系统是操作系统中负责管理持久数据的子系统，
就是负责把用户的文件存到磁盘硬件中，
因为即使计算机断电了，磁盘里的数据并不会丢失，
所以可以持久化的保存文件。

## 7-2：文件系统的组成

每个⽂件分配两个数据结构：索引节点（index node）和⽬录项（directory
entry），它们主要⽤来记录⽂件的元信息和⽬录层次结构。

1. 索引节点，也就是 inode，⽤来记录⽂件的元信息，⽐如 inode 编号、⽂件⼤⼩、访问权限、创建时
            间、修改时间、数据在磁盘的位置等等。索引节点是⽂件的唯⼀标识，它们之间⼀⼀对应，也同样都
            会被存储在硬盘中，所以索引节点同样占⽤磁盘空间。
2. ⽬录项，也就是 dentry，⽤来记录⽂件的名字、索引节点指针以及与其他⽬录项的层级关联关系。多
          个⽬录项关联起来，就会形成⽬录结构，但它与索引节点不同的是，⽬录项是由内核维护的⼀个数据
          结构，不存放于磁盘，⽽是缓存在内存。


### 7-2-1：⽬录项和⽬录是⼀个东⻄呢

不是⼀个东⻄，⽬录是个⽂件，持久化存储在磁盘，
⽬录项是内核⼀个数据结构，缓存在内存。

如果查询⽬录频繁从磁盘读，效率会很低，所以内核会把已经读过的⽬录⽤⽬录项这个数据结构缓存在内
存，下次再次读到相同的⽬录时，只需从内存读就可以，⼤⼤提⾼了⽂件系统的效率。

⽬录项这个数据结构不只是表示⽬录，也是可以表示⽂件的。

### 7-2-2：⽂件数据是如何存储在磁盘


# 8-1：键盘敲⼊字⺟时，期间发⽣了什么？

当⽤户输⼊了键盘字符，键盘控制器就会产⽣扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接
着键盘控制器通过总线给 CPU 发送中断请求。
CPU 收到中断请求后，操作系统会保存被中断进程的 CPU 上下⽂，然后调⽤键盘的中断处理程序。
键盘的中断处理程序是在键盘驱动程序初始化时注册的，那键盘中断处理函数的功能就是从键盘控制器的
寄存器的缓冲区读取扫描码，再根据扫描码找到⽤户在键盘输⼊的字符，如果输⼊的字符是显示字符，那
就会把扫描码翻译成对应显示字符的 ASCII 码，⽐如⽤户在键盘输⼊的是字⺟ A，是显示字符，于是就会
把扫描码翻译成 A 字符的 ASCII 码。
得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏
幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲
区队列」的数据⼀个⼀个写⼊到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕
⾥。
显示出结果后，恢复被中断进程的上下⽂。