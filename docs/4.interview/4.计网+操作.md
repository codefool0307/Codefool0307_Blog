# 1.各层协议

## 1-1：OSI与TCP/IP各层的结构与功能,都有哪些协议?

7. `应用层`
为应用程序提供服务并且规定通信的规范和细节
   常见的协议:
   * HTTP(超文本传输协议)
   * FTP(文件传输协议)
   * TELNET(远程登录协议)
   * SMTP(简单邮件传输协议)
   * DNS(域名解析协议)
6. `表示层`
主要负责数据格式的转换
5. `会话层`
负责建立和断开通信连接
   * 结构化查询语言（ SQL, Structured Query Language ）
   * 网络文件系统（ NFS ， Network File System ）
   * 远程过程调用（ RPC ， Remote Procedure Call ）
4. `传输层`
   是唯一负责总体的数据传输和数据控制的一层。
   * TCP: ~~面向连接 ,可靠性强, 传输效率低~~
   * UDP: ~~无连接,可靠性弱,传输效率快~~
3. `网络层`
   将数据传输到目标地址；主要负责寻找地址和路由选择，网络层还可以实现拥塞控制、网际互连等功能
   * IP
   * IPX
   * RIP
   * OSPF等
2. `数据链路层`
   物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。
   * ARP<font color=pink size='6'>（比较有争议，因为有的书籍放置了在网络层，有的书籍放置在了数据链路层，主要看）</font>
        注：最好的回答是，ARP 是询问具有某个 IP 地址的设备的 MAC 地址的，介于数据链路层和网络层之间。有的划分为数据链路层，这是根据封装方式来划分的，ARP 数据包封装成帧。有的划分为网络层，这是根据协议内容划分的，ARP 报文中有 IP 信息。我觉得应该划分为链路层。
     <font color='#2345'>华为曾经面试问过我</font>
   * RARP
   * SDLC
   * HDLC
   * PPP
   * STP
   * 帧中继等
3. `物理层`
   负责0、1比特流(0/1序列)与电压的高低、光的闪灭之间的转换

## 1-2：⽹络层与数据链路层有什么关系呢？

1. IP 的作⽤是主机之间通信⽤的，负责在「没有直连」的两个⽹络之间进⾏通信传输
2. MAC 的作⽤则是实现「直连」的两个设备之间通信。
理解一下：
就比如说，你想从xx村到海南市，你不得做公交车、汽车、火车、轮船到海南
那么你这整个的一个路线图，就是一个网络层，行程开端就是xx村---->>>源IP，目的IP---->行程结束就是海南
那么我从xx村到xx镇相当于是在这个区间内移动路线，也就是数据链路层，
其中，xx村好⽐源 MAC 地址，xx镇好⽐⽬的 MAC 地址。
（只要是在线路（网络层包含的都是））
这个xx村、海南不会发生变化，但是中间的位置会一直在变，也就是说
IP源、目的不会变， Mac源、目的会变化

## 1-3：网络层的路由算法

修改中

# 1.Get与Post

## 1-1：get与post的区别

1. Get是请求从服务器获取资源，Post用于传输实体本体
2. get和post请求都能使用额外的参数，
   get参数是以查询字符串出现在URL中，post参数存储在实体主体中
3. Http方法不会改变服务器状态，get方法是安全的，
   而post由于是传送实体主体内容，这个内容可能是用户上传的表单数据，
   上传成功后，服务器可能把这个数据存储到数据库中，因此状态也就发生了变化
4. get在调用多次时，客户端收到的结果是一样的，所以是幕等；
   post调用多次，会增加多行记录，不是幕等
5. get可缓存，post不可缓存
6. 对于get请求，浏览器会把http 头和数据一并发送出去，服务器响应200；
   post请求，浏览器先发送header，服务器响应之后，浏览器在发送数据，服务器响应200

## 1-2：后台获取前端数据方法

1. Servlet
```java
@RequestMapping("allCity3")
    public String queryAllCity3(HttpServletRequest request, HttpServletResponse response) {
        List<City> cities = cityService.queryAllCity();
        for (City city : cities) {
            city.setAreas(null);
        }
        // 保存到session中
        /*HttpSession session = request.getSession();
        session.setAttribute("cities", cities);*/
        // 保存到request中
        request.setAttribute("cities", cities);
        // 返回的页面（这里配合相应的springMvc视图解析器）
        return "main";
    }
```
2. ModelAndView
把数据保存到ModelAndView中，直接传送到前端页面
```java
// 把要保存的数据保存进ModelAndView中
modelAndView.addObject("areaStatisticsList", areaStatisticsList);
// 设置跳转页面
modelAndView.setViewName("areaStatistics");
// 返回值
return modelAndView;
```
3. 通过@RequestParam获取
   形式如：@RequestParam(value="username") String userName
4. 通过@RequestBody注解获取
   主要用来接收前端传递给后端的json数据(后台函数形参可以String，也可以是类)；
   GET方式无请求体，所以使用@RequestBody接收数据时，
   前端不能使用GET方式提交数据，而是用POST方式进行提交。
   @RequestBody一般用来处理非Content-Type: application/x-www-form-urlencoded编码格式的数据。

## 1-3：如何通过GET方式上传压缩包




# 2.报文结构与状态码

## 2-1：状态码

2xx （3种）
   1. 200 ：表示从客户端发送给服务器的请求被正常处理并返回；
   2. 204 ：表示客户端发送给客户端的请求得到了成功处理，
   3. 206 ：表示客户端进行了范围请求。
3xx （5种）
   1. 301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；
   2. 302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；
         301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）
   3. 303 See Other：表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；
         302与303的区别：后者明确表示客户端应当采用GET方式获取资源
   4. 304 Not Modified：表示客户端发送附带条件
                       （是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、
                         If-None-Match、If-Range、If-Unmodified-Since中任一首部）
                         的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回改状态码；
   5. 307 Temporary Redirect：临时重定向，与303有着相同的含义，
      307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；
4xx （4种）
   1. 400 Bad Request：表示请求报文中存在语法错误；
   2. 401 Unauthorized：未经许可，需要通过HTTP认证；
   3. 403 Forbidden：服务器拒绝该次访问（访问权限出现问题）
   4. 404 Not Found：表示服务器上无法找到请求的资源，除此之外，
                     也可以在服务器拒绝请求但不想给拒绝原因时使用；
5xx （2种）
   1. 500 Inter Server Error：表示服务器在执行请求时发生了错误，
                              也有可能是web应用存在的bug或某些临时的错误时；
   2. 503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，
                              无法处理请求；

## 2-2：HTTP请求组成

一个HTTP请求报文由四个部分组成：请求行、请求头、空行、请求数据。
1. 请求行
   请求行由请求方法字段、URL字段和HTTP协议版本字段3个字段组成，
   它们用空格分隔。比如 GET /data/info.html HTTP/1.1
2. 请求头部
   HTTP客户程序(例如浏览器)，向服务器发送请求的时候必须指明请求类型(一般是GET或者 POST)。
   如有必要，客户程序还可以选择发送其他的请求头。大多数请求头并不是必需的，。
   常见的请求头字段以及他的含义：
      1. Accept：浏览器可接受的MIME类型。
      2. Accept-Charset：浏览器可接受的字符集。
      3. Accept-Encoding：浏览器能够进行解码的数据编码方式，
                          比如gzip。Servlet能够向支持gzip的浏览器
                          返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。
      4. Accept-Language：浏览器语言种类，
      5. Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头应答
      6. Content-Length：表示请求消息正文的长度。
      7. Host： 客户机告诉服务器，想访问的主机名。
      8. If-Modified-Since：客户机告诉服务器，资源的缓存时间。
      9. Referer：客户机告诉服务器，它是从哪个资源来访问服务器的(防盗链)。
      10. User-Agent：User-Agent头域的内容包含发出请求的用户信息。
      11. Cookie：客户机通过这个头可以向服务器带数据，这是最重要的请求头信息之一。
3. 空行
   它的作用是通过一个空行，告诉服务器请求头部到此为止。
4. 请求数据
  
   若方法字段是POST,则通常来说此处放置的就是要提交的数据

# 3.计算机网络-HTTP-HTTP的1.0-3.0

## 3-1：HTTP-1.0

### 3-1-1：HTTP-1.0优缺点

<font color=red size='5'>I.优点</font>

1. HTTP基本的报⽂格式就是header + body，头部信息也是key-value简单⽂本的形式。
2. HTTP协议⾥的各类请求⽅法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，
   都允许开发⼈员⾃定义和扩充。
3. HTTP由于是⼯作在应⽤层，则它下层可以随意变化。
4. 应⽤⼴泛和跨平台

<font color=red size='5'>II.缺点</font>

1. ⽆状态
   由于⽆状态，它在完成有关联性的操作时会⾮常麻烦。
   例如登录->添加购物⻋->下单->结算->⽀付，这系列操作都要知道⽤户的身份才⾏。
   但服务器不知道这些请求是有关联的，每次都要问⼀遍身份信息。
2. 明⽂传输
   明⽂意味着在传输过程中的信息，是可⽅便阅读的，
   通过浏览器的控制台或抓包软件都可以直接⾁眼查看，
   信息的内容都毫⽆隐私可⾔，很容易就能被窃取。

### 3-1-2：Http为什么是无连接和无状态的

一.无连接
每一个访问都是无连接，
服务器挨个处理访问队列里的访问，
处理完一个就关闭连接，就结束了，
然后处理下一个新的
无连接的含义是限制每次连接只处理一个请求。
服务器处理完客户的请求，并收到客户的应答后，即断开连接

二.无状态
1.协议对于事务处理没有记忆能力
2.对同一个url请求没有上下文关系
3.每次的请求都是独立的，
  它的执行情况和结果与
  前面的请求和之后的请求
  是无直接关系的，
  它不会受前面的请求应答情况
  直接影响，
  也不会直接影响后面的请求应答情况

## 3-2：HTTP-1.1

### 3-2-1：HTTP/1.1相对于HTTP1.0改善

1. 因为早期HTTP/1.0，那就是每发起⼀个请求需要三次握手四次挥手等等操作，
   增加了通信开销。为了解决这些问题，
   HTTP/1.1提出了⻓连接的通信⽅式只要任意⼀端没有明确提出断开连接，
   则保持 TCP 连接状态。
2. HTTP/1.1 采⽤了⻓连接的⽅式，
   可在同⼀个 TCP 连接⾥⾯，客户端可以发起多个请求，
   只要第⼀个请求发出去了，不必等其回来，就可以发第⼆个请求出去，
   可以减少整体的响应时间。
3. 错误状态响应码，
   HTTP1.1新增了很多错误装填响应码，让开发者更加了解错误根源
4. 在HTTP1.0中主要使⽤header⾥的
   If-Modified-Since,Expires来做为缓存判断的标准，
   在HTTP1.1中引⼊了更多的缓存控制策略
5. HTTP1.0中，存在⼀些浪费带宽的现象，
   例如客户端只是需要某个对象的⼀部分，⽽服务器却将整个对象送过来了，
   并且不⽀持断点续传功能，HTTP1.1则在请求头引⼊了range头域，
   它允许只请求资源的某个部分

### 3-2-2：HTTP1.1缺点

1. 延迟难以下降 ，虽然现在⽹络的「带宽」相⽐以前变多了，但是延迟降到⼀定幅度后，
   就很难再下降了，
2. 并发连接有限 ，⾕歌浏览器最⼤并发连接数是 6 个，
   ⽽且每⼀个连接都要经过 TCP 和 TLS 握⼿耗时，以及TCP 慢启动过程给流量带来的影响；
3. 队头阻塞问题 ，同⼀连接只能在完成⼀个 HTTP 事务（请求和响应）后，才能处理下⼀个事务；
4. HTTP  头部巨⼤且重复 ，由于 HTTP 协议是⽆状态的，每⼀个请求都得携带 HTTP 头部，
   特别是对于有携带cookie 的头部，⽽ cookie 的⼤⼩通常很⼤；
5. 不⽀持服务器推送消息 ，因此当客户端需要获取通知时，
   只能通过定时器不断地拉取消息，这⽆疑浪费⼤量了带宽和服务器资源。
6. 报⽂中 Header 部分存在的问题：含很多固定的字段，
   ⽐如Cookie、User Agent、Accept 等，这些字段加起来也⾼达⼏百字节甚⾄上千字节，同时字段重复

### 3-2-3：HTTP⻓连接,短连接(也是TCP连接,短连接)

1. 在HTTP/1.0中默认使⽤短连接。
   客户端和服务器每进⾏⼀次HTTP操作，就建⽴⼀次连接，任务结束就中断连接。
2. 从HTTP/1.1起，默认使⽤⻓连接。
   客户端和服务器之间⽤于传输HTTP数据的TCP连接不会关闭，
   客户端再次访问这个服务器时，会继续使⽤这⼀条已经建⽴的连接

### 3-2-4：HTTP1.1优化方案


第⼀个思路是，通过缓存技术来避免发送 HTTP 请求。客户端收到第⼀个请求的响应后，可以将其缓存在本地磁
盘，下次请求的时候，如果缓存没过期，就直接读取本地缓存的响应数据。如果缓存过期，客户端发送请求的时候
带上响应数据的摘要，服务器⽐对后发现资源没有变化，就发出不带包体的 304 响应，告诉客户端缓存的响应仍然
有效。

第⼆个思路是，减少 HTTP 请求的次数，有以下的⽅法：
   1. 将原本由客户端处理的重定向请求，交给代理服务器处理，这样可以减少重定向请求的次数；
   2. 将多个⼩资源合并成⼀个⼤资源再传输，能够减少 HTTP 请求次数以及 头部的重复传输，再来减少 TCP 连
   接数量，进⽽省去 TCP 握⼿和慢启动的⽹络消耗；
   3. 按需访问资源，只访问当前⽤户看得到/⽤得到的资源，当客户往下滑动，再访问接下来的资源，以此达到延
   迟请求，也就减少了同⼀时间的 HTTP 请求次数。

第三思路是，通过压缩响应资源，降低传输资源的⼤⼩，从⽽提⾼传输效率，所以应当选择更优秀的压缩算法。

1. 将多张⼩图合并成⼀张⼤图供浏览器 JavaScript 来切割使⽤，这样可以将多个请求合并成⼀个请求，但是带
   来了新的问题，当某张⼩图⽚更新了，那么需要重新请求⼤图⽚，浪费了⼤量的⽹络带宽；
2. 将图⽚的⼆进制数据通过 base64 编码后，把编码数据嵌⼊到 HTML 或 CSS ⽂件中，以此来减少⽹络请求
   次数；
3. 将多个体积较⼩的 JavaScript ⽂件使⽤ webpack 等⼯具打包成⼀个体积更⼤的 JavaScript ⽂件，以⼀个请
   求替代了很多个请求，但是带来的问题，当某个 js ⽂件变化了，需要重新请求同⼀个包⾥的所有 js ⽂件；
4. 将同⼀个⻚⾯的资源分散到不同域名，提升并发连接上限，因为浏览器通常对同⼀域名的 HTTP 连接最⼤只
   能是 6 个；

## 3-3：HTTP-2

## http2如何兼容http1.1

第⼀点，HTTP/2 没有在 URI ⾥引⼊新的协议名，仍然⽤「http://」表示明⽂协议，⽤「https://」表示加密协议，
于是只需要浏览器和服务器在背后⾃动升级协议，这样可以让⽤户意识不到协议的升级，很好的实现了协议的平滑升级。
第⼆点，只在应⽤层做了改变，还是基于 TCP 协议传输，应⽤层⽅⾯为了保持功能上的兼容，HTTP/2 把 HTTP 分
解成了「语义」和「语法」两个部分，「语义」层不做改动，与 HTTP/1.1 完全⼀致，⽐如请求⽅法、状态码、头
字段等规则保留不变。

### 3-3-1：HTTP/2 做了什么优化

1. HTTP/2 会压缩头如果你同时发出多个请求，
   他们的头是⼀样的或是相似的，那么，协议会帮你消除重复的部分。
   使用HPACK 算法：在客户端和服务器同时维护⼀张头信息表，所有字段都会存⼊这个表，
   ⽣成⼀个索引号，以后就不发送同样字段了，只发送索引号，这样就提⾼速度了。
2. HTTP/2全⾯采⽤了⼆进制格式，头信息和数据体都是⼆进制，
         计算机收到报⽂后，直接解析⼆进制报⽂，这增加了数据传输的效率。
3. HTTP/2的数据包不是按顺序发送的
4. HTTP/2是可以在⼀个连接中并发多个请求或回应。
5. HTTP/2 还在⼀定程度上改善了传统的「请求 - 应答」⼯作模式，
          服务不再是被动地响应，也可以主动向客户端发送消息。

### 3-3-2：HTTP/2有哪些缺陷

1. HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，
   这样内核才会将缓冲区⾥的数据返回给 HTTP 应⽤，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只
   能存放在内核缓冲区⾥，只有等到这 1 个字节数据到达时，HTTP/2 应⽤层才能从内核中拿到数据，这就是
   HTTP/2 队头阻塞问题。
2. TCP 与 TLS 的握⼿时延迟；
   发起 HTTP 请求时，需要经过 TCP 三次握⼿和 TLS 四次握⼿（TLS 1.2）的过程，
   因此共需要 3 个 RTT 的时延才能发出请求数据。
   还有就是TCP 由于具有「拥塞控制」的特性，所以刚建⽴连接的 TCP 会有个「慢启动」的过程，它会对 TCP 连接
   产⽣"减速"效果。
3. ⽹络迁移需要重新连接；
   ⼀个 TCP 连接是由四元组（源 IP 地址，源端⼝，⽬标 IP 地址，⽬标端⼝）确定的，这意味着如果 IP 地址或者端
   ⼝变动了，就会导致需要 TCP 与 TLS 重新握⼿，这不利于移动设备切换⽹络的场景，⽐如 4G ⽹络环境切换成WIFI。

### 3-3-3：http2做了什么优化

放弃 TCP 协议，转⽽使⽤ UDP 协议作为传输层协
议，HTTP/3 协议就是这样做的


## 3-4：HTTP-3

### 3-4-1：HTTP/3做了哪些优化

HTTP/3使用基于UDP协议的QUIC协议来实现的

HTTP/3把HTTP下层的TCP协议改成了UDP

因为UDP发⽣是不管顺序，也不管丢包的，所以不会出现HTTP/1.1的队头阻塞和HTTP/2的⼀个丢包全部重传问题。

但是由于UDP是不可靠传输的，而基于UDP的QUIC协议可以实现类似TCP的可靠性传输。主要是依赖

1. 当某个流发⽣丢包时，只会阻塞这个流， 其他流不会受到影响。

2.  HTTP/3 的 QPACK 通过两个特殊的单向流来同步双⽅的动态表，解决了 HTTP/2 的 HPACK 队头阻塞问题

3. HTTP/3 在传输数据前虽然需要 QUIC 协议握⼿，这个握⼿过程只需要 1 RTT，
   握⼿的⽬的是为确认双⽅的「连接ID」，连接迁移就是基于连接 ID 实现的。
   由于HTTP/3 的 QUIC 协议并不是与 TLS 分层，⽽是QUIC 内部包含了 TLS，它在⾃⼰的帧会携带 TLS ⾥的“记
   录”，再加上 QUIC 使⽤的是 TLS1.3，因此仅需 1 个 RTT 就可以「同时」完成建⽴连接与密钥协商，甚⾄在第⼆
  次连接的时候，应⽤数据包可以和 QUIC 握⼿信息（连接信息 + TLS 信息）⼀起发送，达到 0-RTT 的效果。

4. 连接迁移，QUIC 协议没有⽤四元组的⽅式来“绑定”连接，⽽是通过「连接 ID 」来标记通信的两个端点，客
   户端和服务器可以各⾃选择⼀组 ID 来标记⾃⼰，因此即使移动设备的⽹络变化后，导致 IP 地址变化了，只
   要仍保有上下⽂信息（⽐如连接 ID、TLS 密钥等），就可以“⽆缝”地复⽤原连接，消除重连的成本；

## 3-5：http工作流程

域名解析 -> 三次握手 -> 发起HTTP请求 -> 响应HTTP请求并得到HTML代码 
-> 浏览器解析HTML代码  -> 浏览器对页面进行渲染呈现给用户

## 3-6：一个 TCP 连接后是否会在一个 HTTP 请求完成后断开？什么情况下会断开？

在 HTTP/1.0 中，一个服务器在发送完一个 HTTP 响应后，会断开 TCP 链接。
但是这样每次请求都会重新建立和断开 TCP 连接，代价过大。所以虽然标准中没有设定，
某些服务器对 Connection: keep-alive 的 Header 进行了支持。
意思是说，完成这个 HTTP 请求之后，不要断开 HTTP 请求使用的 TCP 连接。
这样的好处是连接可以被重新使用，之后发送 HTTP 请求的时候不需要重新建立 TCP 连接，
以及如果维持连接，那么 SSL 的开销也可以避免

# 4.计算机网络-HTTP与HTTPs前世今生

## 4-1：什么是HTTPS

HTTPS是在HTTP上建立SSL加密层，并对传输数据进行加密，是HTTP协议的安全版

## 4-2：HTTPS 解决了 HTTP 的哪些问题？（为什么要HTTPS）

HTTP 由于是明⽂传输，所以安全上存在以下三个⻛险：
1. 窃听⻛险，⽐如通信链路上可以获取通信内容，⽤户号容易没。
2. 篡改⻛险，⽐如强制植⼊垃圾⼴告，视觉污染，⽤户眼容易瞎。
3. 冒充⻛险，⽐如冒充淘宝⽹站，⽤户钱容易没。
HTTPS 在 HTTP 与 TCP 层之间加⼊了 SSL/TLS 协议，可以很好的解决了上述的⻛险：
1. 混合加密的⽅式实现信息的机密性，解决了窃听的⻛险。
   HTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：
   在通信建立 前 采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密；
   在通信过程中 全部使用对称加密的「会话秘钥」的方式加密明文数据。
   采用「混合加密」的方式的原因：
      对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
      非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。
2. 摘要算法的⽅式来实现完整性，它能够为数据⽣成独⼀⽆⼆的「指纹」，
   指纹⽤于校验数据的完整性，解决了篡改的⻛险。
   客户端在发送明文之前会通过摘要算法算出明文的「指纹」，
   发送的时候把「指纹 + 明文」一同加密成密文后，发送给服务器，
   服务器解密后，用相同的摘要算法算出发送过来的明文，
   通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，
   若「指纹」相同，说明数据是完整的。
3. 将服务器公钥放⼊到数字证书中，解决了冒充的⻛险。

### 4-2-1：加密算法

1. 对称加密：密钥只有⼀个，加密解密为同⼀个密码，且加解密速度快，典型的对称加密，算法有DES、 AES等；
   - 优点：算法公开、计算量小、加密速度快、加密效率高，适合加密比较大的数据。
   - 缺点：
          交易双方需要使用相同的密钥，也就无法避免密钥的传输，
          而密钥在传输过程中无法保证不被截获，因此对称加密的安全性得不到保证。
          每对用户每次使用对称加密算法时，都需要使用其他人不知道的惟一密钥，
          这会使得发收信双方所拥有的钥匙数量急剧增长，密钥管理成为双方的负担。
          对称加密算法在分布式网络系统上使用较为困难，
          主要是因为密钥管理困难，使用成本较高。

2. ⾮对称加密：密钥成对出现（且根据公钥⽆法推知私钥，根据私钥也⽆法推知公钥），
               加密解密使⽤不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），
               相对对称加密速度较慢，典型的⾮对称加密算法有RSA、 DSA等。

   - 优点：算法公开，加密和解密使用不同的钥匙，私钥不需要通过网络进行传输，安全性很高。
   - 缺点：计算量比较大，加密和解密速度相比对称加密慢很多。

#### 4-2-1-1：Https对称加解密的过程

发送端和接收端首先要共享相同的密钥k
即通信前双方都需要知道对应的密钥才能进行通信。
发送端用共享密钥k对明文p进行加密，得到密文c，
并将得到的密文发送给接收端，接收端收到密文后，
并用其相同的共享密钥k对密文进行解密，得出明文p。

#### 4-2-1-2：Https非对称加密过程

加密一方找到接收方的公钥e，
大部分的公钥查找工作实际上都是通过数字证书来实现的，
然后用公钥e对明文p进行加密后得到密文c，
并将得到的密文发送给接收方，接收方收到密文后，
用自己保留的私钥d进行解密，得到明文p，
用公钥加密的密文，只有拥有私钥的一方才能解密，
这样就可以解决加密的各方可以统一使用一个公钥即可。

### 4-2-2：https握手过程

在进⾏ HTTP 通信前，需要先进⾏ TLS 握⼿
由于不同的密钥交换算法，TLS 的握⼿过程可能会有⼀些区别。


#### 4-2-2-1：RSA握手

第一次：客户端⾸先会发⼀个「Client Hello」消息
      消息⾥⾯有客户端使⽤的 TLS 版本号、⽀持的密码套件列表，以及⽣成的随机数（Client Random），这个随机
      数会被服务端保留，它是⽣成对称加密密钥的材料之⼀。

第二次：当服务端收到客户端的「Client Hello」消息后，会确认 TLS 版本号是否⽀持，和从密码套件列表中选择⼀个密码
         套件，以及⽣成随机数（Server Random）。
         接着，返回「Server Hello」消息，消息⾥⾯有服务器确认的 TLS 版本号，也给出了随机数（Server Random），
         然后从客户端的密码套件列表选择了⼀个合适的密码套件。基本的形式是「密钥交换算法 +
签名算法 + 对称加密算法 + 摘要算法」
然后，服务端为了证明⾃⼰的身份，会发送「Server Certificate」给客户端，这个消息⾥含有数字证书。
随后，服务端发了「Server Hello Done」消息，⽬的是告诉客户端，我已经把该给你的东⻄都给你了，本次打招
呼完毕。

客户端拿到了服务端的数字证书后，要校验该数字证书是真实有效

第三次握手，客户端验证完证书后，认为可信则继续往下⾛。接着，客户端就会⽣成⼀个新的随机数 (pre-master)，⽤服务器
的 RSA 公钥加密该随机数，通过「Change Cipher Key Exchange」消息传给服务端。
服务端收到后，⽤ RSA 私钥解密，得到客户端发来的随机数 (pre-master)。
⾄此，客户端和服务端双⽅都共享了三个随机数，分别是 Client Random、Server Random、pre-master。
于是，双⽅根据已经得到的三个随机数，⽣成会话密钥（Master Secret），它是对称密钥，⽤于对后续的 HTTP
请求/响应的数据加解密。
⽣成完会话密钥后，然后客户端发⼀个「Change Cipher Spec」，告诉服务端开始使⽤加密⽅式发送消息。

TLS 第四次握⼿
服务器也是同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，如果双⽅都
验证加密和解密没问题，那么握⼿正式完成。
最后，就⽤「会话密钥」加解密 HTTP 请求和响应了。

#### 4-2-2-2：缺点

使⽤ RSA 密钥协商算法的最⼤问题是不⽀持前向保密。因为客户端传递随机数（⽤于⽣成对称加密密钥的条件之
⼀）给服务端时使⽤的是公钥加密的，服务端收到到后，会⽤私钥解密得到随机数。所以⼀旦服务端的私钥泄漏
了，过去被第三⽅截获的所有 TLS 通讯密⽂都会被破解。

为了解决这⼀问题，于是就有了 DH 密钥协商算法

客户端和服务端各⾃会⽣成随机数，并以此作为私钥，然后根据公开的 DH 计算公示算出各⾃的公钥，通过 TLS
握⼿双⽅交换各⾃的公钥，这样双⽅都有⾃⼰的私钥和对⽅的公钥，然后双⽅根据各⾃持有的材料算出⼀个随机
数，这个随机数的值双⽅都是⼀样的，这就可以作为后续对称加密时使⽤的密钥。
DH 密钥交换过程中，即使第三⽅截获了 TLS 握⼿阶段传递的公钥，在不知道的私钥的情况下，也是⽆法计算出
密钥的，⽽且每⼀次对称加密密钥都是实时⽣成的，实现前向保密。
但因为 DH 算法的计算效率问题，会有其他算法升级

### 4-2-1：数字证书认证机构的流程

1. 服务器的运营人员向第三方机构CA提交
   公钥、组织信息、个人信息(域名)等信息并申请认证;
2. CA通过线上、线下等多种手段验证申请者提供信息的真实性，
   如组织是否存在、企业是否合法，是否拥有域名的所有权等;
3. 如信息审核通过，CA会向申请者签发认证文件-证书。
   证书包含以下信息：申请者公钥、申请者的组织信息
   和个人信息、签发机构 CA的信息、有效时间、证书序列号
   等信息的明文，同时包含一个签名。 
   其中签名的产生算法：
   首先，使用散列函数计算公开的明文信息的信息摘要，
   然后，采用 CA的私钥对信息摘要进行加密，密文即签名;
4. 客户端 Client 向服务器 Server 发出请求时，Server 返回证书文件;
5. 客户端 Client 读取证书中的相关的明文信息，
   采用相同的散列函数计算得到信息摘要，
   然后，利用对应 CA的公钥解密签名数据，
   对比证书的信息摘要，
   如果一致，则可以确认证书的合法性，
   即服务器的公开密钥是值得信赖的。
6. 客户端还会验证证书相关的域名信息、有效时间等信息; 
   客户端会内置信任CA的证书信息(包含公钥)，
   如果CA不被信任，则找不到对应 CA的证书，证书也会被判定非法。

## 4-3：HTTP与HTTPS建立请求过程

### 4-3-1：HTTP 请求过程

1. 建立连接完毕以后客户端会发送响应给服务端
2. 服务端接受请求并且做出响应发送给客户端
3. 客户端收到响应并且解析响应响应给客户

### 4-3-2：HTTPS 请求过程（加密过程）

1. Client发起一个HTTPS
   比如一个https网址
   的请求，根据相关规定，
   Client知道需要连接Server的443（默认）端口。
2. Server把事先配置好的
   公钥证书返回给客户端。
3. Client验证公钥证书：比如是否在有效期内，
   证书的用途是不是匹配Client请求的站点，
   是不是在CRL吊销列表里面，
   它的上一级证书是否有效，
   这是一个递归的过程，
   直到验证到根证书
   操作系统内置的Root证书或者Client内置的Root证书）
   如果验证通过则继续，不通过则显示警告信息。
4. Client使用伪随机数生成器
   生成加密所使用的对称密钥，
   然后用证书的公钥加密这个对称密钥，
   发给Server。
5. Server使用自己的私钥解密这个消息，
   得到对称密钥。
   至此，Client和Server双方都持有了相同的对称密钥。
6. Server使用对称密钥加密“明文内容A”，发送给Client。
7. Client使用对称密钥解密响应的密文，得到“明文内容A”。
8. Client再次发起HTTPS的请求，
   使用对称密钥加密请求的“明文内容B”，
   然后Server使用对称密钥解密密文，得到“明文内容B”。

#### 4-3-2-1：SSL/TLS握⼿

1. ClientHello
⾸先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。
在这⼀步，客户端主要向服务器发送以下信息：
（1）客户端⽀持的 SSL/TLS 协议版本。
（2）客户端⽣产用于「会话秘钥」的随机数。
（3）客户端⽀持的密码套件列表。
2. SeverHello
服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello 。服务器回应的内容有如下内容：
（1）确认 SSL/ TLS 协议版本，如果浏览器不⽀持，则关闭加密通信。
（2）服务器⽣产的随机数（ Server Random ），后⾯⽤于⽣产「会话秘钥」。
（3）确认的密码套件列表，如 RSA 加密算法。
（4）服务器的数字证书。
3. 客户端回应
客户端收到服务器的回应之后，⾸先通过浏览器或者操作系统中的 CA 公钥，
确认服务器的数字证书的真实性。
如果证书没有问题，客户端会从数字证书中取出服务器的公钥，
然后使⽤它加密报⽂，向服务器发送如下信息：
（1）⼀个随机数（ pre-master key ）
（2）加密通信算法改变通知
（3）客户端握⼿结束通知
4. 服务器的最后回应
服务器收到客户端的第三个随机数（ pre-master key ）之后，通过协商的加密算法，
计算出本次通信的「会话秘钥」。然后，向客户端发⽣最后的信息：
（1）加密通信算法改变通知。
（2）服务器握⼿结束通知。

### 4-3-3：HTTP与HTTPS区别

1. HTTP 是超⽂本传输协议，信息是明⽂传输，存在安全⻛险的问题。 
   HTTPS 则解决 HTTP 不安全的缺陷，
   在 TCP 和 HTTP ⽹络层之间加⼊了 SSL/TLS 安全协议，使得报⽂能够加密传输。
2. HTTP 连接建⽴相对简单，TCP 三次握⼿之后便可进⾏HTTP的报⽂传输。
   ⽽ HTTPS 在 TCP 三次握⼿之后，还需进⾏SSL/TLS的握⼿过程，才可进⼊加密报⽂传输。
3. HTTP 的端⼝号是 80， HTTPS 的端⼝号是 443。
4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。

### 4-3-4：为何不所有的网站都使用HTTPS

1. 首先，需要权威CA颁发的SSL证书。
   从证书的选择、购买到部署，传统的模式下都会比较耗时耗力。
2. 其次，HTTPS普遍认为性能消耗要大于HTTP，
   因为与纯文本通信相比，加密通信会消耗更多的CPU及内存资源。
   如果每次通信都加密，会消耗相当多的资源，
   平摊到一台计算机上时，
   能够处理的请求数量必定也会随之减少。
   但事实并非如此，
   用户可以通过性能优化、把证书部署在SLB或CDN，
   来解决此问题。
3. 除此之外，想要节约购买证书的开销也是原因之一。
   要进行HTTPS通信，证书是必不可少的。
   而使用的证书必须向认证机构（CA）购买。

### 4-3-4：https性能消耗

1. 第⼀个环节， TLS 协议握⼿过程；
   
   TLS 协议握⼿过程不仅增加了⽹络延时（最⻓可以花费掉 2 RTT），
   ⽽且握⼿过程中的⼀些步骤也会产⽣性能损耗，⽐如：
      对于 ECDHE 密钥协商算法，握⼿过程中会客户端和服务端都需要临时⽣成椭圆曲线公私钥；
      客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，⽬的是验证服务器的证书是否有被吊销；
      双⽅计算 Pre-Master，也就是对称加密密钥；

2. 第⼆个环节，握⼿后的对称加密报⽂传输。

### 4-3-5：HTTPS优化

1. 硬件优化，
   由于HTTPS 协议是计算密集型，⽽不是I/O密集
   选择更好性能的cpu

2. 软件优化，升级linux内核和升级opengl，但有时候也是存在风险的

3. 协议优化，
   
   主要是对密钥交换过程进行优化，

   对于TLS 1.2 版本如果使⽤的是 RSA 密钥交换算法，那么需要 4 次握⼿，也就是要花费 2 RTT，
   才可以进⾏应⽤数据的传输，⽽且 RSA 密钥交换算法不具备前向安全性。
   总之使⽤ RSA 密钥交换算法的 TLS 握⼿过程，不仅慢，⽽且安全性也不⾼。
   因此如果可以，尽量选⽤ ECDHE 密钥交换算法替换 RSA 算法，因为该算法由于⽀持「False Start」，它是“抢跑”
   的意思，客户端可以在 TLS 协议的第 3 次握⼿后，第 4 次握⼿前，发送加密的应⽤数据，以此将 TLS 握⼿的消息
   往返由 2 RTT 减少到 1 RTT，⽽且安全性也⾼，具备前向安全性。

   把 TLS 1.2 升级成 TLS 1.3，TLS 1.3 ⼤幅度简化了握⼿的步骤，完成 TLS 握⼿只要 1
   RTT，⽽且安全性更⾼，TLS 1.3 把 以一二次握手和公钥交换这两个消息合并成了⼀个消息，
   于是这样就减少到只需 1 RTT 就能完成 TLS 握⼿。

4. 证书优化
   
   主要是是证书传输，在一个是证书验证；

   要让证书更便于传输，那必然是减少证书的⼤⼩，这样可以节约带宽，
   也能减少客户端的运算量。所以，对于服务
   器的证书应该选择椭圆曲线（ECDSA）证书，
   ⽽不是 RSA 证书，因为在相同安全强度下， 
   ECC 密钥⻓度⽐ RSA短的多。

   客户端在验证证书时，是个复杂的过程，会⾛证书链逐级验证，验证的过程不仅需要「⽤ CA 公钥解密证书」以及
「⽤签名算法验证证书的完整性」，⽽且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL
或者 OCSP 数据，以此确认证书的有效性。
这个访问过程是 HTTP 访问，因此⼜会产⽣⼀系列⽹络通信的开销，如 DNS 查询、建⽴连接、收发数据等。

   服务器应该开启 OCSP Stapling 功能，由服务器预先获得 OCSP 的响应，并把响应结果缓存起来，这样
TLS 握⼿的时候就不⽤再访问 CA 服务器，减少了⽹络通信的开销，提⾼了证书验证的效率；

5. 回话重用

常⻅的会话重⽤技术有 Session ID 和 Session Ticket，
⽤了会话重⽤技术，当再次重连 HTTPS 时，只需要 1 RTT
就可以恢复会话。对于 TLS1.3 使⽤ Pre-shared Key 会话重⽤技术，
只需要 0 RTT 就可以恢复会话。


# 5.各种协议的端口号

（1）HTTP：80号端口以提供服务。
（2）DNS：DNS用的是53号端口。　　
（3）SNMP：简单网络管理协议，使用161号端口


# 6.拆包粘包

## 6-1：什么是TCP粘包？

TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，
从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，
例如基于tcp的套接字客户端往服务端上传文件，发送时文件内容是按照一段一段的字节流发送的，
在接收方看了，根本不知道该文件的字节流从何处开始，在何处结束
所谓粘包问题主要还是因为接收方不知道消息之间的界限，不知道一次性提取多少字节的数据所造成的。

## 6-2：粘包的原因

1. 发送方原因
TCP默认使用Nagle算法（主要作用：会将数据量小的，且时间间隔较短的数据一次性发给对方），
而Nagle算法主要做两件事：
只有上一个分组得到确认，才会发送下一个分组
收集多个小分组，在一个确认到来时一起发送
Nagle算法造成了发送方可能会出现粘包问题
2. 接收方原因
TCP接收到数据包时，并不会马上交到应用层进行处理，
或者说应用层并不会立即处理。实际上，TCP将接收到的数据包保存在接收缓存里，
然后应用程序主动从缓存读取收到的分组。
这样一来，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，
多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。

## 6-3：粘包解决方案

（1）发送方
   对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，
   使用TCP_NODELAY选项来关闭算法。
（2）接收方
   接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。
（2）应用层
   解决办法：循环处理，应用程序从接收缓存中读取分组时，
   读完一条数据，就应该循环读取下一条数据，
   直到所有数据都被处理完成，之后开始处理每条数据的长度
   因为每条数据有固定的格式（开始符，结束符），
   但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。
   发送每条数据时，将数据的长度一并发送，
   例如规定数据的前4位是数据的长度，
   应用层在处理时可以根据长度来判断每个分组的开始和结束位置。

## 6-4：UDP会不会产生粘包问题呢？

UDP不会发生粘包拆包现象
UDP则是面向消息传输的，是有保护消息边界的，
接收方一次只接受一条独立的信息，所以不存在粘包问题。
保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，
接收端一次只能接受一条独立的消息
比如说，有三个数据包，大小分别为2k、4k、6k，如果采用UDP发送的话，
不管接受方的接收缓存有多大，我们必须要进行至少三次以上的发送才能把数据包发送完，
但是使用TCP协议发送的话，我们只需要接受方的接收缓存有12k的大小，
就可以一次把这3个数据包全部发送完毕。

## 6-5：HTTP拆包粘包

一个有报文的请求到服务器时，请求头里都会有content_length，这个指定了报文的大小，
报文如果很大的时候，会通过一部分一部分的发送请求，直到结束，
当这个过程中，出现多个请求，第一个请求会带有请求头信息，前面一个请求的如果发送的报文如果没有满时，
会把后面一个请求的内容填上，这个操作就叫粘包。
这样粘包后，它会通过content_length字段的大小，来做拆包。

# 7.计算机网络-HTTP-Cookie与Session

## 7-1：Cookie 和 Session 的区别

1. 安全性： Session 比 Cookie 安全，Session 是存储在服务器端的，Cookie 是存储在客户端的。
2. 存取值的类型不同：Cookie 只支持存字符串数据，想要设置其他类型的数据，
                     需要将其转换成字符串，Session 可以存任意数据类型。
3. 有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，
                Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效。
4. 存储大小不同： 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie，
                 但是当访问量过多，会占用过多的服务器资源。

# 8.计算机网络-HTTP-Cookie

## 8-1：Cookie作用

cookie是服务路发送到用户浏览器并保存在本地的小快数据，
它会在浏览器之后向同一服务器再次发起请求时被携带上，
用于告知服务端两个请求是否来自同一浏览器

## 8-2：HTTP是不保存状态的协议,如何保存⽤户状态

通过Session机制解决，Session的主要作⽤就是通过服务端记录⽤户的状态。

如应用场景购物⻋，当你要添加商品到购物⻋的时候，系统不知道是哪个⽤户操作的，
因为HTTP协议是⽆状态的。服务端给特定的⽤户创建特定Session之后就可以标
识这个⽤户并且跟踪这个⽤户了（⼀般情况下，服务器会在⼀定时间内保存这个Session，
过了时间限制，就会销毁这个Session）

## 8-3：Cookie 被禁⽤怎么办

最常⽤的就是利⽤ URL 重写把 Session ID 直接附加在URL路径的后⾯。

### 8-3-1：cookie被禁用了，session还能用么

能用，但没有使用cookie那么安全。
cookie没有被禁用的时候，
浏览器向服务器发送请求时，
会自动带上cookie。SESSIONID放在cookie里面，
所以服务器可以根据SESSIONID找到相应的session文件。
cookie被禁用时，还可以通过get、post参数来向服务器提供SESSIONID。
php支持通过URL 参数 来向服务器提供SESSIONID。但是要设置php.ini：

## 8-4：cookie如何放置攻击

需要在HTTP头部配上，set-cookie：httponly-。这个属性可以防止XSS,
它会禁止javascript脚本来访问cookie。



# 9.计算机网络-HTTP-Session

## 9-1：Session用户登录状态过程

1. 用户进行登录时，用户提交包含用户名和密码的表单，放入HTTP请求报文中，
2. 服务器验证该用户名和密码，
         如果正确则把用户信息存储到Redis中，
         它在Redis中Key称为Session ID:
3. 服务器返回的响应报文的Se-Coeo首部字段包含了这个Session ID.
         客户端收到响应报文之后将该Cookie值存入浏览器中:
4. 客户编之后对间一 个服务器进行请求时会包含该Cookie值，
       服务器收到之后提取出Session ID.从Redis中取出用户信息，
       维续之前的业务操作。

## 9-2：如何保存session

在服务端保存Session的⽅法很多，
最常⽤的就是内存和数据库(⽐如是使⽤内存数据库redis保存)。

## 9-3：如何实现 Session 跟踪呢？

⼤部分情况下，我们都是通过在Cookie 中附加⼀个 Session ID 来⽅式来跟踪。

## 9-4：Session机制（多个session如何识别）

session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构
（也可能就是使用散列表）来保存信息。
当程序需要为某个客户端的请求创建一个session的时候，
服务器首先检查这个客户端的请求里是否已包含了一个session标识
称为session id，
如果已包含一个session id则说明以前已经为此客户端创建过session，
服务器就按照session id把这个session检索出来使用
（如果检索不到，可能会新建一个），
如果客户端请求不包含session id，则为此客户端创建一个session
并且生成一个与此session相关联的session id，
session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，
这个session id将被在本次响应中返回给客户端保存。

# 10.计算机网络-HTTP-token

## 10-1：token的验证流程

1. 客户端使用用户名跟密码请求登录
2. 服务端收到请求，去验证用户名与密码
3. 验证成功后，服务端会签发一个 token 并把这个 token 发送给客户端
4. 客户端收到 token 以后，会把它存储起来，比如放在 cookie 里或者 localStorage 里
5. 客户端每次向服务端请求资源的时候需要带着服务端签发的 token
6. 服务端收到请求，然后去验证客户端请求里面带着的 token ，如果验证成功，就向客户端返回请求的数据

## 10-2：token和cookie实现的区别

1. Session 是一种记录服务器和客户端会话状态的机制，
   使服务端有状态化，可以记录会话信息。而Token 是令牌
   访问资源接口（API）时所需要的资源凭证。
   Token 使服务端无状态化，不会存储会话信息。
2. Token每一个请求都有签名还能防止监听以及重放攻击，
   而Session就必须依赖链路层来保障通讯安全了。
3. 如果你的用户数据可能需要和第三方共享，
   或者允许第三方调用 API 接口，用Token 。
   如果永远只是自己的网站，自己的 App，用什么就无所谓了。

# 11.计算机网络-HTTP-url/uri

## 11-1：URI和URL的区别是什么?

URI的作⽤像身份证号⼀样， URL的作⽤更像家庭住址⼀样。

# 12.计算机网络-综合应用-输入网址

## 12-1：输入网址过程

1. 输入地址,对URL进⾏解析，从⽽⽣成发送给Web服务器的请求信息。
2. 浏览器查找域名的IP地址,因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。　
   1) 浏览器会首先查看本地硬盘的hosts文件，看看其中有没有和这个域名对应的规则，
      如果有的话就直接使用hosts文件里面的ip地址。
   2) 如果在本地的hosts文件没有能够找到对应的ip地址，
      浏览器会发出一个DNS请求到本地DNS服务器
   3) 查询你输入的网址的DNS请求到达本地DNS服务器之后，
      本地DNS服务器会首先查询它的缓存记录，如果缓存中有此条记录，
      就可以直接返回结果，此过程是递归的方式进行查询。
      如果没有，本地DNS服务器还要向DNS根服务器进行查询。
   4) 根DNS服务器没有记录具体的域名和IP地址的对应关系，
      而是告诉本地DNS服务器，你可以到域服务器上去继续查询，
      并给出域服务器的地址。这种过程是迭代的过程。
   5) 本地DNS服务器继续向域服务器发出请求，比如说请求的对象是.com域服务器。
      .com域服务器收到请求之后，也不会直接返回域名和IP地址的对应关系，
      而是告诉本地DNS服务器，你的域名的解析服务器的地址
   6) 最后，本地DNS服务器向域名的解析服务器发出请求，这时就能收到一个域名和IP地址对应关系，
      本地DNS服务器不仅要把IP地址返回给用户电脑，还要把这个对应关系保存在缓存中，
      以备下次别的用户查询时，可以直接返回结果，加快网络访问。
3. 浏览器向web服务器发送一个HTTP请求
      通过DNS获取到IP后，就可以把HTTP的传输⼯作交给操作系统中的协议栈。
      协议栈的内部分为⼏个部分，分别承担不同的⼯作。
      上下关系是有⼀定的规则的，上⾯的部分会向下⾯的部分委托⼯作，
      下⾯的部分收到委托的⼯作并执⾏。
      应⽤程序也就是浏览器通过调⽤ Socket 库，来委托协议栈⼯作。
      协议栈的上半部分有两块，分别是负责收发数据的 TCP 和 UDP 协议，
      它们两会接受应⽤层的委托执⾏收发数据的操作。
      协议栈的下⼀半是⽤IP协议控制⽹络包收发操作，
      在互联⽹上传数据时，数据会被切分成⼀块块的⽹络包，
      ⽽将⽹络包发送给对⽅的操作就是由 IP 负责的。
      IP 下⾯的⽹卡驱动程序负责控制⽹卡硬件，
      ⽽最下⾯的⽹卡则负责完成实际的收发操作，
      也就是对⽹线中的信号执⾏发送和接收操作。
      拿到域名对应的IP地址之后，
      浏览器会以一个随机端口向服务器的WEB程序80端口发起TCP的连接请求。
      这个连接请求到达服务器端后，进入到网卡，然后是进入到内核的TCP/IP协议栈，
      还有可能要经过防火墙的过滤，最终到达WEB程序，最终建立了TCP/IP的连接。
4. 服务器的永久重定向响应
      服务器给浏览器响应一个301永久重定向响应，这样浏览器就会访问3w了。
5. 浏览器跟踪重定向地址，
      因为现在浏览器知道了 "http://www.google.com/"才是要访问的正确地址，
      所以它会发送另一个http请求
6. 服务器处理请求
      http请求发送到了服务器，后端从在固定的端口接收到TCP报文开始，
      它会对TCP连接进行处理，对HTTP协议进行解析，
      并按照报文格式进一步封装成HTTP Request对象，供上层使用。
7. 服务器返回一个HTTP响应　
      服务器收到了我们的请求，也处理我们的请求，
      到这一步，它会把它的处理结果返回，也就是返回一个HTPP响应。
8. 浏览器显示 HTML,并请求获取嵌入在HTML的资源

## 12-2：为什么域名要分级设计

DNS 中的域名都是⽤句点来分隔的，代表了不同层次之间的界限。

域名的层级关系类似⼀个树状结构：
根 DNS 服务器
顶级域 DNS 服务器（com）
权威 DNS 服务器（server.com）

因此，客户端只要能够找到任意⼀台 DNS 服务器，
就可以通过它找到根域 DNS 服务器，然后再⼀路顺
藤摸⽠找到位于下层的某台⽬标 DNS 服务器。

## 12-3：重定向原因

1. 网站调整（如改变网页目录结构）；
2. 网页被移到一个新地址；
3. 网页扩展名改变(如应用需要把.php改成.Html或.shtml)。

这种情况下，如果不做重定向，则用户收藏夹或搜索引擎数据库中
旧地址只能让访问客户得到一个404页面错误信息，访问流量白白丧失；
还有就是某些注册了多个域名的网站，
也需要通过重定向让访问这些域名的用户自动跳转到主站点等。

# 15.TCP的三次握手

## 15-1：TCP三次握手流程

客户端–发送带有SYN标志的数据包–一次握手–服务端
服务端–发送带有SYN/ACK标志的数据包–二次握手–客户端
客户端–发送带有带有ACK标志的数据包–三次握手–服务端

## 15-2：TCP为什么要三次握⼿

1. 三次握⼿才可以 阻⽌ 重复 历史连接的初始化（主要原因）
   
  * 比如说，客户端连续发送多次SYN建⽴连接的报⽂，在⽹络拥堵情况下：

    * ⼀个「旧 SYN 报⽂」⽐「最新的 SYN报⽂」早到达了服务端；
    * 那么此时服务端就会回⼀个SYN+ACK报⽂给客户端；
    * 客户端收到后可以根据⾃身的上下⽂，判断这是⼀个历史连接，
        比如说序列号过期或超时，那么客户端就会发送 RST 报
           ⽂给服务端，表示中⽌这⼀次连接。

  * 如果是两次握⼿连接，就不能判断当前连接是否是历史连接

2. 三次握⼿才可以 同步 双⽅的 初始 序列号
   
   序列号是作为TCP可靠传输的⼀个关键因素，它的作⽤其实是：

   * 接收⽅可以去除重复的数据；
   * 接收⽅可以根据数据包的序列号按序接收；
   * 可以标识发送出去的数据包中，哪些是已经被对⽅收到的；

   - 四次握⼿其实也能够同步双⽅的初始化序号，
         但由于客户端传输ACK和SYN可以优化成⼀步，所以就成了「三次握⼿」
   - 两次握⼿只保证了⼀⽅的初始序列号能被对⽅成功接收，没办法保证双⽅的初始序列号都能被确认接收。

3. 三次握⼿才可以避免资源浪费
  
  如果只有「两次握⼿」，当客户端的 SYN 请求连接在⽹络中阻塞，
  客户端没有接收到 ACK 报⽂，重复发送多次 SYN 报⽂，
  那么服务器在收到请求后就会建⽴多个冗余的⽆效链接，造成不必要的资源浪费。
  
## 15-3：TCP为什么SYN

接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。

## 15-4：TCP除了SYN，为什么还要 ACK

双⽅通信⽆误必须是两者互相发送信息都⽆误。传了 SYN，证明发送⽅到接收⽅的通道没有问题，但是
接收⽅到发送⽅的通道还需要 ACK 信号来进⾏验证。

## 15-5：什么是 SYN 攻击？如何避免 SYN 攻击？

TCP 连接建立是需要三次握手，假设攻击者短时间伪造不同 IP 地址的 SYN 报文，
服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，
但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，
久而久之就会占满服务端的 SYN 接收队列（未连接队列），使得服务器不能为正常用户服务。

1. 避免 SYN 攻击方式一

通过修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理。

当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。控制该队列的最大值

2. 避免 SYN 攻击方式二

正常流程下,

当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「 SYN 队列」；

接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文；

服务端接收到 ACK 报文后，从「 SYN 队列」移除放入到「 Accept 队列」；

应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。

但是

如果应用程序过慢时，就会导致「 Accept 队列」被占满。

如果不断受到 SYN 攻击，就会导致「 SYN 队列」被占满。

当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」；

计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端，

服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」。

最后应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。

## 15-5：如何对三次握手进行性能优化

1. 三次握⼿建⽴连接的⾸要⽬的是「同步序列号」。只有同步了序列号才有可靠传输，所以当客户端发起 SYN 包时，可以通
   过tcp_syn_retries 控制其重传的次数，⽐如内⽹通讯不畅时，就可以适当调低重试次数，尽快把错误暴露给应⽤程序

2. 当服务端SYN半连接队列溢出后，会导致后续连接被丢弃，可以通过backlog等参数来调整 SYN 半连接队列的⼤⼩。

3. TCP Fast Open 功能可以绕过三次握⼿，使得 HTTP 请求减少了 1 个 RTT 的时间，所以也是一种性能优化方案

## 15-6：如何绕过三次握手发送数据

TCP Fast Open 功能可以绕过三次握⼿，使得 HTTP 请求减少了1个RTT的时间， 

第⼀次发起 HTTP GET请求的时候，还是需要正常的三次握⼿流程。

之后发起 HTTP GET请求的时候，可以绕过三次握⼿，这就减少了握⼿带来的 1 个 RTT 的时间消耗。

## 15-7：TCP Fast Open的过程

I、客户端⾸次建⽴连接时的过程：
1. 客户端发送SYN报⽂，该报⽂包含Fast Open选项，且该选项的Cookie为空；

2. ⽀持 TCP Fast Open 的服务器⽣成 Cookie，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；

3. 客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。


II、如果客户端再次向服务器建⽴连接时的过程：

1. 客户端发送 SYN 报⽂，该报⽂包含「数据」以及此前记录的 Cookie；
2. ⽀持 TCP Fast Open 的服务器会对收到 Cookie 进⾏校验：如果 Cookie 有效，服务器将在 SYNACK 报⽂中对 SYN 和
   「数据」进⾏确认，服务器随后将「数据」递送⾄相应的应⽤程序；如果Cookie ⽆效，服务器将丢弃 SYN 报⽂中包含的
   「数据」，且其随后发出的 SYN-ACK 报⽂将只确认 SYN 的对应序列号；
3. 如果服务器接受了 SYN 报⽂中的「数据」，服务器可在握⼿完成之前发送「数据」， 这就减少了握⼿带来的 1 个 RTT 的
   时间消耗；
4. 客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报⽂中发送的「数据」没有被确
   认，则客户端将重新发送「数据」；

## 15-8：为什么需要 TCP 协议？

IP 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。

如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。

因为 TCP 是一个工作在传输层的可靠数据传输的服务，它能确保接收端接收的网络包是无损坏、无间隔、非冗余和按序的。

## 15-9：什么是 TCP 连接？

这个我还真不太清楚，但是我看到过关于传输控制协议的定义

简单来说就是，用于保证可靠性和流量控制维护的某些状态信息，

## 15-10：如何唯一确定一个 TCP 连接呢？

TCP 四元组可以唯一的确定一个连接，四元组包括：

源地址

源端口

目的地址

目的端口

源地址和目的地址的字段是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。

源端口和目的端口的字段（16位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。

## 15-11：有一个 IP 的服务器监听了一个端口，它的 TCP 的最大连接数是多少？

服务器通常固定在某个本地端口上监听，等待客户端的连接请求。

因此，客户端 IP 和 端口是可变的，其理论值计算公式应该是:

客户端的IP数X客户端的端口数

## 15-12：服务端最大并发 TCP 连接数远不能达到理论上限

首先主要是文件描述符限制，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目；

另一个是内存限制，每个 TCP 连接都要占用一定内存，操作系统是有限的。

## 15-15：为什么客户端和服务端的初始序列号 ISN 是不相同的？

因为网络中的报文会延迟、会复制重发、也有可能丢失，

这样会造成的不同连接之间产生互相影响，所以为了避免互相影响，客户端和服务端的初始序列号是随机且不同的。

## 15-16：什么是Mss

除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度；

MTU：一个网络包的最大长度，以太网中一般为 1500 字节；

## 15-17：既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？

当 IP 层有一个超过 MTU 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，

把数据分片成若干片，保证每一个分片都小于 MTU。

把一份 IP 数据报进行分片以后，由目标主机的 IP 层来进行重新组装后，在交给上一层 TCP 传输层。

但这存在隐患的，那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传。

因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。

当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，则不会响应 ACK 给对方，那么发送方的 TCP 在超时后，就会重发「整个 TCP 报文（头部 + 数据）」。

所以 IP 层进行分片传输，是非常没有效率的。

所以，为了达到最佳的传输效率， TCP 协议在建立连接的时候通常要协商双方的 MSS 值，

当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。

# 16.TCP的四次挥手

## 16-1：TCP四次挥手流程

客户端-发送一个FIN，用来关闭客户端到服务器的数据传送
服务器-收到这个FIN，它发回一个ACK，确认序号为收到的序号加1 。和SYN一样，一个FIN将占用一个序号
服务器-关闭与客户端的连接，发送一个FIN给客户端
客户端-发回ACK报文确认，并将确认序号设置为收到序号加1

## 16-2：TCP为什么要四次挥手

四次挥手主要是从FIN过程进行分析：

1. 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。

2. 服务器收到客户端的 FIN 报⽂时，先回⼀个 ACK 应答报⽂，⽽服务端可能还有数据需要处理和发送，等服务端不再发送数
   据时，才发送 FIN 报⽂给客户端来表示同意现在关闭连接。

服务端通常需要等待完成数据的发送和处理，所以服务端的ACK和FIN⼀般都会分开发送，从⽽⽐三次握⼿导致多了⼀次

## 16-3：TCP第四次挥手后会理解断开呢？

要等待2MSL后才断开链接

假如第四次挥手失败了，因为丢失而未到达服务器，这样，服务器会一直收不到客户端的回应，
也就无法得知客户端是否收到了即将要断开连接的请求。
客户端此刻还蒙在鼓里，还在等待服务器继续发送消息。服务器不能判断客户端是否收到，
本身就是一个BUG，于是才有的等待2MSL的情况。为了保证客户端最后一次挥手的报文能够到达服务器，
若第4次挥手的报文段丢失了，服务器就会超时重传第3次挥手的报文段，
所以客户端此时不是直接进入CLOSED，而是保持TIME_WAIT（等待2MSL就是TIME_WAIT）。
当客户端再次受到服务器因为超时重传而发送的第3次挥手的请求时，
客户端就会重新给服务器发送第4次挥手的报文（保证服务器能够受到客户端的回应报文）。
最后，客户端、服务器才真正断开连接。等待2MSL就是为了确保服务器能够受到客户端最后的回应。
2.如果客户端直接CLOSED，然后又再次向服务器发起一个新连接，
谁也不能保证新发起的连接和刚关闭的连接的端口号是不同的，
有可能新、老连接的端口号就是一样的。
假设新、老连接端口号一致，若老连接的一些数据仍滞留在网络中，
这些滞留数据在新连接建立后才到达服务器，鉴于前后端口号一致，
TCP协议就默认这些数据属于新连接，于是数据就这样乱成一锅粥了。
所以TCP连接还要在TIME_WAIT状态下等待2MSL，确保所有老连接的数据都在网络中消失！

主要就是
1.为了确保第四次挥手的ACK能被服务器接收到
2.确保所有的老链接都在网络中消失。

## 16-3：如何对四次挥手进行优化

1. 主动发起FIN报⽂断开连接的⼀⽅，如果迟迟没收到对⽅的 ACK 回复，则会重传 FIN 报⽂，重传的次数由 
   tcp_orphan_retries 参数决定。

当主动⽅收到 ACK 报⽂后，连接就进⼊ FIN_WAIT2 状态，根据关闭的⽅式不同，优化的⽅式也不同：

* 如果是 close 函数关闭的连接，那么它就是孤⼉连接。如果在系统设置的时间内 （tcp_fin_timeout） 没有收到对⽅的 
  FIN 报⽂，连接就直接关闭。同时，为了应对孤⼉连接占⽤太多的资源， tcp_max_orphans定义了最⼤孤⼉连接的数量，超
  过时连接就会直接释放。

* 反之是 shutdown 函数关闭的连接，则不受此参数限制；
  
2. 当主动⽅接收到 FIN 报⽂，并返回 ACK 后，主动⽅的连接进⼊ TIME_WAIT 状态。为了防⽌ TIME_WAIT 状态占⽤太多的
   资源， tcp_max_tw_buckets 定义了最⼤数量，超过时连接也会直接释放。

3. 被动方关闭的连接，它在回复 ACK 后就进⼊了 CLOSE_WAIT 状态，等待进程调⽤ close函数关闭连接。因此，出现⼤量 
   CLOSE_WAIT 状态的连接时，应当从应⽤程序中找问题。
   
4. 当被动⽅发送 FIN 报⽂后，连接就进⼊ LAST_ACK 状态，在未等到 ACK 时，会在tcp_orphan_retries 参数的控制下重
   发 FIN 报⽂。

## 16-4：为什么TIME_WAIT 等待的时间是 2MSL？

⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包被接收⽅处理后⼜会向对⽅发送响应，所以⼀来⼀回需要等待2 
倍的时间。

## 16-5：为什么需要TIME_WAIT状态？（已经主动关闭连接了为啥还要保持资源一段时间呢？）

1. 防⽌具有相同端口的的「旧」数据包被收到；
   
   比如说服务端在关闭连接之前发送一个报⽂，但是被⽹络延迟了。这时有相同端⼝的TCP连接被复⽤后，被延迟的报文抵达
   了客户端，那么客户端是有可能正常接收这个过期的报⽂，这就会产⽣数据错乱等严重的问题。而使用TIME_WAIT这个时
   间，⾜以让两个⽅向上的数据包都被丢弃

2. 保证连接正确关闭

  比如客户端四次挥⼿的最后⼀个ACK报⽂如果在⽹络中被丢失了，此时如果客户端TIME-WAIT 过短或没有，则就直接进⼊
  了 CLOSED 状态了，那么服务端则会⼀直处在 LASE_ACK状态。当客户端发起建⽴连接的 SYN 请求报⽂后，服务端会发送 
  RST 报⽂给客户端，连接建⽴的过程就会被终⽌。如果有这个时间，就会正常关闭，即使没有收到ACK报文，我也有时间重发
  并关闭

## 16-6：TIME_WAIT 过多有什么危害？

1. 内存资源占⽤；

2. 对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝；被占满就会导致⽆法创建新的连接。

## 16-7：如何优化 TIME_WAIT？

p146页

## 16-8：如果已经建⽴了连接，但是客户端突然出现故障了怎么办？

TCP 有⼀个机制是保活机制。

定义在一个时间段内，如果没有任何连接相关的活动， TCP 保活机制会开始作⽤，每隔⼀个时间间隔，发送⼀个探测报⽂，该

探测报⽂包含的数据⾮常少，如果连续⼏个探测报⽂都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息

通知给上层应⽤程序。

# 17.TCP传输数据优化方案

## 17-1：TCP传输数据优化

TCP 会保证每⼀个报⽂都能够抵达对⽅，报⽂发出去后，必须接收到对⽅返回的确认报⽂ ACK，
如果迟迟未收到，就会超时重发该报⽂，直到收到对⽅的 ACK 为⽌。
所以， TCP 报⽂发出去后，并不会⽴⻢从内存中删除，
因为重传时还需要⽤到它。这种⽅式的缺点是效率⽐较低的
可以采用并⾏批量发送报⽂，再批量确认报⽂即可，
但是当接收⽅硬件不如发送⽅，或者系统繁忙、资源紧张时，
是⽆法瞬间处理这么多报⽂的。于是，这些报⽂只能被丢掉，
使得⽹络效率⾮常低。
为了解决这种现象发⽣， 
TCP 提供⼀种机制可以让「发送⽅」根据「接收⽅」的实际接收能⼒控制发送的数据量
因为⽹络的传输能⼒是有限的，当发送⽅依据发送窗⼝，
发送超过⽹络处理能⼒的报⽂时，路由器会直接丢弃这些报⽂。
影响了传输速度，发送缓冲区的⼤⼩最好是往带宽时延积靠近

# 18.TCP与UDP/IP比较

## 18-1：TCP与UDP区别

![avatar](http://qd6kny79g.bkt.clouddn.com/03-TCP.jpg)

1. `连接方面来看`
   TCP 是面向连接的传输层协议，传输数据前先要建立连接。
   UDP 是不需要连接，即刻传输数据。
2. `服务对象`
   TCP 是一对一的两点服务，即一条连接只有两个端点。
   UDP 支持一对一、一对多、多对多的交互通信
3. `可靠性`
   TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。
   UDP 是尽最大努力交付，不保证可靠交付数据。
4. `拥塞控制、流量控制`
   TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。
   UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。
5. `首部开销`
   TCP 首部长度较长，会有一定的开销，
      首部在没有使用「选项」字段时是 20 个字节，
      如果使用了「选项」字段则会变长的。
   UDP 首部只有 8 个字节，并且是固定不变的，开销较小。

## 18-2：TCP 和 UDP 应用场景

由于TCP是面向连接，能保证数据的可靠性交付，因此经常用于：
  * FTP 文件传输
  * HTTP / HTTPS
接受邮件、远程登录
由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：
  * 包总量较少的通信，如 DNS 、SNMP 等
  * 视频、音频等多媒体通信
  * 广播通信
QQ聊天、在线视频、网络语音电话

## 18-3：TCP与IP的区别

IP层接收由更低层（网络接口层例如以太网设备驱动程序）发来的数据包，并把该数据包发送到更高层—TCP层；
IP层也把从TCP接收来的数据包传送到更低层。
我认为TCP和IP的关系是：IP提供基本的数据传送，而高层的TCP对这些数据包做进一步加工，如提供端口号等等。

# 19.TCP

## 19-1：TCP

一种面向连接(连接导向)的、可靠的、 基于IP的传输层协议

### 19-1-1：为什么TCP面向流

TCP是基于字节流的，虽然应用层和TCP传输层之间的数据交互是大小不等的数据块，
但是TCP把这些数据块仅仅看成一连串无结构的字节流，没有边界；
另外从TCP的帧结构也可以看出，在TCP的首部没有表示数据长度的字段

## 19-2：TCP首部

1） 源端口和目的端口，各占2个字节，分别写入源端口和目的端口。

2） 序号，占4字节。序号范围是【0，2^32 - 1】，共2^32个序号。
    序号增加到2^32-1后，下一个序号就又回到0。
    TCP是面向字节流的。在一个TCP连接中传送的字节流中的每一个字节都按顺序编号。
    整个要传送的字节流的起始序号必须在连接建立时设置。
    首部中的序号字段值则是指的是本报文段所发送的数据的第一个字节的序号。
    例如，一报文段的序号是301，而接待的数据共有100字节。
    这就表明：本报文段的数据的第一个字节的序号是301，
    最后一个字节的序号是400。显然，下一个报文段（如果还有的话）的数据序号应当从401开始，
    即下一个报文段的序号字段值应为401。这个字段的序号也叫“报文段序号”。

3） 确认号，占4字节，是期望收到对方下一个报文段的第一个数据字节的序号。
    例如，B正确收到了A发送过来的一个报文段，其序号字段值是501，
    而数据长度是200字节（序号501~700），这表明B正确收到了A发送的到序号700为止的数据。
    因此，B期望收到A的下一个数据序号是701，
    于是B在发送给A的确认报文段中把确认号置为701。

4） 数据偏移，占4位，它指出TCP报文段的数据起始处距离TCP报文段的起始处有多远。
   这个字段实际上是指出TCP报文段的首部长度。由于首部中还有长度不确定的选项字段，
   因此数据偏移字段是必要的，

5） 保留，占6位，保留为今后使用，但目前应置为0 。

还有几个控制位，用来说明报文段的性质。
6） 紧急URG，当URG=1时，表明紧急指针字段有效。
    它告诉系统此报文段中有紧急数据，应尽快发送（相当于高优先级的数据），
    而不要按原来的排队顺序来传送。
    例如，已经发送了很长的一个程序要在远地的主机上运行。
    但后来发现了一些问题，需要取消该程序的运行，因此用户从键盘发出中断命令。
    如果不使用紧急数据，那么这两个字符将存储在接收TCP的缓存末尾。
    只有在所有的数据被处理完毕后这两个字符才被交付接收方的应用进程。
    这样做就浪费了很多时间。

7） 确认ACK，仅当ACK = 1时确认号字段才有效，
    当ACK = 0时确认号无效。TCP规定，在连接建立后所有的传送的报文段都必须把ACK置为1。

8） 推送 PSH，当两个应用进程进行交互式的通信时，
    有时在一端的应用进程希望在键入一个命令后立即就能收到对方的响应。
    在这种情况下，TCP就可以使用推送（push）操作。
    这时，发送方TCP把PSH置为1，并立即创建一个报文段发送出去。
    接收方TCP收到PSH=1的报文段，就尽快地（即“推送”向前）交付接收应用进程。
    而不用再等到整个缓存都填满了后再向上交付。

9） 复位RST，当RST=1时，表名TCP连接中出现了严重错误（如由于主机崩溃或其他原因），
    必须释放连接，然后再重新建立传输连接。
    RST置为1还用来拒绝一个非法的报文段或拒绝打开一个连接。

10） 同步SYN，在连接建立时用来同步序号。当SYN=1而ACK=0时，
     表明这是一个连接请求报文段。对方若同意建立连接，
     则应在响应的报文段中使SYN=1和ACK=1，
     因此SYN置为1就表示这是一个连接请求或连接接受报文。

11） 终止FIN，用来释放一个连接。当FIN=1时，
     表明此报文段的发送发的数据已发送完毕，并要求释放运输连接。

12） 窗口，占2字节。窗口值是【0，2^16-1】之间的整数。
     窗口指的是发送本报文段的一方的接受窗口（而不是自己的发送窗口）。
     窗口值告诉对方：从本报文段首部中的确认号算起，
     接收方目前允许对方发送的数据量（以字节为单位）。
     之所以要有这个限制，是因为接收方的数据缓存空间是有限的。
     总之，窗口值作为接收方让发送方设置其发送窗口的依据。
     例如，发送了一个报文段，其确认号是701，窗口字段是1000.
     这就是告诉对方：“从701算起，我（即发送方报文段的一方）
     的接收缓存空间还可接受1000个字节数据（字节序号是701~1700），

13） 检验和，占2字节。检验和字段检验的范围包括首部和数据这两部分。
     和UDP用户数据报一样，在计算检验和时，
     要在TCP报文段的前面加上12字节的伪首部。
     伪首部的格式和UDP用户数据报的伪首部一样。
     但应把伪首部第4个字段中的17改为6（TCP的协议号是6）；
     把第5字段中的UDP中的长度改为TCP长度。接收方收到此报文段后，
     仍要加上这个伪首部来计算检验和。若使用TPv6,则相应的伪首部也要改变。

14） 紧急指针，占2字节。紧急指针仅在URG=1时才有意义，
     它指出本报文段中的紧急数据的字节数（紧急数据结束后就是普通数据）。
     因此，在紧急指针指出了紧急数据的末尾在报文段中的位置。
     当所有紧急数据都处理完时，TCP就告诉应用程序恢复到正常操作。
     值得注意的是，即使窗口为0时也可以发送紧急数据。

15） 选项，长度可变，最长可达4字节。当没有使用“选项”时，TCP的首部长度是20字节。

## 19-2：TCP作用

保证数据通信的完整性和可靠性，防止丢包
因为IP 协议只是一个地址协议，并不保证数据包的完整。
如果路由器丢包（比如缓存满了，新进来的数据包就会丢失），
就需要发现丢了哪一个包，以及如何重新发送这个包。
这就要依靠 TCP 协议。

## 19-3：TCP 数据包的大小

TCP 负载实际为1400字节左右。
以太网数据包（packet）的大小是固定的，1522字节。其中， 
1500 字节是负载（payload），22字节是头信息（head）。
IP 数据包在以太网数据包的负载里面，它也有自己的头信息，
最少需要20字节，所以 IP 数据包的负载最多为1480字节。
TCP 数据包在 IP 数据包的负载里面。
它的头信息最少也需要20字节，
因此 TCP 数据包的最大负载是 1480 - 20 = 1460 字节。
由于 IP 和 TCP 协议往往有额外的头信息，
所以 TCP 负载实际为1400字节左右。
   * 注：可不可以压缩加快速度---》HTTP/2进行了优化

## 19-4：TCP 数据包的编号（SEQ）

一个包1400字节，那么一次性发送大量数据，
就必须分成多个包
发送的时候，TCP 协议为每个包编号，
以便接收的一方按照顺序还原。万一发生丢包，
也可以知道丢失的是哪一个包。
第一个包的编号是一个随机数。
假设为1号包。假定这个包的负载长度是100字节，
那么可以推算出下一个包的编号应该是101。
每个数据包都可以得到两个编号：
自身的编号，以及下一个包的编号。
接收方由此知道，应该按照什么顺序将它们还原成原始文件。

## 19-5：TCP 数据包的组装

收到TCP数据包以后，组装还原是操作系统完成的。
应用程序不会直接处理 TCP 数据包。
对于应用程序来说，不用关心数据通信的细节。
除非线路异常，收到的总是完整的数据。
应用程序需要的数据放在TCP数据包里面，
有自己的格式（比如HTTP协议）。
TCP 并没有提供任何机制，表示原始文件的大小，
这由应用层的协议来规定。
比如，HTTP 协议就有一个头信息Content-Length，
表示信息体的大小。对于操作系统来说，
就是持续地接收 TCP 数据包，
将它们按照顺序组装好，一个包都不少。
操作系统不会去处理 TCP 数据包里面的数据。
一旦组装好 TCP 数据包，就把它们转交给应用程序。
TCP 数据包里面有一个端口（port）参数，
就是用来指定转交给监听该端口的应用程序。

## 19-6：TCP首部组成




# 20.TCP/UDP如何保证可靠传输方式

## 20-1：TCP如何保证稳定传输

1. 确认应答+序列号：TCP给发送的每⼀个包进⾏编号，
                  接收⽅对数据包进⾏排序，把有序数据传送给应⽤层。
2. 校验和：TCP 将保持它⾸部和数据的检验和。
          ⽬的是检测数据在传输过程中的任何变化。
          如果收到段的检验和有差错， 
          TCP将丢弃这个报⽂段和不确认收到此报⽂段。
3. 流量控制：TCP 连接的每⼀⽅都有固定⼤⼩的缓冲空间， 
             TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。
             当接收⽅来不及处理发送⽅的数据，能提示发送⽅降低发送的速率，
             防⽌包丢失。 
             TCP 使⽤的流量控制协议是可变⼤⼩的滑动窗⼝协议。 
             （TCP 利⽤滑动窗⼝实现流量控制）

4. 拥塞控制：当⽹络拥塞时，减少数据的发送。
5. ARQ协议： 也是为了实现可靠传输的，
             它的基本原理就是每发完⼀个分组就停⽌发送，
             等待对⽅确认。在收到确认后再发下⼀个分组。
6. 超时重传： 当 TCP 发出⼀个段后，它启动⼀个定时器，
              等待⽬的端确认收到这个报⽂段。
              如果不能及时收到⼀个确认，将重发这个报⽂段。

## 20-2：UDP如何做可靠传输

由于UDP在传输层无法保证数据的可靠传输，只能通过应用层来实现了。
实现的方式可以参照tcp可靠性传输的方式，只是实现不在传输层，实现转移到了应用层。
实现确认机制、重传机制、窗口确认机制。


# 21.重传机制

## 21-1：常见的重传机制

      1. 超时重传
      2. 快速重传
      3. SACK
      4. D-SACK

## 21-2：超时重传

在发送数据时，设定一个定时器，当超过指定的时间后，
没有收到对方的 ACK 确认应答报文，就会重发该数据。

## 21-3：什么时候会发生超时重传

- 数据包丢失
- 确认应答丢失

## 21-4：超时重传存在的问题

如果超时重发的数据，再次超时的时候，又需要重传的时候，
TCP 的策略是超时间隔加倍。这样超时周期可能相对较长

## 21-5：快速重传

它不以时间为驱动，而是以数据驱动重传

比如说，发送方发出了 1，2，3，4，5 份数据：

第一份 Seq1 先送到了，于是就 Ack 回 2；
结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；
后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；
发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。
最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。

所以说，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段

## 21-6：快速重传的问题

快速重传机制只解决了一个问题，就是超时时间的问题，
但是它依然面临着另外一个问题。就是重传的时候，
是重传之前的一个，还是重传所有的问题。

## 21-7：SACK方法

这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，
它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，
哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。
比如说发送方收到了三次同样的 ACK 确认报文，
于是就会触发快速重发机制，
通过 SACK 信息发现只有某段数据丢失，
则重发时，就只选择了这个 TCP 段进行重复。

## 21-8：D-SACK

主要是使用了 SACK 来告诉「发送方」有哪些数据被重复接收了

例子看p157

## 21-9：D-SACK好处

1. 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了;
2. 可以知道是不是「发送方」的数据包被网络延迟了;
3. 可以知道网络中是不是把「发送方」的数据包给复制了;

# 22.滑动窗口与流量控制

## 22-1：引入窗口概念的原因

TCP 是每发送一个数据，都要进行一次确认应答。
当上一个数据包收到了应答了，
再发送下一个，效率比较低，如果数据包的往返时间越长，通信的效率就越低

## 22-2：什么是窗口

在往返时间较长的情况下，它也不会降低网络通信的效率，这个就是窗口
窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值
窗口的实现实际上是操作系统开辟的一个缓存空间，
发送方主机在等到确认应答返回之前，
必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据
就可以从缓存区清除。
比如说ACK 100 确认应答报文丢失，也没关系，
因为可以通过下一个确认应答进行确认，
只要发送方收到了 ACK 200 确认应答，
就意味着 200 之前的所有数据「接收方」都收到了。

## 22-3：窗口大小由哪一方决定？

通常窗口的大小是由接收方的窗口大小Window来决定的

这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。
于是发送端就可以根据这个接收端的处理能力来发送数据，
而不会导致接收端处理不过来。
发送方发送的数据大小不能超过接收方的窗口接收数据大小，
否则接收方就无法正常接收到数据。

## 22-4：发送方的窗口

一共分为了四部分

1. 是已发送并收到 ACK确认的数据
2. 是已发送但未收到 ACK确认的数据
3. 是未发送但总大小在接收方处理范围内（接收方还有空间）
4. 是未发送但总大小超过接收方处理范围（接收方没有空间）

p160-163未整理

## 22-5：流量控制

如果一直发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。为了解决这种现象发生，TCP 提供一种机制可以「发
送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。

## 22-6：流量控制的过程

p165

## 22-7：

## 22-8：


## 22-9：TCP 是如何解决窗口关闭时，潜在的死锁现象呢？

为了解决这个问题，TCP 为每个连接设有一个持续定时器，
只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。
如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，
而对方在确认这个探测报文时，给出自己现在的接收窗口大小。
如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；
如果接收窗口不是 0，那么死锁的局面就可以被打破了。

# 23.拥塞控制

## 23-1：为什么要有拥塞控制呀，不是有流量控制了吗？

流量控制是避免「发送方」的数据填满「接收方」的缓存，
但是并不知道网络的中发生了什么
在网络出现拥堵时，如果继续发送大量数据包，
可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，
但是一重传就会导致网络的负担更重，
于是会导致更大的延迟以及更多的丢包，
这个情况就会进入恶性循环被不断地放大
为了避免「发送方」的数据填满整个网络，有了拥塞控制


## 23-2：什么是拥塞控制

在某段时间，若对⽹络中某⼀资源的需求超过了该资源所能提供的可⽤部分，
⽹络的性能就要变坏。这种情况就叫拥塞。
拥塞控制就是为了防⽌过多的数据注⼊到⽹络中，
这样就可以使⽹络中的路由器或链路不致过载。
拥塞控制所要做的都有⼀个前提，就是⽹络能够承受现有的⽹络负荷。

## 23-3：什么是拥塞窗口？和发送窗口有什么关系呢？

拥塞窗口是发送方维护的一个的状态变量，
它会根据网络的拥塞程度动态变化的。
发送窗口和接收窗口是约等于的关系，
那么由于加入了拥塞窗口的概念后，
此时发送窗口的值是是拥塞窗口和接收窗口中的最小值。
拥塞窗口 cwnd 变化的规则：
只要网络中没有出现拥塞， cwnd 就会增大；
但网络中出现了拥塞， cwnd 就减少；

## 23-4：那么怎么知道当前网络是否出现了拥塞呢？

只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了用拥塞。

## 23-5：拥塞控制算法

TCP的拥塞控制采⽤了四种算法：

- 慢启动
- 拥塞避免
- 拥塞发生
- 快速恢复

1. 慢开始： 当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1
2. 拥塞避免： 每当收到一个 ACK 时，cwnd 增加 1/cwnd.
3. 拥塞发生算法：
4. 快重传与快恢复： 它能快速恢复丢失的数据包。
                    没有 FRR，如果数据包丢失了， 
                    TCP 将会使⽤定时器来要求传输暂停。
                    在暂停的这段时间内，没有新的或复制的数据包被发送。
                    有了 FRR，如果接收机接收到⼀个不按顺序的数据段，
                    它会⽴即给发送机发送⼀个重复确认。
                    如果发送机接收到三个重复确认，
                    它会假定确认件指出的数据段丢失了，
                    并⽴即重传这些丢失的数据段。
                    有了FRR，就不会因为重传时要求的暂停被耽误。
                    当有单独的数据包丢失时，
                    快速重传和恢复（FRR）能最有效地⼯作。
                    当有多个数据信息包在某⼀段很短的时间内丢失时，
                    它则不能很有效地⼯作。

## 23-6：那慢启动涨到什么时候是个头呢？

有一个叫慢启动门限 ssthresh （slow start threshold）状态变量。

- 当 cwnd < ssthresh 时，使用慢启动算法。
- 当 cwnd >= ssthresh 时，就会使用「拥塞避免算法」。

## 23-7：重传机制何时结束

当触发了重传机制，也就进入了「拥塞发生算法」。

# 24.ARQ协议

## 24-1：什么是ARQ协议

ARQ协议是⾃动重传请求，
他是OSI模型中数据链路层和传输层的错误纠正协议之⼀。
它通过使⽤确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。
如果发送⽅在发送后⼀段时间之内没有收到确认帧，它通常会重新发送。 
ARQ包括停⽌等待ARQ协议和连续ARQ协议。

## 24-2：什么是停⽌等待ARQ协议

停⽌等待协议是为了实现可靠传输的，它的基本原理就是每发完⼀个分组就停⽌发送，
等待对⽅确认（回复ACK）。如果过了⼀段时间（超时时间后），
还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，
直到收到确认后再发下⼀个分组；在停⽌等待协议中，
若接收⽅收到重复分组，就丢弃该分组，但同时还要发送确认；

1. 优点： 简单
2. 缺点： 信道利⽤率低，等待时间⻓

## 24-3: 什么是连续ARQ协议

连续 ARQ 协议可提⾼信道利⽤率。发送⽅维持⼀个发送窗⼝，
凡位于发送窗⼝内的分组可以连续发送出去，
⽽不需要等待对⽅确认。
接收⽅⼀般采⽤累计确认，对按序到达的最后⼀个分组发送确认，
表明到这个分组为⽌的所有分组都已经正确收到了。

1. 优点： 信道利⽤率⾼，容易实现，即使确认丢失，也不必重传。
2. 缺点： 不能向发送⽅反映出接收⽅已经正确收到的所有分组的信息。 
   * ⽐如：发送⽅发送了5条消息，中间第三条丢失（3号），这时接收⽅只能对前两个发送确认。
           发送⽅⽆法知道后三个分组的下落，⽽只好把后三个全部重传⼀次。
           这也叫 Go-Back-N（回退 N），
           表示需要退回来重传已经发送过的N 个消息。

# 25.Socket

## 25-1： TCP 应该如何 Socket 编程？

服务端和客户端初始化 socket，得到文件描述符；
服务端调用 bind，将绑定在 IP 地址和端口;
服务端调用 listen，进行监听；
服务端调用 accept，等待客户端连接；
客户端调用 connect，向服务器端的地址和端口发起连接请求；
服务端 accept 返回用于传输的 socket 的文件描述符；
客户端调用 write 写入数据；服务端调用 read 读取数据；
客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，
就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。

# 26.ping

ping 是基于 ICMP 协议⼯作的

## 26-1：ICMP协议

这是一个互联⽹控制报⽂协议，ICMP 主要的功能包括：
确认 IP 包是否成功送达⽬标地址、报告发送过程中 IP 包被废弃的原因和改善⽹络设置
等。
在 IP 通信中如果某个 IP 包因为某种原因未能达到⽬标地址，那么这个具体的原因将由 ICMP 负责通知。




# ------操作系统--------------------------------------------

# 1. 内存管理-虚拟内存

## 1-1：什么是虚拟地址

我们可以把进程所使用的地址「隔离」开来，
让操作系统为每个进程分配独立的一套「虚拟地址」，
自己使用自己的地址就行，互不干涉。

## 1-2：操作系统是如何管理虚拟地址与物理地址之间的关系？

主要有两种方式，分别是

1. 内存分段
2. 内存分页

## 1-3：什么是虚拟内存

虚拟内存是一种存储模式，
通过这种模式能让我们有种感觉，
我们的内存本身能够处理远比内存大的多的数据或者文件。

### 1-3-1：虚拟内存的优缺点

`优点`
（1）可以使用有限的内存资源，
     处理比实际内存更大的文件或者数据
（2）更加高效的内存利用
（3）在有限的内存资源内，
    让系统运行更多的程序实例，
    因为每个程序都是按需取。
`缺点`
（1）如果内存严重不足，
     而处理超级大的文件时，
     会频繁引起内存和磁盘进行swap，从而降低系统性能。
（2）在多个应用程序之间切换会花费更多的时间
（3）虚拟内存本质上是充分了磁盘空间，
     但同时变相的提供用户使用的实际磁盘空间也会变小。

### 1-3-2：为什么虚拟内存可以大于物理内存

应该是是windows推出时候的一个技术措施，
用于缓解当时缓慢的硬盘速度，
由于内存比硬盘快很多，所以造成了大量的由硬盘读取的数据无法一下子读取，
所以就产生了这个虚拟内存技术，在空闲时候
预读
数据到虚拟内存上，其实虚拟内存也是硬盘空间，
但是由于数据已经经过处理所以比起单纯读取硬盘要快很多。
由于是预读数据，所以存在错误预读的情况，于是虚拟内存的容量会大于
物理内存
容量很多才能追上。


# 2.内存管理-内存分段

## 2-1：什么是内存分段

由于程序是由若干个逻辑分段组成的，
比如说是由代码分段、数据分段、栈段、堆段组成。
不同的段是有不同的属性的，
所以就用分段（Segmentation）的形式把这些段分离出来。

## 2-2：分段机制下，虚拟地址和物理地址是如何映射的？

分段机制下的虚拟地址由两部分组成，段选择子和段内偏移量

段选择子就保存在段寄存器里面。段选择子里面最重要的是段号，

用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。

虚拟地址中的段内偏移量应该位于 0 和段界限之间，

如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

也就是说虚拟地址是通过段表与物理地址进行映射的，

分段机制会把程序的虚拟地址分成 4 个段，

每个段在段表中有一个项，

在这一项找到段的基地址，

再加上偏移量，于是就能找到物理内存中的地址

## 2-3：访问某段偏移量xxx的虚拟地址

如访问段 3 中偏移量 500 的虚拟地址，

我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。

## 2-4：内存分段缺陷

1. 内存碎片的问题。

   比如说有 1G 的物理内存，用户执行了多个程序，其中：
         QQ占用了512MB内存，浏览器占用了128MB内存，音乐占用了256 MB内存。
这个时候，如果我们关闭了浏览器，则空闲内存还有256MB。
如果这个256MB不是连续的，被分成了两段128MB内存，这就会导致没有空间再打开一个200MB的程序。
这样就会在两处产生内存碎片问题：
    1）外部内存碎片，也就是产生了多个不连续的小物理内存，
                     导致新的程序无法被装载；
    2）内部内存碎片，程序所有的内存都被装载到了物理内存，
                     但是这个程序有部分的内存可能并不是很常使用
                     这也会导致内存的浪费；

2. 内存交换的效率低的问题

   对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，
   产生了内存碎片，那不得不重新 Swap 内存区域，这个过程会产生性能瓶颈。
   因为硬盘的访问速度要比内存慢太多了，每一次内存交换，
   我们都需要把一大段连续的内存数据写到硬盘上。
   所以，如果内存交换的时候，
   交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。




## 2-5：如何解决内存分段的缺陷

1. 解决外部内存碎片的问题就是内存交换。

   比如说可以把音乐程序占用的那256MB内存写到硬盘上，
   然后再从硬盘上读回来到内存里。
   不过再读回的时候，我们不能装载回原来的位置，
   而是紧紧跟着那已经被占用了的 512MB 内存后面。
   这样就能空缺出连续的 256MB 空间，
   于是新的 200MB 程序就可以装载进来。

2. 使用内存分页来解决

# 3. 内存管理-内存分页

## 3-1：为什么有内存分页（内存分页定义）

虽然分段的好处就是能产生连续的内存空间，
但是会出现内存碎片和内存交换的空间太大的问题。
要解决这些问题，那么就要想出能少出现一些内存碎片的办法。
另外，当需要进行内存交换的时候，
让需要交换写入或者从磁盘装载的数据更少一点，
这样就可以解决问题了。这个办法，也就是内存分页（Paging）。
分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。
这样一个连续并且尺寸固定的内存空间，这个也就是页。
在 Linux 下，每一页的大小为 4KB。虚拟地址与物理地址之间通过页表来映射

页表实际上存储在 CPU 的内存管理单元（MMU）中，
于是 CPU 就可以直接通过 MMU，找出要实际要访问的物理内存地址。
而当进程访问的虚拟地址在页表中查不到时，
系统会产生一个缺页异常，
进入系统内核空间分配物理内存、更新进程页表，
最后再返回用户空间，恢复进程的运行。

## 3-2：分页是怎么解决分段的内存碎片、内存交换效率低的问题？

因为采用了分页，那么释放的内存都是以页为单位释放的，
也就不会产生无法给进程使用的小内存。
如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」
的内存页面给释放掉，也就是暂时写在硬盘上，也是换出（Swap Out）。
一旦需要的时候，再加载进来，称为换入（Swap In）。
所以，一次性写入磁盘的也只有少数的一个页或者几个页，
不会花太多时间，内存交换的效率就相对比较高。
分页的方式使得我们在加载程序的时候，
不再需要一次性都把程序加载到物理内存中。
我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，
并不真的把页加载到物理内存里，而是只有在程序运行中，
需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。

## 3-3：分页机制下，虚拟地址和物理地址是如何映射的？

在分页机制下，虚拟地址分为两部分，
页号和页内偏移。页号作为页表的索引，
页表包含物理页每页所在物理内存的基地址，
这个基地址与页内偏移的组合就形成了物理内存地址

对于一个内存地址转换，其实就是这样三个步骤：

1. 第一步把虚拟内存地址，切分成页号和偏移量；

2. 第二步根据页号，从页表里面，查询对应的物理页号；

3. 第三步直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

## 3-4：简单的分页有什么缺陷吗？

有空间上的缺陷。

因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。
比如说在32位的环境下，虚拟地址空间共有4GB，如果说一个页的大小是4KB（2^12），
那么就需要大约几百万 （2^20）个页，
每个「页表项」需要 4 个字节大小来存储，
那么整个 4GB 空间的映射就需要有 4MB 的内存来存储页表。
这 4MB 大小的页表，看起来也不是很大。
但是因为每个进程都是有自己的虚拟地址空间的，也是有自己的页表的。
那么，100 个进程的话，就需要 400MB 的内存来存储页表，
这是非常大的内存了，

## 3-5：简单的分页缺陷的解决方案

要解决简单的分页问题，就需要采用多级页表（Multi-Level Page Table）的解决方案。

因为对于单页表的实现方式，在32位和页大小4KB的环境下，
一个进程的页表需要装下100多万个「页表项」，
并且每个页表项是占用4字节大小的，
于是相当于每个页表需占用4MB大小的空间。
我们把这个100多万个「页表项」的单级页表再分页，
将页表（一级页表）分为 1024 个页表（二级页表），
每个表（二级页表）中包含 1024 个「页表项」，形成二级分页。

## 3-6：分了二级表，内存不是变大了呢

每个进程都有 4GB 的虚拟地址空间，
而显然对于大多数程序来说，其使用到的空间远未达到 4GB，
因为会存在部分对应的页表项都是空的，根本没有分配，
对于已分配的页表项，如果存在最近一定时间未访问的页表，
在物理内存紧张的情况下，操作系统会将页面换出到硬盘，
也就是说不会占用物理内存。
如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，
但如果某个一级页表的页表项没有被用到，
也就不需要创建这个页表项对应的二级页表了，
也就是可以在需要时才创建二级页表。

## 3-7：为什么不分级的页表就做不到这样节约内存呢？

因为保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。
假如虚拟地址在页表中找不到对应的页表项，
计算机系统就不能工作了。
所以页表一定要覆盖全部虚拟地址空间，
而不分级的页表就需要有 100 多万个页表项来映射，
而二级分页则只需要1024个页表项
（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。
我们可以把二级分页再推广到多级页表，
就会发现页表占用的内存空间更少了，
这样其实对于64位的系统来说，两级分页肯定不够了，就变成了四级目录，

1. 全局页目录项 PGD（Page Global Directory）；
2. 上层页目录项 PUD（Page Upper Directory）；
3. 中间页目录项 PMD（Page Middle Directory）；
4. 页表项 PTE（Page Table Entry）；

## 3-8：多级页表的缺陷以及解决方案

多级页表虽然解决了空间上的问题，
但是虚拟地址到物理地址的转换就多了几道转换的工序，
这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。
程序是有局部性的，即在一段时间内，
整个程序的执行仅限于程序中的某一部分。
相应地，执行所访问的存储空间也局限于某个内存区域。
然后，在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，
这个 Cache 就是 TLB（Translation Lookaside Buffer） ，
通常称为页表缓存、转址旁路缓存、快表等
在 CPU 芯片里面，封装了内存管理单元（Memory Management Unit）芯片，
它用来完成地址转换和 TLB 的访问与交互。
有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。
TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。

# 4.内存管理-段页式内存管理

## 4-1：什么是段页式内存管理

因为内存分段和内存分页并不是对立的，
它们是可以组合起来在同一个系统中使用的，
那么组合起来后，通常称为段页式内存管理

## 4-2：段页式内存管理实现的方式

先将程序划分为多个有逻辑意义的段
接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；
这样，地址结构就由段号、段内页号和页内位移三部分组成。
用于段页式地址变换的数据结构是每一个程序一张段表，
每个段又建立一张页表，段表中的地址是页表的起始地址，
而页表中的地址则为某页的物理页号
段页式地址变换中要得到物理地址须经过三次内存访问：
第一次访问段表，得到页表起始地址；
第二次访问页表，得到物理页号；
第三次将物理页号与页内位移组合，得到物理地址。

# 5. 内存管理-linux内存管理

## 5-1：Linux 操作系统采用了哪种方式来管理内存

Linux 内存主要采用的是页式内存管理，
但同时也不可避免地涉及了段机制

Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），
也就是所有的段的起始地址都是一样的。
在Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，
所面对的地址空间都是线性地址空间（虚拟地址），
这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护

## 5-2：Linux 的虚拟地址空间是如何分布的？

在 Linux 操作系统中，
虚拟地址空间的内部又被分为内核空间和用户空间两部分，

## 5-3：内核空间与用户空间的区别

1. 进程在用户态时，只能访问用户空间内存；

2. 只有进入内核态后，才可以访问内核空间的内存；

虽然每个进程都各自有独立的虚拟内存，
但是每个虚拟内存中的内核地址，
其实关联的都是相同的物理内存。
这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

## 5-4：用户空间分布

有 7 种不同的内存段：

1. 程序文件段，包括二进制可执行代码；

2. 已初始化数据段，包括静态常量；

3. 未初始化数据段，包括未初始化的静态变量；

4. 堆段，包括动态分配的内存，从低地址开始向上增长；

5. 文件映射段，包括动态库、共享内存等，
   从低地址开始向上增长（跟硬件和内核版本有关）

6. 栈段，包括局部变量和函数调用的上下文等。
   栈的大小是固定的，一般是 8 MB。
   当然系统也提供了参数，以便我们自定义大小；


# 6.用户态和内核态

内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。

用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。

## 6-1：为什么要有用户态和内核态？

由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 
或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级 -- 用户态和内核态。

## 6-2：用户态与内核态的切换

用户态程序将一些数据值放在寄存器中, 或者使用参数创建一个堆栈(stack frame), 以此表明需要操作系统提供的服务.
用户态程序执行陷阱指令
CPU切换到内核态, 并跳到位于内存指定位置的指令, 这些指令是操作系统的一部分, 他们具有内存保护, 不可被用户态程序访问
这些指令称之为陷阱(trap)或者系统调用处理器(system call handler). 他们会读取程序放入内存的数据参数, 并执行程序请求的服务
系统调用完成后, 操作系统会重置CPU为用户态并返回系统调用的结果

## 6-3：用户态和内核态的概念区别

这两种状态的主要差别是

处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的
处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的。

内核态与用户态是操作系统的两种运行级别，当程序运行在3级特权级上时，
就可以称之为运行在用户态。因为这是最低特权级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态；

当程序运行在0级特权级上时，就可以称之为运行在内核态。

运行在用户态下的程序不能直接访问操作系统内核数据结构和程序。
当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态（比如操作硬件）。






# 7.内存管理-页面置换算法

## 7-1：页面置换算法

1. OPT ⻚⾯置换算法（最佳⻚⾯置换算法） ：
   
   最佳(Optimal, OPT)置换算法所选择的被淘汰⻚⾯将是以后永不使⽤的，
   或者是在最⻓时间内不再被访问的⻚⾯,这样可以保证获得最低的缺⻚率。
   但由于⼈们⽬前⽆法预知进程在内存下的若千⻚⾯中哪个是未来最⻓时间内不再被访问的，
   因⽽该算法⽆法实现。⼀般作为衡量其他置换算法的⽅法。

2. FIFO（First In First Out） ⻚⾯置换算法（先进先出⻚⾯置换算法） : 
   
   总是淘汰最先进⼊内存的⻚⾯，即选择在内存中驻留时间最久的⻚⾯进⾏淘汰。

3. LRU （Least Currently Used）⻚⾯置换算法（最近最久未使⽤⻚⾯置换算法） ： 
   
   LRU算法赋予每个⻚⾯⼀个访问字段，⽤来记录⼀个⻚⾯⾃上次被访问以来所经历的时间 T，
   当须淘汰⼀个⻚⾯时，选择现有⻚⾯中其 T 值最⼤的，即最近最久未使⽤的⻚⾯予以淘汰。

4. LFU （Least Frequently Used）⻚⾯置换算法（最少使⽤⻚⾯置换算法） : 
   
   该置换算法选择在之前时期使⽤最少的⻚⾯作为淘汰⻚。

## 6-2：⻚⾯置换算法的作⽤?

地址映射过程中，若在⻚⾯中发现所要访问的⻚⾯不在内存中，则发⽣缺⻚中断。
缺⻚中断 就是要访问的⻚不在主存，需要操作系统将其调⼊主存后再进⾏访问。 在这个时候，被
内存映射的⽂件实际上成了⼀个分⻚交换⽂件。
当发⽣缺⻚中断时，如果当前内存中并没有空闲的⻚⾯，操作系统就必须在内存选择⼀个⻚⾯将其移出
内存，以便为即将调⼊的⻚⾯让出空间。⽤来选择淘汰哪⼀⻚的规则叫做⻚⾯置换算法，我们可以把⻚
⾯置换算法看成是淘汰⻚⾯的规则。

## 6-3：手写LRU缓存

主要思路就是

1.可以使用最基础的单向链表处理
2.使用双向链表,可以加入hash表做优化
3.最简单的实现是使用JDK中自带的LinkedHashMap,
  需要重写removeEldestEntry()方法,
  这是LinkedHashMap提供的一个删除最老条目的方法;

```java
class LRUNode{
    private String key;
    private String value;
    private LRUNode pre;
    private LRUNode next;

    public LRUNode(String key, String value) {
        this.key = key;
        this.value = value;
    }
}


private volatile LRUNode head;
private volatile Integer lenght = 0;

public LRUcache(Integer lenght){
    this.lenght = lenght;
}

public  boolean put(String key,String value){

    LRUNode cur = this.head;
    Integer curLenght = 1;
    while (null != cur && cur.next != null){
        curLenght++;
        if(cur.key.equals(key)){
            //包含 修改其前后node的指向
            cur.pre.next = cur.next;
            cur.next.pre = cur.pre;

            cur.pre = null;
            cur.next = head;
            head.pre = cur;
            return true;
        }else {
            cur = cur.next;
        }
    }
    //没有找到
    LRUNode newNode = new LRUNode(key,value);
    if(curLenght >= lenght){
        //删除最后一个对象
        cur.pre.next = null;
        cur.pre = null;
        newNode.next = head;
        head.pre = newNode;

    }else{
        if(head != null){
            head.pre = newNode;
            newNode.next = head;
        }
    }
    head = newNode;
    return true;
}

public LRUNode get(String key){
    if(key.isEmpty()){
        throw new RuntimeException("key not is find");
    }
    LRUNode cur = head;
    while (lenght != 0 && cur != null){
        if(cur.key.equals(key)){
            if(null != cur.pre && null != cur.next){
                cur.pre.next = cur.next;
                cur.next.pre = cur.pre;

            }else if(null != cur.pre){
                cur.pre.next = null;
            }

            cur.pre = null;
            cur.next = head;
            head.pre = cur;
            head = cur;
            return cur;
        }
        cur = cur.next;
    }
    throw new NullPointerException("this key not find");
}
```

# 8.调度算法


## 1. 进程调度算法

### -1-1：什么时候会发⽣ CPU 调度呢

1. 当进程从运⾏状态转到等待状态；
2. 当进程从运⾏状态转到就绪状态；
3. 当进程从等待状态转到就绪状态；
4. 当进程从运⾏状态转到终⽌状态；

### -1-2：调度算法

1. 先来先服务调度算法
2. 最短作业优先调度算法
3. 高响应比优先调度算法
4. 时间片轮转调度算法
5. 最高优先级调度算法
6. 多级反馈队列调度算法

7. `非抢占式的先来先服务（First Come First Severd, FCFS）算法`
   先来后到，每次从就绪队列选择最先进入队列的进程，
   然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。
   这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。
   FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。
8. `最短作业优先（Shortest Job First, SJF）调度算法 `
   它会优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。
   这显然对长作业不利，很容易造成一种极端现象。
   比如，一个长作业在就绪队列等待运行，
   而这个就绪队列有非常多的短作业，
   那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。
9. `高响应比优先 （Highest Response Ratio Next, HRRN）调度算法`
   主要是权衡了短作业和长作业。
   每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，  
   响应比优先级的计算公式：
   优先权=（等待时间+要求服务时间）/要求服务时间
   - 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，
      「响应比」就越高，这样短作业的进程容易被选中运行；
   - 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，
   这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，
   当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；
10. `时间片轮转（Round Robin, RR）调度算法`
   每个进程被分配一个时间段，称为时间片（Quantum），即允许该进程在该时间段中运行。
   如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；
   如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；
   另外，时间片的长度就是一个很关键的点：
   - 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
   - 如果设得太长又可能引起对短作业进程的响应时间变长；
   注：通常时间片设为 20ms~50ms 通常是一个比较合理的折中值。
11. `最高优先级调度算法`
   希望调度程序能从就绪队列中选择最高优先级的进程进行运行，
   进程的优先级可以分为，静态优先级或动态优先级：
   - 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
   - 动态优先级：根据进程的动态变化调整优先级， 
   比如如果进程运行时间增加，则降低其优先级，
   如果进程等待时间（就绪队列的等待时间）增加， 
   则升高其优先级，也就是随着时间的推移增加等待进程的优先级。
   该算法也有两种处理优先级高的方法，非抢占式和抢占式：
   非抢占式：当就绪队列中出现优先级高的进程，
   运行完当前进程，再选择优先级高的进程。
   抢占式：当就绪队列中出现优先级高的进程，
   当前进程挂起，调度优先级高的进程运行。
   但是依然有缺点，可能会导致低优先级的进程永远不会运行。
12. `多级反馈队列（Multilevel Feedback Queue）调度算法`
   「多级」表示有多个队列，每个队列优先级从高到低，
   同时优先级越高时间片越短。
   「反馈」表示如果有新的进程加入优先级高的队列时，
   立刻停止当前正在运行的进程，转而去运行优先级高的队列；
   设置了多个队列，赋予每个队列不同的优先级，
   每个队列优先级从高到低，同时优先级越高时间片越短；
   新的进程会被放入到第一级队列的末尾，
   按先来先服务的原则排队等待被调度，
   如果在第一级队列规定的时间片没运行完成，
   则将其转入到第二级队列的末尾，以此类推，直至完成；
   当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。
   如果进程运行时，有新进程进入较高优先级的队列，
   则停止当前运行的进程并将其移入到原队列末尾，
   接着让较高优先级的进程运行；可以发现，
   对于短作业可能可以在第一级队列很快被处理完。
   对于长作业，如果在第一级队列处理不完，
   可以移入下次队列等待被执行，
   虽然等待的时间变长了，但是运行时间也会更长了，
   所以该算法很好的兼顾了长短作业，同时有较好的响应时间。
   ⾏；

## 2.页面置换算法

### -2-1：缺页

当 CPU 访问的⻚⾯不在物理内存时，便会产⽣⼀个缺⻚中断，请求操作系统将所缺⻚调⼊到物理内存。那
它与⼀般中断的主要区别在于：

1. 缺⻚中断在指令执⾏「期间」产⽣和处理中断信号，
   ⽽⼀般中断在⼀条指令执⾏「完成」后检查和处理中断信号。

2. 缺⻚中断返回到该指令的开始重新执⾏「该指令」，
   ⽽⼀般中断返回回到该指令的「下⼀个指令」执⾏。

### -2-2：缺页中断流程

1. 在 CPU ⾥访问⼀条 Load M 指令，然后 CPU 会去找 M 所对应的⻚表项。
2. 如果该⻚表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「⽆效
的」，则 CPU 则会发送缺⻚中断请求。
3. 操作系统收到了缺⻚中断，则会执⾏缺⻚中断处理函数，先会查找该⻚⾯在磁盘中的⻚⾯的位置。
4. 找到磁盘中对应的⻚⾯后，需要把该⻚⾯换⼊到物理内存中，但是在换⼊前，需要在物理内存中找空
闲⻚，如果找到空闲⻚，就把⻚⾯换⼊到物理内存中。
     找不到空闲⻚的话，就说明此时内存已满了，这时候，就需要「⻚⾯置换算法」选择⼀个物理⻚，如果该
     物理⻚有被修改过（脏⻚），则把它换出到磁盘，然后把该被置换出去的⻚表项的状态改成「⽆效的」，
     最后把正在访问的⻚⾯装⼊到这个物理⻚中。
5. ⻚⾯从磁盘换⼊到物理内存完成后，则把⻚表项中的状态位修改为「有效的」。
6. 最后，CPU 重新执⾏导致缺⻚异常的指令。
   
### -2-3：页面置换算法

1. 最优页面置换算法
当一个缺页中断发生时，对于保存在内存中的每一个逻辑页面，
计算在它的下一次访问之前，还需要等待多长时间，
从中选择时间最长的那个，作为被置换的页面
这是一种理想的页面置换算法，
在实际系统中是无法实现的，
因为操作系统无从知道每一个页面要等待多长时间
以后才会被访问可以作为其它算法性能评价的依据
2. 先进先出页面置换算法(First-In First-Out，FIFO)
选择在内存中驻留时间最长的页面并淘汰它
OS维护一个链表，记录了所有页面位于内存当中的逻辑页面，
从链表的排列顺序来看，链首页面的驻留时间最长，
页尾页面的驻留时间最短，当发生一个缺页中断时，
把链首页面淘汰出去，并把新的页面添加到链表的末尾
3. 最近最少使用算法(Least Recently Used，LRU)
当一个缺页中断发生时，选择最久未使用的那个页面，并淘汰它
此算法是最优页面置换算的一个近似，其依据是程序的局部性原理，
即在最近一小段时间内，如果某些页面被频繁的访问，
那么在将来的一段时间内，它们还可能会再次被频繁的访问；
反之，如果在过去某些页面长时间未被访问，
那么在将来他们还可能会长时间得不到访问
4. 时钟页面置换算法
需要用到页表项当中的访问位，
当一个页面被装入内存中时，
把该位初始化为0，
然后如果这个页面被访问(读/写)，
则把该位置1
把各个页面组织成环形链表(类似钟表面)，
把指针指向最老的页面(最先进来的)
当发生一个缺页中断时，考察指针所指向的最老页面，
若它的访问位为0，立即淘汰；若访问位为1，
则把该位置0，然后指针往下移动一个，
如此下去，直到找到被淘汰的页面，
然后把指针移动到它的下一个
5. 二次机会算法
此方法与时钟页面置换算法有些类似，
只是二次机会算法考察的是页表项中的 
access 和 dirty 两个位
还是将各个页面组织成环形链表，
当发生缺页中断时，
考察 access 和 dirty两个位，
进行第一轮扫描若找到两个位都是0的页，
直接淘汰；第一轮没有淘汰页，
第二轮扫描 (access == 0 && dirty == 1) 的页，
找到直接淘汰掉；第二轮扫描没有淘汰页，
第三轮扫描将 access 位全部置0，再进行前两轮扫描
6. 最不经常用算法(Least Frequently Used，LFU)
当缺页中断发生时，选择访问次数最少的那个页面，并淘汰之

## 3.磁盘调度算法

磁盘调度算法的⽬的很简单，就是为了提⾼磁盘的访问性能，⼀般是通过优化磁盘的访问请求顺序来做到
的。

## -3-1：常用算法

1. 先来先服务算法

先到来的请求，先被服务。

2. 最短寻道时间优先算法

优先选择从当前磁头位置所需寻道时
间最短的请求

3. 扫描算法算法

可以规定：磁头在⼀个⽅向上移动，访问所有未完成的请求，直到磁头到达该⽅向上
的最后的磁道，才调换⽅向，

4. 循环扫描算法

只有磁头朝某个特定⽅向移动时，才处理磁道访问请求，⽽返
回时直接快速移动⾄最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请
求，该算法的特点，就是磁道只响应⼀个⽅向上的请求。

5. LOOK 与 C-LOOK 算法

# 7.文件系统组成

## 7-1：什么是文件系统

文件系统是操作系统中负责管理持久数据的子系统，
就是负责把用户的文件存到磁盘硬件中，
因为即使计算机断电了，磁盘里的数据并不会丢失，
所以可以持久化的保存文件。

## 7-2：文件系统的组成

每个⽂件分配两个数据结构：索引节点（index node）和⽬录项（directory
entry），它们主要⽤来记录⽂件的元信息和⽬录层次结构。

1. 索引节点，也就是 inode，⽤来记录⽂件的元信息，⽐如 inode 编号、⽂件⼤⼩、访问权限、创建时
            间、修改时间、数据在磁盘的位置等等。索引节点是⽂件的唯⼀标识，它们之间⼀⼀对应，也同样都
            会被存储在硬盘中，所以索引节点同样占⽤磁盘空间。
2. ⽬录项，也就是 dentry，⽤来记录⽂件的名字、索引节点指针以及与其他⽬录项的层级关联关系。多
          个⽬录项关联起来，就会形成⽬录结构，但它与索引节点不同的是，⽬录项是由内核维护的⼀个数据
          结构，不存放于磁盘，⽽是缓存在内存。


### 7-2-1：⽬录项和⽬录是⼀个东⻄呢

不是⼀个东⻄，⽬录是个⽂件，持久化存储在磁盘，
⽬录项是内核⼀个数据结构，缓存在内存。

如果查询⽬录频繁从磁盘读，效率会很低，所以内核会把已经读过的⽬录⽤⽬录项这个数据结构缓存在内
存，下次再次读到相同的⽬录时，只需从内存读就可以，⼤⼤提⾼了⽂件系统的效率。

⽬录项这个数据结构不只是表示⽬录，也是可以表示⽂件的。

### 7-2-2：⽂件数据是如何存储在磁盘


# 8.设备管理

## 8-1：键盘敲⼊字⺟时，期间发⽣了什么？

当⽤户输⼊了键盘字符，键盘控制器就会产⽣扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接
着键盘控制器通过总线给 CPU 发送中断请求。
CPU 收到中断请求后，操作系统会保存被中断进程的 CPU 上下⽂，然后调⽤键盘的中断处理程序。
键盘的中断处理程序是在键盘驱动程序初始化时注册的，那键盘中断处理函数的功能就是从键盘控制器的
寄存器的缓冲区读取扫描码，再根据扫描码找到⽤户在键盘输⼊的字符，如果输⼊的字符是显示字符，那
就会把扫描码翻译成对应显示字符的 ASCII 码，⽐如⽤户在键盘输⼊的是字⺟ A，是显示字符，于是就会
把扫描码翻译成 A 字符的 ASCII 码。
得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏
幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲
区队列」的数据⼀个⼀个写⼊到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕
⾥。
显示出结果后，恢复被中断进程的上下⽂。















