# 1.进程

## 1-1：什么是进程

系统运行一个程序，从创建，运行到消亡的过程这个是一个进程

## 1-2：进程的状态

运行状态（Runing）：该时刻进程占用 CPU；
就绪状态（Ready）：可运行，但因为其他进程正在运行而暂停停止；
阻塞状态（Blocked）：该进程正在等待某一事件发生（如等待输入/输出操作的完成）
                     而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；
创建状态（new）：进程正在被创建时的状态；
结束状态（Exit）：进程正在从系统中消失时的状态；
挂起状态，它表示进程没有占有物理内存空间。
         挂起状态可以分为两种：
               * 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；
               * 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；

### 1-2-1：进程的状态变迁
NULL -> 创建状态：一个新进程被创建时的第一个状态；
创建状态 -> 就绪状态：当进程被创建完成并初始化后，
                     一切就绪准备运行时，
                     变为就绪状态，这个过程是很快的；
就绪态 -> 运行状态：处于就绪状态的进程被操作系统的进程调度器选中后，
                    就分配给 CPU 正式运行该进程；
运行状态 -> 结束状态：当进程已经运行完成或出错时，
                      会被操作系统作结束状态处理；
运行状态 -> 就绪状态：处于运行状态的进程在运行过程中，
                      由于分配给它的运行时间片用完，
                      操作系统会把该进程变为就绪态，
                      接着从就绪态选中另外一个进程运行；
运行状态 -> 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件；
阻塞状态 -> 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；

## 1-3：什么是进程的控制结构

在操作系统中，是用进程控制块（PCB）数据结构来描述进程的。

### 1-3-1：什么是PCB

PCB 是进程存在的唯一标识，这意味着一个进程的存在，
必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失。

### 1-3-2：PCB 具体包含什么信息呢？

1. 进程描述信息：
   * 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；
   * 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；
2. 进程控制和管理信息：
   * 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
   * 进程优先级：进程抢占 CPU 时的优先级；
3. 资源分配清单：
   * 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。
4. CPU 相关信息：
   CPU 中各个寄存器的值，当进程被切换时，
   CPU 的状态信息都会被保存在相应的 PCB 中，
   以便进程重新执行时，能从断点处继续执行。

### 1-3-3：每个 PCB 是如何组织的呢？

通常是通过链表的方式进行组织，把具有相同状态的进程链在一起，组成各种队列。
比如：将所有处于就绪状态的进程链在一起，称为就绪队列；
把所有因等待某事件而处于等待状态的进程链在一起就组成各种阻塞队列；
另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，
因为单核 CPU 在某个时间，只能运行一个程序。
除了链接的组织方式，还有索引方式，它的工作原理：
将同一状态的进程组织在一个索引表中，
索引表项指向相应的 PCB，不同状态对应不同的索引表。
一般会选择链表，因为可能面临进程创建，
销毁等调度导致进程状态发生变化，
所以链表能够更加灵活的插入和删除。

## 1-4：进程的控制

进程的创建、终止、阻塞、唤醒的过程，这些过程也就是进程的控制
1. `创建进程`
   操作系统允许一个进程创建另一个进程，
   而且允许子进程继承父进程所拥有的资源，
   当子进程被终止时，其在父进程处继承的资源应当还给父进程。
   同时，终止父进程时同时也会终止其所有的子进程。

      1）为新进程分配一个唯一的进程标识号，
         并申请一个空白的 PCB，PCB 是有限的，若申请失败则创建失败；
      2）为进程分配资源，此处如果资源不足，进程就会进入等待状态，以等待资源；
      3）初始化 PCB；
      4）如果进程的调度队列能够接纳新进程，那就将进程插入到就绪队列，等待被调度运行；
2. `终止进程`
   进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 kill 掉）。
      1）查找需要终止的进程的 PCB；
      2）如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；
      3）如果其还有子进程，则应将其所有子进程终止；
      4）将该进程所拥有的全部资源都归还给父进程或操作系统；
      5）将其从 PCB 所在队列中删除；
3. `阻塞进程`
   当进程需要等待某一事件完成时，
   它可以调用阻塞语句把自己阻塞等待。
   而一旦被阻塞等待，它只能由另一个进程唤醒。
      1）找到将要被阻塞进程标识号对应的 PCB；
      2）如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行；
      3）将该 PCB 插入的阻塞队列中去；
4. `唤醒进程`
   进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，
   所以处于阻塞状态的进程是绝对不可能叫醒自己的。
   如果某进程正在等待 I/O 事件，
   需由别的进程发消息给它，
   则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。
      1）在该事件的阻塞队列中找到相应进程的 PCB；
      2）将其从阻塞队列中移出，并置其状态为就绪状态；
      3）把该 PCB 插入到就绪队列中，等待调度程序调度；

## 1-5：进程间通信

1. 管道
2. 消息队列
3. 共享内存
4. 信号量
5. 信号
6. socket

### 1-5-1：管道

就是内核里面的一串缓存。
从管道的一段写入的数据，实际上是缓存在内核中的，
另一端读取，也就是从内核中读取这段数据。
另外，管道传输的数据是无格式的流且大小受限。

将前一个命令的输出，作为后一个命令的输入

#### 1-5-1-1：管道分类

1. 「|」表示的管道称为匿名管道，用完了就销毁。
2. 命名管道，也被叫做FIFO，因为数据是先进先出的传输方式。
             在使用命名管道前，先需要通过 mkfifo 命令来创建，并且指定管道名字：

### 1-5-2：消息队列

比如说A 进程要给 B 进程发送消息，
A 进程把数据放在对应的消息队列后就可以正常返回了，
B 进程需要的时候再去读取数据就可以了。
同理，B 进程要给 A 进程发送消息也是如此

消息队列是保存在内核中的消息链表，在发送数据时，
会分成一个一个独立的数据单元，也就是消息体（数据块），
消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，
所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。
如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。

#### 1-5-2-1：消息队列的不足

1. 消息队列不适合比较大数据的传输，
   因为在内核中每个消息体都有一个最大长度的限制，
   同时所有队列所包含的全部消息体的总长度也是有上限。
2. 消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销，
   因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，
   相同，另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。

### 1-5-3：共享内存

共享内存的机制，就是拿出一块虚拟地址空间来，
映射到相同的物理内存中。这样这个进程写入的东西，
另外一个进程马上就能看到了，都不需要拷贝来拷贝去，
传来传去，大大提高了进程间通信的速度

### 1-5-4：信号量

信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，
而不是用于缓存进程间通信的数据。

#### 1-5-4-1：控制信号量的方式

1. 一个是 P 操作，
   这个操作会把信号量减去 -1，相减后如果信号量 < 0，
   则表明资源已被占用，进程需阻塞等待；
   相减后如果信号量 >= 0，则表明还有资源可使用，进程可正常继续执行。
2. 一个是 V 操作，
   这个操作会把信号量加上 1，相加后如果信号量 <= 0，
   则表明当前有阻塞中的进程，于是会将该进程唤醒运行；
   相加后如果信号量 > 0，则表明当前没有阻塞中的进程；
P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。

#### 1-5-4-2：两个进程互斥访问共享内存流程

1. 进程 A 在访问共享内存前，先执行了 P 操作，
   由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，
   表示共享资源可用，于是进程 A 就可以访问共享内存。
2. 若此时，进程 B 也想访问共享内存，执行了 P 操作，
   结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。
3. 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，
   接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，
   最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。

#### 1-5-4-3：信号量来实现多进程同步的方式

1. 如果进程 B 比进程 A 先执行了，那么执行到 P 操作时，
   由于信号量初始值为 0，故信号量会变为 -1，
   表示进程 A 还没生产数据，于是进程 B 就阻塞等待；
2. 接着，当进程 A 生产完数据后，执行了 V 操作，
   就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B；
3. 最后，进程 B 被唤醒后，意味着进程 A 已经生产了数据，
   于是进程 B 就可以正常读取数据了。

### 1-5-5：信号

信号是进程间通信机制中唯一的异步通信机制，
因为可以在任何时候发送信号给某一进程，
一旦有信号产生，有几种方式可以进行用户进程对信号的处理方式

#### 1-5-5-1：用户进程对信号的处理方式

1. 执行默认操作。Linux 对每种信号都规定了默认操作，
                 比如终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面，
                 方便程序员事后进行分析问题在哪里
2. 捕捉信号。我们可以为信号定义一个信号处理函数。
             当信号发生时，我们就执行相应的信号处理函数。
3. 忽略信号。当我们不希望处理某些信号的时候，
             就可以忽略该信号，不做任何处理。
             有两个信号是应用进程无法捕捉和忽略的，
             即 SIGKILL 和 SEGSTOP，它们用于在任何时候中断或结束某一进程。

### 1-5-6：Socket

跨网络与不同主机上的进程之间通信，就需要Socket通信了

## 1-6：进程的内存结构

代码区：存放CPU执行的机器指令，代码区是可共享，并且是只读的。
数据区：存放已初始化的全局变量、静态变量（全局和局部）、常量数据。
BBS区：存放的是未初始化的全局变量和静态变量。
栈区：由编译器自动分配释放，存放函数的参数值、返回值和局部变量，
     在程序运行过程中实时分配和释放，栈区由操作系统自动管理，无须程序员手动管理。
堆区：堆是由malloc()函数分配的内存块，使用free()函数来释放内存，
      堆的申请释放工作由程序员控制，容易产生内存泄漏。


# 2.线程

## 2-1：线程概念

线程是进程当中的一条执行流程
同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，
但每个线程都有独立一套的寄存器和栈，这样可以确保线程的控制流是相对独立的

### 2-1-1：为什么使用线程

对于单进程的这种方式，存在几个问题：
  1. 比如说我看电影，播放出来的画面和声音会不连贯，
     因为当 CPU 能力不够强的时候，
     Read的时候可能进程就等在这了，
     这样就会导致等半天才进行数据解压和播放；
  2. 各个函数之间不是并发执行，影响资源的使用效率；
那改进成多进程的方式，依然会存在问题：
比如说进程之间如何通信，共享数据
维护进程的系统开销较大，如创建进程时，
分配资源、建立 PCB；终止进程时，
回收资源、撤销 PCB；进程切换时，
保存当前进程的状态信息；
为了满足实体之间可以并发运行；
       实体之间共享相同的地址空间；
所以就是线程，线程之间可以并发运行且共享相同的地址空间。

### 2-1-2：线程的优缺点

1. 线程的优点：
   一个进程中可以同时存在多个线程；
   各个线程之间可以并发执行；
   各个线程之间可以共享地址空间和文件等资源；
2. 线程的缺点：
   当进程中的一个线程奔溃时，
   会导致其所属进程的所有线程奔溃。

## 2-2：三种线程的实现方式

1. `用户线程（User Thread）`
   在用户空间实现的线程，不是由内核管理的线程，
   是由用户态的线程库来完成线程的管理；
2. `内核线程（Kernel Thread）`
   在内核中实现的线程，是由内核管理的线程；
3. `轻量级进程（LightWeight Process）`
   在内核中来支持用户线程；

### 2-2-1：用户线程和内核线程的对应关系

第一种关系是多对一的关系，也就是多个用户线程对应同一个内核线程：
第二种是一对一的关系，也就是一个用户线程对应一个内核线程：
第三种是多对多的关系，也就是多个用户线程对应到多个内核线程：

### 2-2-2：用户线程如何理解？

用户线程是基于用户态的线程管理库来实现的，
那么线程控制块（Thread Control Block, TCB） 
也是在库里面来实现的，
对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。
用户线程的整个线程管理和调度，
操作系统是不直接参与的，
而是由用户级线程库函数来完成线程的管理，
包括线程的创建、终止、同步和调度等。

#### 2-2-2-1：用户线程存在什么优势和缺陷？

1. 用户线程的优点：
   1）每个进程都需要有它私有的线程控制块（TCB）列表，
      用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），
      TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统；
   2）用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快；
2. 用户线程的缺点：
   1）由于操作系统不参与线程的调度，
      如果一个线程发起了系统调用而阻塞，
      那进程所包含的用户线程都不能执行了。
   2）当一个线程开始运行后，
      除非它主动地交出 CPU 的使用权，
      否则它所在的进程当中的其他线程无法运行，
      因为用户态的线程没法打断当前运行中的线程，
      它没有这个特权，只能操作系统才有，
      但是用户线程不是由操作系统管理的。
   3）由于时间片分配给进程，
      与其他进程比，在多线程执行时，
      每个线程得到的时间片较少，执行会比较慢；

### 2-2-3：内核线程如何理解？

内核线程是由操作系统管理的，
线程对应的 TCB 自然是放在操作系统里的，
这样线程的创建、终止和管理都是由操作系统负责。

#### 2-2-3-1：内核线程存在什么优势和缺陷

1. 内核线程的优点：
   1）在一个进程当中，
      如果某个内核线程发起系统调用而被阻塞，
      并不会影响其他内核线程的运行；
   2）分配给线程，多线程的进程获得更多的 CPU 运行时间；
2. 内核线程的缺点：
   1）在支持内核线程的操作系统中，
      由内核来维护进程和线程的上下问信息，如 PCB 和 TCB；
   2）线程的创建、终止和切换都是通过系统调用的方式来进行，
      因此对于系统来说，系统开销比较大；

### 2-2-4：轻量级进程如何理解

轻量级进程（Light-weight process，LWP）是内核支持的用户线程，
一个进程可有一个或多个LWP，每个 LWP 是跟内核线程一对一映射的，
也就是 LWP 都是由一个内核线程支持。
LWP与普通进程的区别也在于它只有一个最小的执行上下文和调度程序所需的统计信息。

### 2-2-5： LWP 与用户线程的对应关系

1. 1 : 1 模式
   一个线程对应到一个 LWP 再对应到一个内核线程
   优点：实现并行，当一个 LWP 阻塞，不会影响其他 LWP；
   缺点：每一个用户线程，就产生一个内核线程，创建线程的开销较大。
2. N : 1 模式
   多个用户线程对应一个 LWP 再对应一个内核线程，
   线程管理是在用户空间完成的，
   此模式中用户的线程对操作系统不可见。
   优点：用户线程要开几个都没问题，且上下文切换发生用户空间，
         切换的效率较高；
   缺点：一个用户线程如果阻塞了，则整个进程都将会阻塞，
        另外在多核 CPU  中，是没办法充分利用 CPU 的。
3. M : N 模式
   根据前面的两个模型混搭一起，就形成 M:N 模型，
   该模型提供了两级控制，首先多个用户线程对应到多个 LWP，
   LWP 再一一对应到内核线程，
   优点：综合了前两种优点，大部分的线程上下文发生在用户空间，
         且多个线程又可以充分利用多核 CPU 的资源。
4. 组合模式
   结合 1:1 模型和 M:N 模型。
   开发人员可以针对不同的应用特点调节内核线程的
   数目来达到物理并行性和逻辑并行性的最佳方案。

## 2-3：线程之间通信方式

1. 同步
2. while轮询的方式
3. wait/notify机制
4. 管道通信

### 2-3-1：同步

通过synchronized关键字这种方式来实现线程间的通信。
本质上就是“共享内存”式的通信。
多个线程需要访问同一个共享变量，
谁拿到了锁（获得了访问权限），谁就可以执行。

```java
public class MyObject {

    synchronized public void methodA() {//do something....}
    synchronized public void methodB() {//do some other thin}}

public class ThreadA extends Thread {
    private MyObject object;
//省略构造方法
    @Override
    public void run() {
        super.run();
        object.methodA();}}
public class ThreadB extends Thread {
    private MyObject object;
//省略构造方法
    @Override
    public void run() {
        super.run();
        object.methodB();}}
public class Run {
    public static void main(String[] args) {
        MyObject object = new MyObject();

        //线程A与线程B 持有的是同一个对象:object
        ThreadA a = new ThreadA(object);
        ThreadB b = new ThreadB(object);
        a.start();
        b.start();}}
```
### 2-3-2：while轮询的方式

```java
public class MyList {
    private List<String> list = new ArrayList<String>();
    public void add() {
        list.add("elements");}
    public int size() {
        return list.size();}}
-------------------------------------------------------
public class ThreadA extends Thread {
    private MyList list;
    public ThreadA(MyList list) {
        super();
        this.list = list;
    }
    @Override
    public void run() {
        try {
            for (int i = 0; i < 10; i++) {
                list.add();
                System.out.println("添加了" + (i + 1) + "个元素");
                Thread.sleep(1000);
            }
        } catch (InterruptedException e) {
            e.printStackTrace();}}}
---------------------------------------------------------------------
public class ThreadB extends Thread {
    private MyList list;
    public ThreadB(MyList list) {
        super();
        this.list = list;}
  @Override
    public void run() {
        try {
            while (true) {
                if (list.size() == 5) {
                    System.out.println("==5, 线程b准备退出了");
                    throw new InterruptedException();}}}
               atch (InterruptedException e) {
            e.printStackTrace();}}}
------------------------------------------------------------------
public class Test {
    public static void main(String[] args) {
        MyList service = new MyList();
        ThreadA a = new ThreadA(service);
        a.setName("A");
        a.start();
        ThreadB b = new ThreadB(service);
        b.setName("B");
        b.start();}}
```
线程A不断地改变条件，
线程ThreadB不停地通过while语句检测这个条件是否成立 ，
从而实现了线程间的通信。但是这种方式会浪费CPU资源。
因为JVM调度器将CPU交给线程B执行时，它没做啥“有用”的工作，
只是在不断地测试 某个条件是否成立。
线程都是先把变量读取到本地线程栈空间，
然后再去再去修改的本地变量。
因此，如果线程B每次都在取本地的条件变量，
那么尽管另外一个线程已经改变了轮询的条件，
它也察觉不到，这样也会造成死循环。

### 2-3-3：wait/notify机制

```java
public class MyList {
    private static List<String> list = new ArrayList<String>();
    public static void add() {
        list.add("anyString");}
    public static int size() {
        return list.size();}}
--------------------------------------------------------------------------
public class ThreadA extends Thread {
    private Object lock;
    public ThreadA(Object lock) {
        super();
        this.lock = lock;}
    @Override
    public void run() {
        try {
            synchronized (lock) {
                if (MyList.size() != 5) {
                    System.out.println("wait begin "
                            + System.currentTimeMillis());
                    lock.wait();
                    System.out.println("wait end  "
                            + System.currentTimeMillis());}}}
                catch (InterruptedException e) {
            e.printStackTrace();}}}
----------------------------------------------------------------
public class ThreadB extends Thread {
    private Object lock;
    public ThreadB(Object lock) {
        super();
        this.lock = lock;}
    @Override
    public void run() {
        try {
            synchronized (lock) {
                for (int i = 0; i < 10; i++) {
                    MyList.add();
                    if (MyList.size() == 5) {
                        lock.notify();
                        System.out.println("已经发出了通知");
                    }
                    System.out.println("添加了" + (i + 1) + "个元素!");
                    Thread.sleep(1000);}}}
                catch (InterruptedException e) {
            e.printStackTrace();}}}

public class Run {
    public static void main(String[] args) {
        try {
            Object lock = new Object();
            ThreadA a = new ThreadA(lock);
            a.start();
            Thread.sleep(50);
            ThreadB b = new ThreadB(lock);
            b.start();
        } catch (InterruptedException e) {
            e.printStackTrace();}}}
       
```
比如，线程B先执行，
一下子添加了5个元素并调用了notify()发送了通知，
而此时线程A还执行；当线程A执行并调用wait()时，
那它永远就不可能被唤醒了。
因为，线程B已经发了通知了，
以后不再发通知了。
这说明：通知过早，会打乱程序的执行逻辑。

### 2-3-4：管道通信

# 3.线程与进程的区别

1. 拥有资源
   进程是资源分配的基本单位，虽然线程不拥有资源，线程可以访问隶属进程的资源
2. 调度
   线程是独立调度的基本单位
3. 线程的创建时间比进程快，因为进程在创建的过程中，
   还需要资源管理信息，比如内存管理信息、文件管理信息，
   而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；
4. 线程的终止时间比进程快，因为线程释放的资源相比进程少很多；
   同一个进程内的线程切换比进程切换快，
   因为线程具有相同的地址空间（虚拟内存共享），
   这意味着同一个进程的线程都具有同一个页表，
   那么在切换的时候不需要切换页表。
   而对于进程之间的切换，
   切换的时候要把页表给切换掉，
   而页表的切换过程开销是比较大的；
   由于同一进程的各线程间共享内存和文件资源，
   那么在线程之间数据传递的时候，
   就不需要经过内核了，这就使得线程之间的数据交互效率更高了；
5. 通信方面
   线程间可以通过读写进行通信，进程通信需要IPC（进程间通信）

## 3-1：进程切换与线程切换的区别

进程切换与线程切换的一个最主要区别就在于进程切换涉及到虚拟地址空间的切换而线程切换则不会。
因为每个进程都有自己的虚拟地址空间，而线程是共享所在进程的虚拟地址空间的，
因此同一个进程中的线程进行线程切换时不涉及虚拟地址空间的转换。

## 3-2：为什么进程花销比线程花销大

由于保存线程的上下文明显比进程的上下文小，因此系统切换线程时，必然开销更小。

# 4. 上下文切换
## 4-1：什么是上下⽂切换?

当前任务在执⾏完 CPU时间⽚ 切换到另⼀个任务 
之前会先保存⾃⼰的状态，以便下次再切换回这个任务时，
可以再加载这个任务的状态。 任务从保存到再加载的过程就是⼀次上下⽂切换。

## 4-2：CPU 上下文切换

CPU 上下文切换就是先把前一个任务的 CPU 上下文
（CPU 寄存器和程序计数器）保存起来，
然后加载新任务的上下文到这些寄存器和程序计数器，
最后再跳转到程序计数器所指的新位置，运行新任务。
这个新任务比如说进程、线程和中断

## 4-3：进程的上下文

## 4-3-1：进程上下文概念

各个进程之间是共享 CPU 资源的，
在不同的时候进程之间需要切换，
让不同的进程可以在 CPU 执行，
那么这个一个进程切换到另一个进程运行

### 4-3-2：进程的上下文切换到底是切换什么呢？

因为进程是由内核管理和调度的，所以进程的切换只能发生在内核态。
进程的上下文切换比如说，虚拟内存、栈、全局变量等用户空间的资源，
应该还有比如说内核堆栈、寄存器等内核空间的资源。
通常，会把交换的信息保存在进程的 PCB，
当要运行另外一个进程的时候，
我们需要从这个进程的 PCB 取出上下文，
然后恢复到 CPU 中，这使得这个进程可以继续执行

### 4-3-3：发生进程上下文切换有哪些场景？

1）为了保证所有进程可以得到公平调度，
   CPU 时间被划分为一段段的时间片，
   这些时间片再被轮流分配给各个进程。这样，
   当某个进程的时间片耗尽了，就会被系统挂起，
   切换到其它正在等待 CPU 的进程运行；

2）进程在系统资源不足（比如内存不足）时，
   要等到资源满足后才可以运行，
   这个时候进程也会被挂起，
   并由系统调度其他进程运行；

3）当进程通过睡眠函数 sleep 
   这样的方法将自己主动挂起时，
   自然也会重新调度；

4）当有优先级更高的进程运行时，
   为了保证高优先级进程的运行，
   当前进程会被挂起，由高优先级进程来运行；

5）发生硬件中断时，CPU 上的进程会被中断挂起，
   转而执行内核中的中断服务程序；

## 4-4：线程上下文

1. 当两个线程不是属于同一个进程，
   则切换的过程就跟进程上下文切换一样；

2. 当两个线程是属于同一个进程，
   因为虚拟内存是共享的，所以在切换时，
   虚拟内存这些资源就保持不动，
   只需要切换线程的私有数据、寄存器等不共享的数据；

# 5.调度

## 5-1：什么是调度

选择一个进程运行这一功能是在操作系统中完成的

## 5-2：什么时候会发生调度

在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，
其实会触发一次调度
比如说
1. 从就绪态 -> 运行态：当进程被创建时，
               会进入到就绪队列，
               操作系统会从就绪队列选择一个进程运行；
2. 从运行态 -> 阻塞态：当进程发生 I/O 事件而阻塞时，
               操作系统必须另外一个进程运行；
3. 从运行态 -> 结束态：当进程退出结束后，
               操作系统得从就绪队列选择另外一个进程运行；

## 5-3：调度原则

1. 原则一：如果运行的程序，
   发生了 I/O 事件的请求，那 CPU 使用率必然会很低，
   因为此时进程在阻塞等待硬盘的数据返回。
   这样的过程，势必会造成 CPU 突然的空闲。所以，
   为了提高 CPU 利用率，在这种发送 I/O 事件致使 CPU 空闲的情况下，
   调度程序需要从就绪队列中选择一个进程来运行。
2. 原则二：程序执行某个任务花费的时间会比较长，
   如果这个程序一直占用着 CPU，
   会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。
   所以，要提高系统的吞吐率，调度程序要权衡长任务和短任务进程的运行完成数量。
3. 原则三：从进程开始到结束的过程中，实际上是包含两个时间，
   分别是进程运行时间和进程等待时间，这两个时间总和就称为周转时间。
   进程的周转时间越小越好，如果进程的等待时间很长而运行时间很短，
   那周转时间就很长，这不是我们所期望的，调度程序应该避免这种情况发生。
4. 原则四：处于就绪队列的进程，也不能等太久，
   当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执行。
   所以，就绪队列中进程的等待时间也是调度程序所需要考虑的原则。
5. 原则五：对于鼠标、键盘这种交互式比较强的应用，
   我们当然希望它的响应时间越快越好，否则就会影响用户体验了。
   所以，对于交互式比较强的应用，响应时间也是调度程序需要考虑的原则。

## 5-4：进程调度算法

1. 先来先服务调度算法
2. 最短作业优先调度算法
3. 高响应比优先调度算法
4. 时间片轮转调度算法
5. 最高优先级调度算法
6. 多级反馈队列调度算法

1. `非抢占式的先来先服务（First Come First Severd, FCFS）算法`
   先来后到，每次从就绪队列选择最先进入队列的进程，
   然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。
   这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。
   FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。
2. `最短作业优先（Shortest Job First, SJF）调度算法 `
   它会优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。
   这显然对长作业不利，很容易造成一种极端现象。
   比如，一个长作业在就绪队列等待运行，
   而这个就绪队列有非常多的短作业，
   那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。
3. `高响应比优先 （Highest Response Ratio Next, HRRN）调度算法`
   主要是权衡了短作业和长作业。
   每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，  
   响应比优先级的计算公式：   
   优先权=（等待时间+要求服务时间）/要求服务时间
   - 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，
      「响应比」就越高，这样短作业的进程容易被选中运行；
   - 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，
   这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，
   当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；
4. `时间片轮转（Round Robin, RR）调度算法`
   每个进程被分配一个时间段，称为时间片（Quantum），即允许该进程在该时间段中运行。
   如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；
   如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；
   另外，时间片的长度就是一个很关键的点：
   - 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
   - 如果设得太长又可能引起对短作业进程的响应时间变长；
   注：通常时间片设为 20ms~50ms 通常是一个比较合理的折中值。
5. `最高优先级调度算法`
   希望调度程序能从就绪队列中选择最高优先级的进程进行运行，
   进程的优先级可以分为，静态优先级或动态优先级：
   - 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
   - 动态优先级：根据进程的动态变化调整优先级， 
   比如如果进程运行时间增加，则降低其优先级，
   如果进程等待时间（就绪队列的等待时间）增加， 
   则升高其优先级，也就是随着时间的推移增加等待进程的优先级。
   该算法也有两种处理优先级高的方法，非抢占式和抢占式：
   非抢占式：当就绪队列中出现优先级高的进程，
   运行完当前进程，再选择优先级高的进程。
   抢占式：当就绪队列中出现优先级高的进程，
   当前进程挂起，调度优先级高的进程运行。
   但是依然有缺点，可能会导致低优先级的进程永远不会运行。
6. `多级反馈队列（Multilevel Feedback Queue）调度算法`
   「多级」表示有多个队列，每个队列优先级从高到低，
   同时优先级越高时间片越短。
   「反馈」表示如果有新的进程加入优先级高的队列时，
   立刻停止当前正在运行的进程，转而去运行优先级高的队列；
   设置了多个队列，赋予每个队列不同的优先级，
   每个队列优先级从高到低，同时优先级越高时间片越短；
   新的进程会被放入到第一级队列的末尾，
   按先来先服务的原则排队等待被调度，
   如果在第一级队列规定的时间片没运行完成，
   则将其转入到第二级队列的末尾，以此类推，直至完成；
   当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。
   如果进程运行时，有新进程进入较高优先级的队列，
   则停止当前运行的进程并将其移入到原队列末尾，
   接着让较高优先级的进程运行；可以发现，
   对于短作业可能可以在第一级队列很快被处理完。
   对于长作业，如果在第一级队列处理不完，
   可以移入下次队列等待被执行，
   虽然等待的时间变长了，但是运行时间也会更长了，
   所以该算法很好的兼顾了长短作业，同时有较好的响应时间。

# 6.互斥与同步

## 6-1：互斥概念

互斥就说保证一个线程在临界区执行时，
其他线程应该被阻止进入临界区，
也就是这段代码执行过程中，最多只能出现一个线程。

### 6-1-1：临界区

当多线程相互竞争操作共享变量时，
由于随机性，也是在执行过程中发生了上下文切换，
每次运行都可能得到不同的结果，因此输出的结果存在不确定性（indeterminate）。

由于多线程执行操作共享变量的这段代码可能会导致竞争状态，
因此我们将此段代码称为临界区（critical section），
它是访问共享资源的代码片段，一定不能给多线程同时执行。

## 6-2：同步概念

就是并发进程/线程在一些关键点上可能需要互相等待与互通消息，
这种相互制约的等待与互通信息称为进程/线程同步

### 6-2-1：同步的原理

同步主要是基于进入和退出Monitor对象来实现方法同步和代码块同步，
但两者的实现细节不一样。都是通过使用monitorenter和monitorexit指令实现，
monitorenter指令是在编译后插入到同步代码块的开始位置，
而monitorexit是插入到方法结束处和异常处，
JVM要保证每个monitorenter必须有对应的monitorexit与之配对。
任何对象都有一个 monitor 与之关联，当且一个monitor 被持有后，
它将处于锁定状态。线程执行到 monitorenter 指令时，
将会尝试获取对象所对应的 monitor 的所有权，即尝试获得对象的锁。

### 6-2-1：线程同步的方式

1. 互斥量(Mutex)：采用互斥对象机制，
   只有拥有互斥对象的线程才有访问公共资源的权限。
   因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。
   比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。
2. 信号量(Semphares) ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量
3. 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操

### 6-2-2：同步方法和同步代码块的区别是什么

同步方法默认用this或者当前类class对象作为锁；
同步代码块可以选择以什么来加锁，比同步方法要更细颗粒度，
我们可以选择只同步会发生同步问题的部分代码而不是整个方法；

### 6-2-3：在监视器内部，是如何做线程同步的？

监视器和锁在Java虚拟机中是一块使用的。
监视器监视一块同步代码块，
确保一次只有一个线程执行同步代码块。
每一个监视器都和一个对象引用相关联。
线程在获取锁之前不允许执行同步代码。
java 还提供了显式监视器( Lock )和隐式监视器( synchronized )两种锁方案。

## 6-3：同步与互斥的区别

1. 同步就好比：「操作 A 应在操作 B 之前执行」，
   「操作 C 必须在操作 A 和操作 B 都完成之后才能执行」等；

2. 互斥就好比：「操作 A 和操作 B 不能在同一时刻执行」；

### 6-3-1：互斥与同步实现

1. 锁
   
   使用加锁操作和解锁操作可以解决并发线程/进程的互斥问题
   任何想进入临界区的线程，必须先执行加锁操作。若加锁操作顺利通过，
   则线程可进入临界区；在完成对临界资源的访问后再执行解锁操作，以释放该临界资源。

2. 信号量

# 7.生产者与消费者问题（同步问题）

需要三个信号量，分别是：

1. 互斥信号量 mutex：用于互斥访问缓冲区，初始化值为 1；

2. 资源信号量 fullBuffers：用于消费者询问缓冲区是否有数据，
                           有数据则读取数据，初始化值为 0
                           （表明缓冲区一开始为空）；

3. 资源信号量 emptyBuffers：用于生产者询问缓冲区是否有空位，
                           有空位则生成数据，初始化值为 n （缓冲区大小）；

## 7-1：java中几种解决同步问题的方式

（1）wait()与notify()方法
（2）Lock与Condition机制
（3）BlockingQueue阻塞队列
方式一：
```java
public class ProAndCon {
    //最大容量
    public static final int MAX_SIZE = 2;
    //存储媒介
    public static LinkedList<Integer> list = new LinkedList<>();
    class Producer implements Runnable {
        @Override
        public void run() {
            synchronized (list) {
                //仓库容量已经达到最大值
                while (list.size() == MAX_SIZE) {
                    System.out.println("仓库已满，生产者" + Thread.currentThread().getName() + "不可生产.");
                    try {
                        list.wait();
                    } catch (InterruptedException e) {
                        e.printStackTrace();}}
                list.add(1);
                System.out.println("生产者" + Thread.currentThread().getName() + "生产, 仓库容量为" + list.size());
                list.notify();}}}
    class Consumer implements Runnable {
        @Override
        public void run() {
            synchronized (list) {
                while (list.size() == 0) {
                    System.out.println("仓库为空，消费者" + Thread.currentThread().getName() + "不可消费.");
                    try {
                        list.wait();
                    } catch (InterruptedException e) {
                        e.printStackTrace();}}
                list.removeFirst();
                System.out.println("消费者" + Thread.currentThread().getName() + "消费，仓库容量为" + list.size());
                list.notify();}}}
    public static void main(String[] args) {
        ProAndCon proAndCon = new ProAndCon();
        Producer producer = proAndCon.new Producer();
        Consumer consumer = proAndCon.new Consumer();
        for (int i = 0; i < 10; i++) {
            Thread pro = new Thread(producer);
            pro.start();
            Thread con = new Thread(consumer);
            con.start();}}}
```
方式二：

在JDK5.0之后，Java提供了Lock与Condition机制。
Condition接口的await()和signal()是用来做同步的两种方法，
它们的功能基本上和Object的wait()、nofity()相同，
或者说可以取代它们，但是它们和Lock机制是直接挂钩的。
通过在Lock对象上调用newCondition()方法，
将条件变量和一个锁对象进行绑定，
进而控制并发程序访问竞争资源的安全。

```java
public class ProAndCon2 {
    public static final int MAX_SIZE = 2;
    public static LinkedList<Integer> list = new LinkedList<>();
    public static Lock lock = new ReentrantLock();
    //仓库满的条件变量
    public static Condition full = lock.newCondition();
    //仓库空的条件变量
    public static Condition empty = lock.newCondition();
    class Producer implements Runnable {
        @Override
        public void run() {
            lock.lock();
            while (list.size() == MAX_SIZE) {
                try {
                    System.out.println("仓库已满，生产者" + Thread.currentThread().getName() + "不可生产.");
                    full.await();
                } catch (InterruptedException e) {
                    e.printStackTrace();}}
            list.add(1);
            System.out.println("生产者" + Thread.currentThread().getName() + "生产, 仓库容量为" + list.size());
            //唤醒其他生产者与消费者线程
            full.signal();
            empty.signal();
            lock.unlock();}}
    class Consumer implements Runnable {
        @Override
        public void run() {
            lock.lock();
            while (list.size() == 0) {
                try {
                    System.out.println("仓库为空，消费者" + Thread.currentThread().getName() + "不可消费.");
                    empty.await();
                } catch (InterruptedException e) {
                    e.printStackTrace();}}
            list.removeFirst();
            System.out.println("消费者" + Thread.currentThread().getName() + "消费，仓库容量为" + list.size());
            //唤醒其他生产者与消费者线程
            full.signal();
            empty.signal();
            lock.unlock();}}
    public static void main(String[] args) {
        ProAndCon2 proAndCon = new ProAndCon2();
        Producer producer = proAndCon.new Producer();
        Consumer consumer = proAndCon.new Consumer();
        for (int i = 0; i < 10; i++) {
            Thread pro = new Thread(producer);
            pro.start();
            Thread con = new Thread(consumer);
            con.start();}}}
```

方式三：
```java
public class ProAndCon3 {
    public static final int MAX_SIZE = 2;
    public static BlockingQueue<Integer> queue = new LinkedBlockingQueue<>(MAX_SIZE);
    class Producer implements Runnable {
        @Override
        public void run() {
            if (queue.size() == MAX_SIZE) {
                System.out.println("仓库已满，生产者" + Thread.currentThread().getName() + "不可生产.");
            }
            try {
                queue.put(1);
                System.out.println("生产者" + Thread.currentThread().getName() + "生产, 仓库容量为" + queue.size());
          } catch (InterruptedException e) {
                e.printStackTrace();}}}
    class Consumer implements Runnable {
        @Override
        public void run() {
            if (queue.size() == 0) {
                System.out.println("仓库为空，消费者" + Thread.currentThread().getName() + "不可消费.");
            }
            try {
                queue.take();
                System.out.println("消费者" + Thread.currentThread().getName() + "消费，仓库容量为" + queue.size());
 } catch (InterruptedException e) {
                e.printStackTrace();}}}
    public static void main(String[] args) {
        ProAndCon3 proAndCon = new ProAndCon3();
        Producer producer = proAndCon.new Producer();
        Consumer consumer = proAndCon.new Consumer();
 
        for (int i = 0; i < 10; i++) {
            Thread pro = new Thread(producer);
            pro.start();
            Thread con = new Thread(consumer);
            con.start();}}}
```

# 8.并发级别（并行和并发）

1. `阻塞`
一个线程是阻塞的，那么在其他线程释放资源之前，
当前线程无法继续执行。比如：使用synchronize关键字或者其他重入锁，
我们得到的就是阻塞的线程。无论是synchronized或者重入锁，
都会在试图执行后续代码前，得到临界区的锁，如果得不到，
线程就会被挂起等待，直到占有了所需资源为止。
2. `无饥饿（Starvation-Free）`
如果线程之间是有优先级的，那么线程调度的时候总是会倾向于满足高优先级的线程。
也就是说，对于同一个资源的分配，是不公平的。
锁也分公平锁和非公平锁，对于非公平锁来说，系统允许高优先级的线程插队。
这样就有可能导致低优先级的线程产生饥饿。
但是如果是公平锁，满足先来后到，那么饥饿就不会产生，
不管新来的线程优先级多高，要想获得资源，就必须乖乖排队。
这样所有的线程都有机会执行。
3. `无障碍（Obstruction-Free）`
无障碍是一种最弱的非阻塞调度。
两个线程如果是无障碍的执行，那么他们不会因为临界区的问题导致一方被挂起。
换言之，大家都可以大摇大摆的进入临界区了。
那么如果大家一起修改共享区数据，把数据修改坏了怎么办呢？对于无障碍的线程来说，
一旦检测到这种情况，它就会立即对自己所做的修改进行回滚，确保数据安全。
但是如果没有数据竞争发生，那么线程就可以顺利完成自己的工作，走出临界区。
从这个策略可以看出，无障碍的多线程程序不一定能顺畅的运行。
因为当临界区中存在严重的冲突时，所有的线程可能都会不断的回滚自己的操作，
导致没有一个线程能顺利走出临界区。这种情况会影响系统的正常执行。
一种可行的无障碍实现可以依赖一个“一致性标记”来实现：
线程在操作之前，先读取并保存这个标记，在操作完成之后，再次读取，
检查这个标记是否更改过，如果两者是一致的，则说明资源访问没有冲突。
如果不一致，则说明资源可能在操作过程中与其他线程存在冲突，
需要重新操作。而任何对资源有修改操作的线程，在修改数据前，
都需要更新这个一致性的标记，表示数据不再安全。
4. `无锁（Lock-Free）`
无锁的并行都是无障碍的。
在无锁的情况下，所有的线程都能尝试对临界区进行访问，
但不同的是，无锁的并发保证必然有一个线程能够在有限步内完成操作走出临界区。
在无锁的调用中，一个典型的特点是可能会包含一个无线循环。
在这个循环中，线程会不断尝试修改共享变量，如果没有冲突，修改成功，那么程序退出。
否则继续尝试修改，但无论如何，无锁的并行总能保证一个线程胜出，不会全军覆没。
至于临界区中竞争失败的线程，它们则必须不断重试，直到自己获胜，
如果运气不好，总是不成功，则会出现饥饿的现象，线程会停止不前。
5. `无等待（Wait-Free）`
无锁只要求一个线程可以在有限步数内完成操作，
而无等待则是在无锁的基础上更进一步进行扩展，
它要求所有的线程都必须在有限步数内完成，这样就不会引起线程饥饿问题。
一种典型的无等待结构是RCU（Read-Copy-Update）。
它的基本思想是，对数据的读可以不加控制。
因此，所有的读操作是无等待的，他们既不会被锁定等待也不会引起任何冲突。
但是在写数据的时候，先取得原始数据的副本，
接着只修改副本数据（这就是为什么读可不加控制），修改完成后，在合适的时机回写数据。

## 8-1：并行与并发概念

1. 并发： 同⼀时间段，多个任务都在执⾏；
2. 并⾏： 单位时间内，多个任务同时执⾏。

# 9.并发特性

1. 原子性 : 要么所有的操作都执行，要么都不执行。
2. 可见性 ：当一个变量对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。
3. 有序性 ：代码在执行的过程中的先后顺序，Java 在编译器以及运行期间的优化，
            代码的执行顺序未必就是编写代码时候的顺序。
            volatile 关键字可以禁止指令进行重排序优化。

# 5.多线程

## 5-1：为什么要使⽤多线程呢

1. 线程间的切换和调度的成本远远⼩于进程。减少了线程上下⽂切换的开销。
2. 现在的系统动不动就要求百万级甚⾄千万级的并发量，
   ⽽多线程并发编程正是开发⾼并发系统的基础，
   利⽤好多线程机制可以⼤⼤提⾼系统整体的并发能⼒以及性能。
3. 发挥多核 CPU 的优势
4. 防止阻塞

### 5-1-1：使⽤多线程可能带来什么问题?

并发编程的⽬的就是为了能提⾼程序的执⾏效率提⾼程序运⾏速度，
但是并发编程并不总是能提⾼程序运⾏速度的，⽽且并发编程可能会遇到很多题，
⽐如：内存泄漏、上下⽂切换、死锁还有受限于硬件和软件的资源闲置问题。

## 5-2：多线程公共用一个数据注意什么

1. 当我们在线程对象( Runnable )中定义了全局变量, run方法会修改该变量时,
   如果有多个线程同时使用刻线程对象, 
   那么就会造成全局变量的值被同时修改,造成错误
2. ThreadLocal是JDK引入的一种机制,它用于解决线程间共享变量,
   使用ThreadLocal声明的变量，
   即使在线程中属于全局变量,
   针对每个线程来讲,这个变量也是独立的。
3. volatile变量每次被线程访问时,
   都强迫线程从主内存中重读该变量的最新值,
   而当该变量发生修改变化时,也会强迫线程将最新的值刷新回主内存中。
   这样一来 ,不同的线程都能及时的看到该变量的最新值。

### 5-2-1：如何确保 N 个线程可以访问 N 个资源同时又不导致死锁？

1. 加锁顺序（线程按照一定的顺序加锁）
2. 加锁时限（线程尝试获取锁的时候加上一定的时限， 
            超过时限则放弃对该锁的请求，并释放自己占有的锁）
3. 死锁检测

### 5-2-2：单cpu上多线程效率和单线程比如何

单CPU来说（没有开启超线程），在同一时间只能执行一个线程，所以如果想实现多任务，
那么就只能每个进程或线程获得一个时间片，在某个时间片内，只能一个线程执行，
然后按照某种策略换其他线程执行。由于时间片很短，这样给用户的感觉是同时有好多线程在执行。
但是线程切换是有代价的，因此如果采用多进程，
那么就需要将线程所隶属的该进程所需要的内存进行切换，
这时间代价是很多的。而线程切换代价就很少，线程是可以共享内存的。
所以采用多线程在切换上花费的比多进程少得多。

# 6.线程的基本操作

## 6-1：说说 sleep() ⽅法和 wait() ⽅法区别和共同点?

1. sleep ⽅法没有释放锁，⽽ wait ⽅法释放了锁 。
2. 两者都可以暂停线程的执⾏。
3. Wait 通常被⽤于线程间交互/通信， sleep 通常被⽤于暂停执⾏。
4. wait() ⽅法被调⽤后，线程不会⾃动苏醒，
   需要别的线程调⽤同⼀个对象上的 notify() 或notifyAll() ⽅法。
   sleep() ⽅法执⾏完成后，线程会⾃动苏醒。或者可以使⽤ wait(longtimeout)超时后线程会⾃动苏醒。

## 6-2：yield join notify notifyAll

1. yield()方法是停止当前线程， 让同等优先权的线程或更高优先级的线程有执行的机会。
   如果没有的话， 那么 yield()方法将不会起作用， 并且由可执行状态后马上又被执行。
2. join 方法是用于在某一个线程的执行过程中调用另一个线程执行，
   等到被调用的线程执行结束后， 再继续执行当前线程。
   如： t.join();//主要用于等待 t 线程运行结束， 若无此句，main 则会执行完毕， 导致结果不可预测。
3. notify 方法只唤醒一个等待（对象的）
   线程并使该线程开始执行。 所以如果有多个线程等待一个对象，
   这个方法只会唤醒其中一个线程， 选择哪个线程取决于操作系统对多线程管理的实现。
4. notifyAll 会唤醒所有等待(对象的)线程， 尽管哪一个线程将会第一个处理取决于操作系统的实现


## 6-3：为什么我们调⽤ start() ⽅法时会执⾏ run() ⽅法，为什么我们不能直接调⽤run() ⽅法

new ⼀个 Thread，线程进⼊了新建状态;调⽤ start() ⽅法，
会启动⼀个线程并使线程进⼊了就绪状态，当分配到时间⽚后就可以开始运⾏了。
start() 会执⾏线程的相应准备⼯作，然后⾃动执⾏run() ⽅法的内容，
这是真正的多线程⼯作。 ⽽直接执⾏ run() ⽅法，
会把 run ⽅法当成⼀个 main线程下的普通⽅法去执⾏，
并不会在某个线程中执⾏它，所以这并不是多线程⼯作。

## 6-4：中断线程方法（36期）

1. 当 run 方法完成后线程终止。
2. 通过 return 退出 run 方法
3. 通过对有些状态中断抛异常退出thread.interrupt() 中断。
4. 使用 stop 方法强行终止线程（过期）

## 6-5：一般线程和守护线程以及两者区别

所谓守护线程是指在程序运行的时候在后台提供一种通用服务的线程，
比如垃圾回收线程就是一个很称职的守护者， 并且这种线程并不属于程序中不可或缺的部分。
因 此，当所有的非守护线程结束时， 程序也就终止了，
同时会杀死进程中的所有守护线程。
反过来说， 只要任何非守护线程还在运行， 程序就不会终止。

区别：
为守护线程是 JVM 自动创建的线程， 用户线程是程序创建的线程；
比如 JVM的垃圾回收线程是一个守护线程，
当所有线程已经撤离， 不再产生垃圾， 守护线程自然就没事可干了，
当垃圾回收线程是 Java 虚拟机上仅剩的线程时， Java 虚拟机会自动离开。

# 12.创建线程

## 12-1：创建线程的方式

1. `继承 Thread类创建线程`
  1）定义Thread类的子类，并重写该类的run方法，代表了线程要完成的任务
  2）创建了线程对象。
  3）调用线程对象的start()方法来启动该线程
2. `实现Runnable接口创建线程`
   1）定义runnable接口的实现类，
      并重写该接口的run()方法，
      该run()方法的方法体同样是该线程的线程执行体。
   2）创建 Runnable实现类的实例，
      并以此实例作为Thread的 target来创建Thread对象，
      该Thread对象才是真正的线程对象。
   3）调用线程对象的start()方法来启动该线程。
3. `使用Callable和Future创建线程`
   1）创建Callable接口的实现类，并实现call方法，call方法作为线程执行体
   2）创建Callable实现类的实例，使用FutureTask类来包装Callable对象，
      该FutureTask对象封装了该Callable对象的call（）方法的返回值
      FutureTask是一个包装器，它通过接受Callable来创建，
      它同时实现了Future和Runnable接口
   3）使用FutureTask作为对象
      作为Thread对象的target创建并启动新线程。
   4）调用FutureTask对象的get方法获得子线程执行结束后的返回值
4. `使用线程池`
   比如说用Executor框架

### 12-1-1：实现Runnable接⼝和Callable接⼝的区别

Runnable ⾃Java 1.0以来⼀直存在，
但 Callable 仅在Java 1.5中引⼊,⽬的就是为了来处理 Runnable不⽀持的⽤例。 
Runnable 接⼝不会返回结果或抛出检查异常，
但是 Callable 接⼝可以。
所以，如果任务不需要返回结果或抛出异常推荐使⽤ Runnable 接⼝，
这样代码看起来会更加简洁。

### 12-1-2：实现 Runnable 接口比继承 Thread 类所具有的优势

1. 适合多个相同的程序代码的线程去处理同一个资源 
2. 可以避免 java 中的单继承的限制 
3. 增加程序的健壮性， 代码可以被多个线程共享， 代码和数据独立
4. 线程池只能放入实现 Runable 或 callable 类线程， 不能直接放入继承 Thread 的类 
5. runnable 实现线程可以对线程进行复用， 因为 runnable 是轻量级的对象， 
   重复 new 不会耗费太大资源， 而 Thread 则不然， 
   它是重量级对象， 而且线程执行完就完了， 无法再次利用

### 12-1-3：run()方法和start()方法的区别

线程的run()方法是由java虚拟机直接调用的，
如果我们没有启动线程（没有调用线程的start()方法）而是在应用代码中直接调用run()方法，
那么这个线程的run()方法其实运行在当前线程（即run()方法的调用方所在的线程）之中，
而不是运行在其自身的线程中，从而违背了创建线程的初衷；

## 12-2：创建线程的对比

创建线程的方式的对比
1)采用实现Runnable、Callable接口的方式创建多线程时，线程类只是实现了Runnable接口或Callable接口，还可以继承其他类。
  在这种方式下，多个线程可以共享同一个 target对象，
  所以非常适合多个相同线程来处理同一份资源的情况，从而可以将CPU、代码和数据分开。
  但是，缺点是编程稍微复杂，如果要访问当前线程，则必须使用 Thread.currentThread)方法。
2）使用继承Thread类的方式创建多线程时，
   如果需要访问当前线程，则无需使用Thread.currentThread()方法，直接使用this即可获得当前线程。
   缺点是线程类已经继承了Thread类，所以不能再继承其他父类。
3）Runnable和Callable的区别
  1) Callable规定（重写）的方法是call()，Runnable规定（重写）的方法是run()。
  2) Callable的任务执行后可返回值，而Runnable的任务是不能返回值的。
  3)call方法可以抛出异常，run方法不可以。
  4)运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，
    并检索计算的结果。通过Future对象可以了解任务执行情况，可取准任务的执行，还可获取执行结果。

# 13.线程安全

## 13-1：什么是线程安全

如果你的代码所在的进程中有多个线程在同时运行，
而这些线程可能会同时运行这段代码。
如果每次运行结果和单线程运行的结果是一样的，
而且其他的变量的值也和预期的是一样的，就是线程安全的。

线程安全问题都是由全局变量及静态变量引起的。
若每个线程中对全局变量、静态变量只有读操作，
而无写操作，一般来说，这个全局变量是线程安全的；
若有多个线程同时执行写操作，
一般都需要考虑线程同步，否则的话就可能影响线程安全。

### 13-1-1：举例说明线程不安全

比如一个 ArrayList 类，在添加一个元素的时候，
它可能会有两步来完成：
1. 在 Items[Size] 的位置存放此元素；
2. 增大 Size 的值。
在单线程运行的情况下，如果 Size = 0，
添加一个元素后，此元素在位置 0，而且 Size=1；
而如果是在多线程情况下，比如有两个线程，
线程 A 先将元素存放在位置 0。但是此时 CPU 调度线程A暂停，
线程 B 得到运行的机会。线程B也向此 ArrayList 添加元素，
因为此时 Size 仍然等于 0 
我们假设的是添加一个元素是要两个步骤哦，而线程A仅仅完成了步骤1，
所以线程B也将元素存放在位置0。然后线程A和线程B都继续运行，
都增加 Size 的值。
那么，元素实际上只有一个，存放在位置 0，
而 Size 却等于 2。这就是“线程不安全”了。

## 13-2：线程安全的方式

1. synchronized关键字
2. volatile实现同步
 　　1）使用volatile关键字会强制将修改的缓存值立即写入主存。
　　 2）使用volatile关键字，当线程2进行修改时，会导致线程1的工作内存中变量缓存无效，
        然后线程1读取时发现自己的缓存无效他会等待缓存行对应的主存地址被更新之后，
        然后去主存读取最新信息。
　　 3）禁止指令重排序
　　　　　　（1）当程序执行到volatile变量的读操作或者写操作时，
                其前面的操作的更改肯定全部已经进行，
                且结果已经对后面的操作可见，在其后面的操作还没有进行。
　　　　　　（2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，
                也不能把volatile变量后面的语句放在其前面执行。
3. ThreadLocal管理变量
   ThreadLocal的作用是提供线程内的局部变量，
   这种变量在线程的生命周期内起作用，
   减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。
4. 原子类
  原子类是基本类的原子化版本，通过线程安全的方式操作，等同于自动加synchronized
5. 使用Lock，读写锁
   lock更灵活，可以自由定义多把锁的枷锁解锁顺序
   （synchronized要按照先加的后解顺序）
   提供多种加锁方案，lock 阻塞式, trylock 无阻塞式, 
   lockInterruptily 可打断式， 还有trylock的带超时时间版本。
   本质上和监视器锁（即synchronized）是一样的
6. 容器类（BlockingQueue、ConcurrentHashMap）

# 14.synchronized关键字

## 10-1：synchronized关键字

1. 解决的是多个线程之间访问资源的同步性，
   synchronized关键字可以保证
   被它修饰的⽅法或者代码块在任意时刻只能有⼀个线程执⾏。
2. 在Java早期版本中， synchronized属于重量级锁，效率低下，
   因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的， 
   Java 的线程是映射到操作系统的原⽣线程之上的。
   如果要挂起或者唤醒⼀个线程，都需要操作系统帮忙完成，
   ⽽操作系统实现线程之间的切换时需要从⽤户态转换到内核态，
   这个状态之间的转换需要相对⽐较⻓的时间，时间成本相对较⾼，
   但是在JDK1.6对锁的实现引⼊了⼤量的优化，
   如⾃旋锁、适应性⾃旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。

## 10-2：JDK1.6优化有哪些？

JDK1.6 对锁的实现引⼊了⼤量的优化，
如偏向锁、轻量级锁、⾃旋锁、适应性⾃旋锁、
锁消除、锁粗化等技术来减少锁操作的开销。
1. 偏向锁
引入偏向锁的目是为了没有多线程竞争的前提下，
减少传统的重量级锁使用操作系统互斥量产生的性能消耗。
但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替
使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉。
2. 轻量级锁
轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，
减少传统的重量级锁使用操作系统互斥量产生的性能消耗，
因为使用轻量级锁时，不需要申请互斥量。
如果没有竞争，轻量级锁使用CAS操作避免了使用互斥操作的开销。但如果存在锁竞争，
除了互斥量开销外，还会额外发生CAS操作，因此在有锁竞争的情况下，
轻量级锁比传统的重量级锁更慢！
如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！
3. 自旋锁和自适应自旋
一般线程持有锁的时间都不是太长，所以仅仅为了这一点时间去挂起线程/恢复线程是得不偿失的。
为了让一个线程等待，我们只需要让线程执行一个忙循环（自旋）也就是自旋。
另外,在 JDK1.6 中引入了自适应的自旋锁。自适应的自旋锁带来的改进就是：
自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定。
4. 锁消除
就是把锁干掉。当Java虚拟机运行时发现有些共享数据不会被线程竞争时就可以进行锁消除。
5. 锁粗化
如果一系列的连续操作都对同一个对象反复加锁和解锁，
甚至加锁操作都是出现在循环体体之中，就算真的没有线程竞争，
频繁地进行互斥同步操作将会导致不必要的性能
损耗，所以就采取了一种方案：把加锁的范围扩展（粗化）到整个操作序列的外部，
这样加锁解锁的频率就会大大降低，从而减少了性能损耗。
   
## 10-3：底层原理

每个对象都有个 monitor 对象， 加锁就是在竞争 monitor 对象，
代码块加锁是在代码块前后分别加上 monitorenter 和 monitorexit 指令来实现的，
方法加锁是通过一个标记位来判断的。

### 10-3-1：monitor基本原理

monitor是一种syncronized的构造，其允许线程相互之间排斥和协作；
排斥同竞争锁，协作如生产和消费模式（wait,notify模式）。
当一个线程进入monitor是有三个时期；
1. 进入monitor，此时是被分配在entry set 中，等待lock的拥有者释放锁；
2. 当线程获得lock就会成为lock的拥有者（仅有一个线程可以拥有），此时处于 the owner 区域；
3. 当释放lock就会进入wait set ，但是处于wait set 中的线程还是有机会获得lock的拥有权。

每一个Java对象都可以与一个监视器monitor关联，
把它理解成为一把锁，
当一个线程想要执行一段被synchronized圈起来的同步方法或者代码块时，
该线程得先获取到synchronized修饰的对象对应的monitor。
monitor并不是随着对象创建而创建的。
通过synchronized修饰符告诉JVM需要为我们的某个对象创建关联的monitor对象。
每个线程都存在两个ObjectMonitor对象列表，
分别为free和used列表。 同时JVM中也维护着global locklist。
当线程需要ObjectMonitor对象时，
首先从线程自身的free表中申请，若存在则使用若不存在则从global list中申请。

## 10-4：synchronized的优势

1. 只需要基础的同步功能时，用synchronized。
2. Lock应该确保在finally块中释放锁。如果使用synchronized，JVM确保即使出现异常，锁也能被自动释放。
3. 使用Lock时，Java虚拟机很难得知哪些锁对象是由特定线程锁持有的。

## 10-5：synchronized锁的膨胀（升级）过程

1. 整个膨胀过程在自旋下完成；
2. mark->has_monitor()方法判断当前是否为重量级锁，
   即Mark Word的锁标识位为 10，如果当前状态为重量级锁，执行步骤（3），否则执行步骤（4）；
3. mark->monitor()方法获取指向ObjectMonitor的指针，并返回，说明膨胀过程已经完成；
4. 如果当前锁处于膨胀中，说明该锁正在被其它线程执行膨胀操作，
   则当前线程就进行自旋等待锁膨胀完成，这里需要注意一点，
   虽然是自旋操作，但不会一直占用cpu资源，
   每隔一段时间会通过os::NakedYield方法放弃cpu资源，
   或通过park方法挂起；如果其他线程完成锁的膨胀操作，则退出自旋并返回；
5. 如果当前是轻量级锁状态，即锁标识位为 00，膨胀过程如下：
    通过omAlloc方法，获取一个可用的ObjectMonitor monitor，并重置monitor数据；
    通过CAS尝试将Mark Word设置为markOopDesc:INFLATING，标识当前锁正在膨胀中，
    如果CAS失败，说明同一时刻其它线程已经将Mark Word设置为
    markOopDesc:INFLATING，当前线程进行自旋等待膨胀完成；
    如果CAS成功，设置monitor的各个字段：_header、_owner和_object等，并返回；
6. 如果是无锁，重置监视器值；


## 10-6：那如何判断共享数据不会被线程竞争？

利用逃逸分析技术：分析对象的作用域，如果对象在A方法中定义后，被作为参数传递到B方法中，
则称为方法逃逸；如果被其他线程访问，则称为线程逃逸。
在堆上的某个数据不会逃逸出去被其他线程访问到，
就可以把它当作栈上数据对待，认为它是线程私有的，同步加锁就不需要了。

## 10-7：synchronized将线程的并行处理转为串行处理，有什么缺点

synchronized将并行改为串行，当然会影响程序的执行效率，
执行速度会受到影响。
其次，synchronized操作线程的堵塞，
也就是由操作系统控制CPU的内核进行上下文的切换，
这个切换本身也是耗时的。
所以使用synchronized关键字会降低程序的运行效率。

## 10-8：使用Synchronized关键字需要注意什么

1.Synchronized使用时需要注意的地方锁对象不能为空。
  锁对象的信息是保留在对象头中的，如果对象为空，则锁的信息也就不存在了。
2.作用域不宜过大
  synchronized代码块的代码量不宜过多，如果把过多的代码放在其中，
  程序的运行会变为串行，速度会下降。各个线程并行可以提高效率，
  我们应该仅把那些影响线程安全的代码，
  放入synchronized代码块中，串行执行；
  不需要考虑线程安全的代码，并行执行，达到效率最高。
3.避免死锁
  避免让线程对锁持有并等待的情况出现。

## 10-9：synchronized在内存层面，是如何实现加锁和释放锁的？

进入synchronized代码块时，会将代码块内用到的变量从该线程的工作内存中清除，转而从主内存中获取。

退出synchronized代码块时，会将代码块内用到的变量的修改，刷新到主内存中。

## 10-10：synchronized关键字，是怎么保证线程安全的呢？

在Java中所有的对象，都会有一把锁，
叫做内置锁，也称作监视器锁。这是种排他锁。
排他锁：一个线程获取后，其他线程只能等待其释放后，
才有机会获得该锁。
Java中每个对象，都可以把内置锁，
当做一个同步锁来使用。当一个线程在进入到synchronized代码块前时，
会自动获取到监视器锁，此时其他线程在访问synchronized代码块时，
就会被堵塞挂起。
拿到锁的线程会在执行完成、或者抛出异常、或者调用wait系列方法时释放该锁。
其他线程只能等待锁被释放后才能获取该锁。
synchronized关键字将代码块中代码由并行转变成了串行，这样就保证了代码被顺序执行。

# 15.volatile关键字

## 15-1：基本原理

1. 缓存一致性协议
   缓存一致性可以通过Store Buffer和Invalidate Queue两种结构进行加速，
   但这两种方式会造成一系列不一致性的问题。因此提出了内存屏障的概念，
   分为读屏障和写屏障，以此修正Store Buffer和Invalidate Queu产生的问题。

2. 通过读屏障和写屏障，又发展出了LoadLoad屏障，StoreStore屏障，LoadStore屏障，StoreLoad屏障
   JVM也是利用了这几种屏障，实现volatile关键字。

### 15-1-1：缓存一致性协议

主要是通过缓存一致性协议，
主要是由于cpu核心（Kernel）读取内存数据较慢，
于是就出现了缓存的概念。也是希望针对频繁读写的某个内存变量，
提升本核心的访问速率。
因此我们会给每个核心设计缓存区(Cache)，缓存该变量。
由于缓存硬件的读写速度比内存快，所以通过这种方式可以提升变量访问速度。
但是，如果涉及到并发任务，多个核心读取同一个变量值，
由于每个核心读取的是自己那一部分的缓存，
每个核心的缓存数据不一致将会导致一系列问题。
缓存一致性的问题根源就在于，对于某个变量，
好几个核心对应的缓存区都有，到底哪个是新的数据呢？
如果只有一个CPU核心对应的缓存区有该变量，那这个缓存肯定是新的。

`所以为了保证缓存的一致性，业界有两种思路：`

1. 写失效(Write Invalidate)：当一个核心修改了一份数据，其它核心如果有这份数据，就把valid标识为无效；
2. 写更新(Write update)：当一个核心修改了一份数据，其它核心如果有这份数据，就都更新为新值，并且还是标记valid有效。
其中MESI协议是目前实现较为流行的实现缓存一致性的协议。

MESI协议，原先的valid是一个比特位，代表有效/无效两种状态。
在MESI协议中，该位改成两位，不再只是有效和无效两种状态，而是有四个状态：

1. M（Modified）：表示核心的数据被修改了，缓存数据属于有效状态，
   但是数据只处于本核心对应的缓存，还没有将这个新数据写到内存中。
   由于此时数据在各个核心缓存区只有唯一一份，不涉及缓存一致性问题；
2. E（Exclusive）：表示数据只存在本核心对应的缓存中，
   别的核心缓存没这个数据，缓存数据属于有效状态，
   并且该缓存中的最新数据已经写到内存中了。
   同样由于此时数据在各个核心缓存区只有一份，
   也不涉及缓存一致性问题；
3. S（Shared）：表示数据存于多个核心对应的缓存中，
   缓存数据属于有效状态，和内存一致。这种状态的值涉及缓存一致性问题；
4. I（Invalid）：表示该核心对应的缓存数据无效。

为了保证缓存一致性，每个核心要写新数据前，
需要确保其他核心已经置同一变量数据的缓存行状态位为Invalid后，
再把新数据写到自己的缓存行，并之后写到内存中。

### 15-1-2：内存屏障

内存屏障我认为就是利用一行命令，规定了某个针对缓存的操作

#### 15-1-2-1：volatile中的内存屏障

针对volatile变量，JVM采用的内存屏障是：

1. 针对volatile修饰变量的写操作：在写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障；
2. 针对volatile修饰变量的读操作：在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障；

通过这种方式，就可以保证被volatile修饰的变量具有线程间的可见性和禁止指令重排序的功能了。

## 15-2：为什么要是用volatile关键字

目前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，
而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，
而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。
要解决这个问题，就需要把变量声明为volatile，
这就指示JVM，这个变量是不稳定的，每次使用它都到主存中进行读取。
volatile 关键字的主要作用就是保证变量的可见性然后还有一个作用是防止指令重排序。

## 15-3：volatile的作用

volatile的作用就是当一个线程更新某个volatile声明的变量时，
会通知其他的cpu使缓存失效，从而其他cpu想要做更新操作时，
需要从内存重新读取数据

## 15-4：原子性

### 15-4-1：volatile为什么不保证原子性吗

比如说，当20个线程同时给number自增1，执行1000次以后，
单线程情况下，肯定是20000，但是在多线程情况下，
执行了某个指令number的值取到操作栈顶时，volatile关
键字保证了number的值在此时是正确的，
但是在执行压栈等指令的时候，
其他线程可能已经把number的值改变了，
而操作栈顶的值就变成了过期的数据，
就可能把较小的number值同步回主内存之中。

### 15-4-2：怎么保证输出结果是20000呢

1. synchronized同步代码块
   我们可以通过使用synchronized同步代码块来保证原子性。
   但是使用synchronized太重了，会造成阻塞，只有一个线程能进入到这个方法。
   我们可以使用Java并发包（JUC）中的AtomicInterger工具包。
2. AtomicInterger原子性操作
   两个线程，线程1和线程2都有主内存中变量的拷贝，值都等于10
   线程1想要将值更新为20，先要将工作内存中的变量值与主内存中的变量进行比较，
   值都等于10，所以可以将主内存中的值替换成20
   线程1将主内存中的值替换成20，并将线程1中的工作内存中的副本更新为20
   线程2想要将变量更新为30，先要将线程2的工作内存中的值与主内存进行比较10不等于20，
   所以不能更新线程2将工作内存的副本更新为与主内存一致：20

### 15-5-3：volatile都不保证原子性，为啥我们还要用它？

1. volatile是轻量级的同步机制，对性能的影响比synchronized小。
  * 比如线程试图通过类似于数绵羊的传统方法进入休眠状态，为了使这个示例能正确执行，
    sleep必须为volatile变量。否则，当asleep被另一个线程修改时，执行判断的线程却发现不了。
2. 因为synchorized和lock是排他锁（悲观锁），如果有多个线程需要访问这个变量，
   将会发生竞争，只有一个线程可以访问这个变量，其他线程被阻塞了，会影响程序的性能。

## 15-6：安全性

volatile不是保护线程安全的。
它保护的是变量安全。
主要的功能是保护变量不被主函数和中断函数反复修改造成读写错误。

## 15-7：重排

### 15-7-1：为什么其他线程能感知到变量更新

当多个CPU持有的缓存都来自同一个主内存的拷贝，当有其他CPU偷偷改了这个主内存数据后，
其他CPU并不知道，那拷贝的内存将会和主内存不一致，
那我们为了保证缓存一致，这里就需要操作系统来共同制定一个同步规则来保证，
而这个规则就有MESI协议。
当CPU写数据时，如果发现操作的变量是共享变量，即在其它CPU中也存在该变量的副本，
系统会发出信号通知其它CPU将该内存变量的缓存行设置为无效。
当其它CPU读取这个变量的时，发现自己缓存该变量的缓存行是无效的，那么它就会从内存中重新读取。
为了让其他CPU是怎么知道要将缓存更新为失效的，这里是用到了总线嗅探技术。
每个CPU不断嗅探总线上传播的数据来检查自己缓存值是否过期了，
如果处理器发现自己的缓存行对应的内存地址被修改，
就会将当前处理器的缓存行设置为无效状态，
当处理器对这个数据进行修改操作的时候，
会重新从内存中把数据读取到处理器缓存中。

### 15-7-2：为什么要重排

计算机在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。

### 15-7-3： 有哪几种重排

1. 编译器优化重排：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
2. 指令级的并行重排：现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，
                     处理器可以改变语句对应机器指令的执行顺序。
3. 内存系统的重排：由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。

### 15-7-4：举例说一下指令重排

定义了变量num=0和变量flag=false，线程1调用初始化函数init()执行后，线程调用add()方法，
当另外线程判断flag=true后，执行num+100操作，那么我们预期的结果是num会等于101，
但因为有指令重排的可能，num=1和flag=true执行顺序可能会颠倒，以至于num可能等于100

## 15-8：happen-before原则是什么

定义了哪些指令不能重排。
八大原则:
1. 单线程happen-before原则:在同一个线程中，书写在前面的操作happen-before后面的操作。
2. 锁的happen-before原则:同一个锁的unlock操作happen-before此锁的lock操作。
3. volatile的happen-before原则:对一个volatile变量的写操作happen-before对此变量的任意操作(当然也包
   括写操作了)。
4. happen-before的传递性原则:如果A操作happen-before B 操作，B操作 happen-before C操作，那么A操作
   happen-beforeC操作。
5. 线程启动的happen-before原则:同一个线程的start方法 happen-before此线程的其它方法。
6. 线程中断的happen-before原则:对线程interrupt方法的调用happen-before被中断线程的检测到中断发送的
   代码。
7. 线程终结的happen-before原则:线程中的所有操作都 happen-before线程的终止检测。
8. 对象创建的happen-before原则:一个对象的初始化完成先于他的finalize方法调用。

## 15-9：volatile的典型应用场景

1. volatile用于保障 long/double 型变量的读、写操作的原子性
2. 使用volatile变量作为状态标志。在这种情况中，应用程序的某个状态由一个线程设置，
   其他线程会读取该状态并以该状态作为其计算的依据
   或者仅仅读取并输出这个状态值。
   此时使用 volatile变量作为同步机制的好处是一个线程能够 
   “通知” 另外一个线程某种事件例如，网络连接断连之后重新连上的发生，
   而这些线程又无须因此而使用锁，从而避免了锁的开销以及相关问题。
3. 使用 volatile  保障可见性。在这种情况中，多个线程共享一个可变状态变量 ，
   其中一个线程更新了该变量之后。其他线程在元须加锁的情况下也能够看到该更新。
4. 使用 volatile变量替代锁。volatile 关键字并非锁的替代品，
   但是在一定的条件下它比锁更合适 （ 性能开销小 、代码简单 ）
   多个线程共享一组可变状态变量的时候，
   通常我们需要使用锁来保障对这些变量的更新操作的原子性，
   以避免产生数据不一致问题。
   利用 volatile 变量写操作具有的原子性 ，
   我们可以把这一组可变状态变量封装成一个对象，
   那么对这些状态变量的更新操作就可以通过
   创建一个新的对象并将该对象引用赋值给相应的引用型变量来实现。
   在这个过程中， volatile 保障了原子性和可见性。从而避免了锁的使用。


# 16.synchronized与其他的区别

## 16-1：谈谈 synchronized和ReentrantLock 的区别

1. 两者都是可重入锁
   自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，
   此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的。
2. synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API
3. ReentrantLock提供了一种能够中断等待锁的线程的机制，
   通过lock.lockInterruptibly()来实现这个机制。
   也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。
4. ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。
   ReentrantLock默认情况是非公平的，
   可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。
5. synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类借助于
   Condition接口与newCondition() 方法。 在使用notify()/notifyAll()方法进行通知时，
   被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，

## 16-2：Lock和synchronized的区别

1. Lock需要手动获取锁和释放锁。
2. Lock 是一个接口，而 synchronized 是 Java 中的关键字， synchronized 是内置的语言实现。
3. synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；
   而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，
   因此使用 Lock 时需要在 finally 块中释放锁。
4. Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，
   等待的线程会一直等待下去，不能够响应中断。
5. 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。
6. Lock 可以通过实现读写锁提高多个线程进行读操作的效率。

## 16-3：synchronized 关键字和 volatile 关键字的区别

1. volatile关键字是线程同步的轻量级实现，
   但是volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块。
   synchronized关键字在1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的
   偏向锁和轻量级锁以及其它各种优化之后执行效率有了显著提升，
   实际开发中使用 synchronized 关键字的场景还是更多一些。
2. 多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞
3. volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。
4. volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized关键字解决的是多个线程之间访问资源的同步性。

# 17.线程池--死锁

## 17-1：使⽤线程池的好处

1. 降低资源消耗。通过重复利⽤已创建的线程降低线程创建和销毁造成的消耗。
2. 提⾼响应速度。当任务到达时，任务可以不需要的等到线程创建就能⽴即执⾏。
3. 提⾼线程的可管理性。线程是稀缺资源，如果⽆限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使⽤线
   程池可以进⾏统⼀的分配，调优和监控。

## 17-2：常规实现线程池方法

（1） `newFixedThreadPool定长线程池`
   它是一种固定大小的线程池;
   corePoolSize和maximunPoolSize都为用户设定的线程数量nThreads:
   keepAliveTime为0，意味着一旦有多余的空闲线程，
   就会被立即停止掉;但这里keepAliveTime无效;
   阻塞队列采用了LinkedBlockingQueue，
   它是一个无界队列;由于阻塞队列是一个无界队列，
   因此永远不可能拒绝任务;由于采用了无界队列，
   实际线程数量将永远维持在 nThreads，
   因此 maximumPoolSize和keepAliveTime将无效。
(2） `newCachedThreadPool可缓存`
   它是一个可以无限扩大的线程池;
   它比较适合处理执行时间比较小的任务;corePoolSize为O，
   maximumPoolSize为无限大，意味着线程数量可以无限大;
   keepAliveTime为 60S，
   意味着线程空闲时间超过60S就会被杀死;
   采用SynchronousOueue装等待的任务，
   这个阻塞队列没有存储空间，
   这意味着只要有请求到来，
   就必须要找到一条工作线程处理他，
   如果当前没有空闲的线程，
   那么就会再创建一条新的线程。
(3） `newSingleThreadExecutor单一线程池`
     它只会创建一条工作线程处理任务;采用的阻塞队列为LinkedBlockingQueue
(4）`ScheduledThreadPool可调度的线程池`
     实现周期性线程调度，比较常用。

## 17-3：线程池增长策略

当一个任务通过execute(Runnable)方法欲添加到线程池时:
1、如果此时线程池中的数量`小于`corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处 
   理被添加的任务。
2、如果此时线程池中的数量等于corePoolSize，但是缓冲队列workQueue未满，那么任务被放入缓冲队列。
3、如果此时线程池中的数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量小于
    maximumPoolSize，建新的线程来处理被添加的任务。
4、如果此时线程池中的数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量等于     
   maximumPoolSize，那么通过handler所指定的策略来处理此任务。也就是:处理任务的优先级为:核心线程
   corePooSize、最大线程maximumPoolSize、任务队列workQueue
   如果三者都满了，使用handler处理被拒绝的任务。当线程池中的线程数大于corePooSize，如果某线程空闲
   时间超过keepAliveTime，线程将被终止。这样，线程池可以动态的调整池中的线程数。且体增长策略要看你
   使用什么workQueue。

### 17-3-1：线程池的核心线程数和最大线程数

核心线程数就像是本身所具有的，最大线程数，就是工厂临时工作量加大，
临时进行了申请，临时加正式的和就是最大线程数，
等这批任务结束后，临时的要辞退的，而正式的留下。

## 17-4：线程池拒绝策略

1、`AbortPolicy`:这种策略直接抛出异常，丢弃任务（当都满了）
2、`CallerRunsPolicy`:策略显然个想放弁执行任务。但是由于池中已经没有任何资源了，那么就直接使用调用该
                    execute 的线程本身来执行。很有可能造成当前线程也被阻塞。
3、`DiscardPolicy`:不能执行的任务将被删除，这种策略和AbortPolicy几乎一样，也是丢弃任务，只不过他不
                  抛出异常。
4、`DiscardOldestPolicy`:如果执行程序尚未关闭，则位于工作队列头部的任务将被删除，然后重试执行程序
                      （如果再次失败，则重复此过程）

## 17-5：线程池参数

1. `corePoolSize 核心线程大小`
   线程池中最小的线程数量，即使处理空闲状态，也不会被销毁，除非设置了allowCoreThreadTimeOut。
   CPU密集型：核心线程数 = CPU核数 + 1
   IO密集型：核心线程数 = CPU核数 * 2+1
   注：IO密集型（某大厂实践经验）
   核心线程数 = CPU核数 / （1-阻塞系数）
   例如阻塞系数 0.8，CPU核数为4，则核心线程数为20
2. `maximumPoolSize 线程池最大线程数量`
   一个任务被提交后，首先会被缓存到工作队列中，
   等工作队列满了，则会创建一个新线程，
   处理从工作队列中的取出一个任务。
3. `keepAliveTime 空闲线程存活时间`
   当线程数量大于corePoolSize时，一个处于空闲状态的线程，在指定的时间后会被销毁。
4. `unit 空间线程存活时间单位`
   keepAliveTime的计量单位
5. `workQueue 工作队列，`jdk中提供了四种工作队列
   新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。
   ①ArrayBlockingQueue
   基于数组的有界阻塞队列，按FIFO排序。
   ②LinkedBlockingQuene
   基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。
   ④PriorityBlockingQueue
   具有优先级的无界阻塞队列，优先级通过参数Comparator实现。
6. `threadFactory 线程工厂`
   创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等
7. `handler 拒绝策略`
   当工作队列中的任务已满并且线程池中的线程数量也达到最大，
   这时如果有新任务提交进来，拒绝策略就是解决这个问题的，
   jdk中提供了4中拒绝策略：
      ①CallerRunsPolicy
      该策略下，在调用者线程中直接执行被拒绝任务的run方法，
      除非线程池已经shutdown，则直接抛弃任务。
      ②AbortPolicy
      该策略下，直接丢弃任务，
      并抛出RejectedExecutionException异常。
      ③DiscardPolicy
      该策略下，直接丢弃任务，什么都不做。
      ④DiscardOldestPolicy
      该策略下，抛弃最早进入队列的那个任务，
      然后尝试把这次拒绝的任务放入队列。

## 17-6：线程池处理流程

第一步 ：线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。
        如果核心线程池里的线程都在执行任务，则执行第二步。
第二步 ：线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里进行等待。
         如果工作队列满了，则执行第三步。
第三步 ：线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。
        如果已经满了，则交给饱和策略来处理这个任务。
即任务处理优先级：核心线程池的线程 > 工作队列 > 线程池的线程 > 饱和策略

## 17-7：设计线程池

### 17-7-1：如何设计一个线程池

修改


## 17-8：死亡问题

### 17-8-1：单一线程池中线程死亡该怎么办

每个线程都有个计时器，发现线程卡死直接杀死掉，
最好是及时移除超时任务，重启线程池

### 17-8-2：如何防止线程池线程死掉

比较简单的就是利用线程池，线程死掉后，会自动再创建线程。
如果是主线程的话，就用一个监视线程来管理，如果主线程死掉，
通知监视线程，监视线程再创建一个线程。如果监视线程死掉，那就彻底挂了。
心跳机制，线程每隔一段时间往另一服务器进程发送数据包，
如果服务器进程长时间没有到心跳包，则说明当前线程已经死机！

## 17-9：线程池的风险

用线程池构建的应用程序容易
遭受任何其它多线程应用程序容易遭受的所有并发风险，
诸如同步错误和死锁，
它还容易遭受特定于线程池的少数其它风险，
诸如与池有关的死锁、资源不足和线程泄漏。
1. 死锁
   虽然任何多线程程序中都有死锁的风险，
   但线程池却引入了另一种死锁可能，在那种情况下，
   所有池线程都在执行已阻塞的等待队列中另一任务的执行结果的任务，
   但这一任务却因为没有未被占用的线程而不能运行。
   当线程池被用来实现涉及许多交互对象的模拟，
   被模拟的对象可以相互发送查询，
   这些查询接下来作为排队的任务执行，
   查询对象又同步等待着响应时，会发生这种情况。
2. 资源不足
   如果线程池太大，那么被那些线程消耗的资源可能严重地影响系统性能。
   在线程之间进行切换将会浪费时间，
   而且使用超出比您实际需要的线程可能会引起资源匮乏问题，
   因为池线程正在消耗一些资源，
   而这些资源可能会被其它任务更有效地利用。
   除了线程自身所使用的资源以外，
   服务请求时所做的工作可能需要其它资源，
   例如 JDBC 连接、套接字或文件。
   这些也都是有限资源，
   有太多的并发请求也可能引起失效，例如不能分配 JDBC 连接。
3. 线程泄漏
   各种类型的线程池中一个严重的风险是线程泄漏，
   当从池中除去一个线程以执行一项任务，
   而在任务完成后该线程却没有返回池时，
   会发生这种情况。
   发生线程泄漏的一种情形出现在任务抛出一个
   RuntimeException 或一个 Error 时。
   如果池类没有捕捉到它们，
   那么线程只会退出而线程池的大小
   将会永久减少一个。
   当这种情况发生的次数足够多时，
   线程池最终就为空，
   而且系统将停止，
   因为没有可用的线程来处理任务。

### 17-9-1：死锁

#### 17-9-1-1：什么是线程死锁

多个线程同时被阻塞，它们中的⼀个或者全部都在等待某个资源被释放。
由于线程被⽆限期地阻塞，因此程序不可能正常终⽌。

#### 17-9-1-2：产生死锁的条件

1. 该资源任意⼀个时刻只由⼀个线程占⽤。
2. ⼀个进程因请求资源⽽阻塞时，对已获得的资源保持不放。
3. 线程已获得的资源在末使⽤完之前不能被其他线程强⾏剥夺，只有⾃⼰使⽤完毕后才释放资源。
4. 若⼲进程之间形成⼀种头尾相接的循环等待资源关系。

#### 17-9-1-3：如何解决线程死锁问题

1. ⼀次性申请所有的资源。
2. 占⽤部分资源的线程进⼀步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
3. 靠按序申请资源来预防。按某⼀顺序申请资源，释放资源则反序释放。破坏循环等待条件。

## 17-10：线程池单例问题

首先在ThreadPool类里面实现线程池的创建，
我们这里创建的是FixedThreadPool线程池
```java
private ThreadPool(int corepoolsize, int maximumpoolsize, long keepalivetime){
            this.corepoolsize = corepoolsize;
            this.maximumpoolsize = maximumpoolsize;
            this.keepalivetime = keepalivetime;
        }
public void executor(Runnable runnable){

            if (runnable == null){
                return;
            }
            if (mexecutor == null){
                mexecutor = new ThreadPoolExecutor(corepoolsize, //核心线程数
                        maximumpoolsize, //最大线程数
                        keepalivetime, //闲置线程存活时间
                        TimeUnit.MILLISECONDS, // 时间单位
                        new LinkedBlockingDeque<Runnable>(), //线程队列
                        Executors.defaultThreadFactory(), //线程工厂
                        new ThreadPoolExecutor.AbortPolicy() //队列已满，而且当前线程数已经超过最大线程数时的异常处理策略
                );
            }
            mexecutor.execute(runnable);
        }
```
再然后对ThreadPool内部类，在类里面对他实例化，实现单例
```java
 // 获取单例的线程池对象
    public static ThreadPool getThreadPool() {
        if (mThreadPool == null) {
            synchronized (ThreadManager.class) {
                if (mThreadPool == null) {
                    int cpuNum = Runtime.getRuntime().availableProcessors();// 获取处理器数量
                    int threadNum = cpuNum * 2 + 1;// 根据cpu数量,计算出合理的线程并发数
                    mThreadPool = new ThreadPool(threadNum, threadNum, 0L);
                }
            }
        }
        return mThreadPool;
    }
```
# 18.锁

## 18-1：锁

| 序号 | 锁名称  | 应用                                                              |
|----|------|-----------------------------------------------------------------|
| 1  | 乐观锁  | CAS                                                             |
| 2  | 悲观锁  | synchronized、vector、hashtable                                   |
| 3  | 自旋锁  | CAS                                                             |
| 4  | 可重入锁 | synchronized、Reentrantlock、Lock                                 |
| 5  | 读写锁  | ReentrantReadWriteLock，CopyOnWriteArrayList、CopyOnWriteArraySet |
| 6  | 公平锁  | Reentrantlock(true)                                           |
| 7  | 非公平锁 | synchronized、reentrantlock(false)                             |
| 8  | 共享锁  | ReentrantReadWriteLock中读锁                                       |
| 9  | 独占锁  | synchronized、vector、hashtable、ReentrantReadWriteLock中写锁         |
| 10 | 重量级锁 | synchronized                                                    |
| 11 | 轻量级锁 | 锁优化技术                                                           |
| 12 | 偏向锁  | 锁优化技术                                                           |
| 13 | 分段锁  | concurrentHashMap                                               |
| 14 | 互斥锁  | synchronized                                                    |
| 15 | 同步锁  | synchronized                                                    |
| 16 | 死锁   | 相互请求对方的资源                                                       |
| 17 | 锁粗化  | 锁优化技术                                                           |
| 18 | 锁消除  | 锁优化技术                                                           |

并发下，同时访问数据会出现错误，那么，如果我不同时访问，

当并发来的时候，同一时间只允许同一时间访问，这样就可以了

锁是一种数据保护机制，可允许某一个线程（进程）进行操作锁，

当文件锁上时，其他线程（进程）根据锁的性质（读写锁，阻塞非阻塞）

## 18-2：乐观锁与悲观锁

1. 乐观锁
   假定当前环境是读多写少，遇到并发写的概率比较低，读数据时认为别的线程不会正在进行修改也因此没有上锁。
   写数据时，判断当前与期望值是否相同，如果相同则进行更新，
   更新期间加锁，保证是原子性的。
   可以同时进行读操作，读的时候其他线程不能进行写操作。
2. 悲观锁
   认为写多读少，遇到并发写的可能性高，每次去拿数据的时候都认为其他线程会修改，
   所以每次读写数据都会认为其他线程会修改，所以每次读写数据时都会上锁。
   其他线程想要读写这个数据时，会被这个线程block，直到这个线程释放锁然后其他线程获取到锁。
   只能有一个线程进行读操作或者写操作，其他线程的读写操作均不能进行。

### 18-2-1：两种锁的使用场景

1. Java中的乐观锁：
    CAS---比较并替换，
    比较当前值（主内存中的值），
    与预期值（当前线程中的值，主内存中值的一份拷贝）是否一样，
    一样则更新，否则继续进行CAS操作。
2. Java中的悲观锁： 
   synchronized修饰的方法和方法块、ReentrantLock。

### 18-2-2：乐观锁常见的两种实现方式

乐观锁一般会使用版本号机制或CAS算法实现。
1. 版本号机制
   一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，
   当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取
   version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，
   否则重试更新操作，直到更新成功。
2. CAS算法

### 18-2-3：乐观锁的缺点

1. ABA 问题
   JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，
   其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，
   并且当前标志是否等于预期标志，
   如果全部相等，
   则以原子方式将该引用和该标志的值设置为给定的更新值。
2. 循环时间长开销大
   自旋CAS（也就是不成功就一直循环执行直到成功）
   如果长时间不成功，会给CPU带来非常大的执行开销。 
   如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，
   pause指令有两个作用，第一它可以延迟流水线执行指令,
   使CPU不会消耗过多的执行资源，
   延迟的时间取决于具体实现的版本，
   在一些处理器上延迟时间是零。第二它可以避
   免在退出循环的时候因内存顺序冲突而引起CPU流水线被清空，
   从而提高CPU的执行效率。
3. 只能保证一个共享变量的原子操作
   CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。
   但是从 JDK 1.5开始，
   提供了AtomicReference类来保证引用对象之间的原子性，
   你可以把多个变量
   放在一个对象里来进行 CAS 操作.
   所以我们可以使用锁或者利用AtomicReference类
   把多个共享变量合并成一个共享变量来操作。

## 18-3：自旋锁

是指当一个线程在获取锁的时候，
如果锁已经被其它线程获取，
那么该线程将循环等待，
然后不断的判断锁是否能够被成功获取，
直到获取到锁才会退出循环。

### 18-3-1：自旋锁的优缺点

1. 自旋锁的优点： 避免了线程切换的开销。挂起线程和恢复线程的操作都需要转入内核态中完成，
                  这些操作给Java虚拟机的并发性能带来了很大的压力。
2. 自旋锁的缺点： 占用处理器的时间，如果占用的时间很长，会白白消耗处理器资源，
                 而不会做任何有价值的工作，带来性能的浪费。因此自旋等待的时间必须有一定的
                 限度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程。

### 18-3-2：自旋锁的升级——自适应自旋

自适应意味着自旋的时间不再是固定的，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的。
有了自适应自旋，随着程序运行时间的增长及性能监控信息
的不断完善，虚拟机对程序锁的状态预测就会越来越精准。

### 18-3-3：自旋锁使用场景

Java中的自旋锁： CAS操作中的比较操作失败后的自旋等待。

## 18-4：可重入锁（递归锁）

任意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞。可重入锁的作用是避免死锁。
通过组合自定义同步器来实现锁的获取与释放。
- 再次获取锁：识别获取锁的线程是否为当前占据锁的线程，
             如果是，则再次成功获取。获取锁后，进行计数自增，
- 释放锁：释放锁时，进行计数自减。

### 18-4-1：可重入锁使用场景

ReentrantLock、synchronized修饰的方法或代码段。

### 18-4-2：可重入锁如果加了两把，但是只释放了一把会出现什么问题？

程序卡死，线程不能出来，也就是说我们申请了几把锁，就需要释放几把锁。

### 18-4-3：如果只加了一把锁，释放两次会出现什么问题？

会报错，java.lang.IllegalMonitorStateException。


## 18-5：读写锁

通过ReentrantReadWriteLock类来实现。为了提高性能， 
Java 提供了读写锁，在读的地方使用读锁，
在写的地方使用写锁，灵活控制，如果没有写锁的情况下，
读是无阻塞的，在一定程度上提高了程序的执行效率。
读写锁分为读锁和写锁，多个读锁不互斥，
读锁与写锁互斥，这是由 jvm 自己控制的。
读锁： 允许多个线程获取读锁，同时访问同一个资源。
写锁： 只允许一个线程获取写锁，不允许同时访问同一个资源。

## 18-6：公平锁与非公平锁

1. 公平锁
多个线程按照申请锁的顺序来获取锁。在并发环境中，每个线程会先查看此锁维护的等待队列，
如果当前等待队列为空，则占有锁，如果等待队列不为空，
则加入到等待队列的末尾，按照FIFO的原则从队列中拿到线程，然后占有锁。
也就是如果一个线程组里，能保证每个线程都能拿到锁

2. 非公平锁

线程尝试获取锁，如果获取不到，则再采用公平锁的方式。多个线程获取锁的顺序，
不是按照先到先得的顺序，有可能后申请锁的线程比先申请的线程优先获取锁。

### 18-6-1：公平锁与非公平锁优缺点

优点： 非公平锁的性能高于公平锁。
缺点： 有可能造成线程饥饿（某个线程很长一段时间获取不到锁）

### 18-6-2：公平锁与非公平锁使用场景

synchronized是非公平锁，
ReentrantLock通过构造函数指定该锁是公平的还是非公平的，默认是非公平的。

## 18-7：共享锁

a.当试图读取数据时，事务默认会为所依赖的数据资源请求共享锁。
b.持有共享锁时间：从事务得到共享锁到读操作完成。
c.多个事务可以在同一阶段用共享锁作用于同一数据资源。
d.在读取数据时，可以对如何处理锁定进行控制。后面隔离级别会讲到如何对锁定进行控制。

### 18-7-1：共享锁使用场景

ReentrantReadWriteLock

## 18-8：独占锁

只能有一个线程获取锁，以独占的方式持有锁。和悲观锁、互斥锁同义。

### 18-8-1：独占锁使用场景

synchronized，ReentrantLock

## 18-9：重量级锁

内置锁在Java中被抽象为监视器锁（monitor）。
在JDK 1.6之前，
监视器锁可以认为直接对应底层操作系统中的互斥量（mutex）。
这种同步方式的成本非常高，
包括系统调用引起的内核态与用户态切换、线程阻塞造成的线程切换等。

### 18-9-1：重量级锁使用场景

synchronized

## 18-10：轻量级锁

轻量级锁不是为了代替重量级锁，
它的本意是在没有多线程竞争的前提下，
减少传统的重量级锁使用操作系统互斥量产生的性能消耗，
因为使用轻量级锁时，不需要申请互斥量。
如果没有竞争，轻量级锁使用CAS操作避免了使用互斥操作的开销。但如果存在锁竞争，除了互斥量开销外，
还会额外发生CAS操作，因此在有锁竞争的情况下，轻量级锁比传统的重量级锁更慢！
如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！

### 18-10-1：轻量级锁优缺点

优点： 如果没有竞争，通过CAS操作成功避免了使用互斥量的开销。
缺点： 如果存在竞争，除了互斥量本身的开销外，还额外产生了CAS操作的开销，
       因此在有竞争的情况下，轻量级锁比传统的重量级锁更慢。

## 18-11：偏向锁

在没有实际竞争的情况下，还能够针对部分场景继续优化。
如果不仅仅没有实际竞争，自始至终，使用锁的线程都只有一个，
那么，维护轻量级锁都是浪费的。偏向锁的目标是，
减少无竞争且只有一个线程使用锁的情况下，
使用轻量级锁产生的性能消耗。
轻量级锁每次申请、释放锁都至少需要一次CAS，
但偏向锁只有初始化时需要一次CAS。
“偏向”的意思是，偏向锁假定将来只有第一个申请锁的线程会使用锁（不会有任何线程再来申请锁），
因此，只需要在Mark Word中CAS记录owner（本质上也是更新，但初始值为空），
如果记录成功，则偏向锁获取成功，记录锁状态为偏向锁，
以后当前线程等于owner就可以零成本的直接获得锁；
否则，说明有其他线程竞争，膨胀为轻量级锁。
偏向锁无法使用自旋锁优化，因为一旦有其他线程申请锁，就破坏了偏向锁的假定。

### 18-11-1：偏向锁优缺点

优点： 把整个同步都消除掉，连CAS操作都不去做了，优于轻量级锁。
缺点： 如果程序中大多数的锁都总是被多个不同的线程访问，那偏向锁就是多余的。

## 18-12：分段锁

它内部细分了若干个小的 HashMap，称之为段(Segment)。
默认情况下一个 ConcurrentHashMap 被进一步细分为 16 个段，
既就是锁的并发度。
如果需要在ConcurrentHashMap 添加一项key-value，
并不是将整个 HashMap 加锁，
而是首先根据 hashcode 得到该key-value应该存放在哪个段中，
然后对该段加锁，
并完成 put 操作。
在多线程环境中，如果多个线程同时进行put操作，
只要被加入的key-value不存放在同一个段中，
则线程间可以做到真正的并行。
ConcurrentHashMap 是一个 Segment 数组， 
Segment 通过继承ReentrantLock 来进行加锁，
所以每次需要加锁的操作锁住的是一个 segment，
这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全

## 18-13：互斥锁

也是x锁，该锁每一次只能被一个线程锁持有，
加锁后任何线程试图再次加锁的线程会被阻塞，
直到当前线程解锁，例子：如果 线程A 对deta1加上排它锁后，
则其他线程不能再对data1 加任何类型的锁，
获得互斥锁的线既能读取数据又能修改数据

## 18-14：同步锁

表示并发执行的多个线程，在同一时间内只允许一个线程访问共享数据。

## 18-15：死锁

如线程A持有资源x，线程B持有资源y，
线程A等待线程B释放资源y，线程B等待线程A释放资源x，
两个线程都不释放自己持有的资源，
则两个线程都获取不到对方的资源，就会造成死锁。
Java中的死锁不能自行打破，所以线程死锁后，
线程不能进行响应。所以一定要注意程序的并发场景，避免造成死锁。

## 18-16：锁粗化

如果一系列的连续操作都对同一个对象反复加锁和解锁，
甚至加锁操作都是出现在循环体体之中，就算真的没有线程竞争，
频繁地进行互斥同步操作将会导致不必要的性能
损耗，所以就采取了一种方案：
把加锁的范围扩展（粗化）到整个操作序列的外部，
这样加锁解锁的频率就会大大降低，
从而减少了性能损耗。

## 18-17：锁消除

就是把锁干掉。当Java虚拟机运行时发现有些共享数据不会被线程竞争时就可以进行锁消除。

## 18-18：提高锁性能的方法
1. 减少锁持有时间
2. 减少锁粒度
3. 读写分离锁来替换独占锁
4. 锁分离
5. 锁粗话

# 19.ThreadLocal

## 19-1：什么是ThreadLocal，优势在哪里

因为我们常用的局部变量和静态变量，在某种情况下无法满足要求，
比如，我要求缓存一个变量，这个时候使用一个静态map存一下就可以了了，但是有几个问题：
第一：其他线程擅自修改我的这个静态map怎么办？
第二：静态map之间并发访问怎么办？
它的本质就是一个内部的静态map，key是当前线程的一个句柄，
value是需要保存的值，value可以是任何类型的，至少是一个map或者容器，
基于这种设计，每个线程其实根本无法获取到其他线程的key，
由于是内部静态map，不提供遍历和查询的接口，也确保了其他线程只能根据key获取，
所以，每个线程只能取到自己线程的value。
这样，即做到了线程安全，又在线程范围内提供了数据共享的能力。

## 19-2：ThreadLocal的实现原理

每个Thread 维护一个 ThreadLocalMap 映射表，
这个映射表的 key 是 ThreadLocal实例本身，value 是真正需要存储的 Object。
ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。
值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，
弱引用的对象在 GC 时会被回收。

## 19-3：ThreadLocal内存泄露问题

ThreadLocalMap 中使⽤的 key 为 ThreadLocal 的弱引⽤,⽽ value 是强引⽤。
所以，如ThreadLocal 没有被外部强引⽤的情况下，在垃圾回收的时候，
key会被清理掉，⽽ value 不会被清理掉。
这样⼀来， ThreadLocalMap 中就会出现key为null的Entry。
假如我们不做任何措施的话，value 永远⽆法被GC 回收，
这个时候就可能会产⽣内存泄露。 
ThreadLocalMap实现中已经考虑了这种情况，
在调⽤ set() 、 get() 、 remove() ⽅法的时候，
会清理掉 key 为 null 的记录。
使⽤完ThreadLocal ⽅法后 最好⼿动调⽤ remove() ⽅法

### 19-3-1：ThreadLocal如何防止内存泄漏？

每次使用完ThreadLocal，都调用它的remove()方法，清除数据。
在使用线程池的情况下，没有及时清理ThreadLocal，
不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。
所以，使用ThreadLocal就跟加锁完要解锁一样，用完就需要清理。

# 20.线程变量绑定

当使用 ThreadLocal 维护变量的时候 
为每一个使用该变量的线程提供一个独立的变量副本，
也就是每个线程内部都会有一个该变量，
这样同时多个线程访问该变量并不会彼此相互影响，
因此他们使用的都是自己从内存中拷贝过来的变量的副本， 
这样就不存在线程安全问题，也不会影响程序的执行性能。
但是由于在每个线程中都创建了副本，
所以要考虑它对资源的消耗，比如内存的占用会比不使用 ThreadLocal 要大。

# 21.无锁-CAS、Atomic

## 21-1：CAS

CAS是比较并交换。比较变量的现在值与之前的值是否一致，若一致则替换，否则不替换。
CAS的作用：原子性更新变量值，保证线程安全。
需要有三个操作数，变量的当前值（V），旧的预期值（A），准备设置的新值（B）。
CAS指令执行条件：当且仅当V=A时，处理器才会设置V=B，否则不执行更新。
CAS的返回值：V的之前值。
CAS处理过程：原子操作，执行期间不会被其他线程中断，线程安全。

### 21-1-1：CAS的ABA问题

如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，
如果在这段期间它的值曾经改成了B，后来又改成了A，那么CAS操作就会误认为它没有改变过，
这个漏洞称为“ABA”问题。
解决的核心思想是加上时间戳来标识不同阶段的数值。比如:J.U.C包为了解决这个问题，
提供了一个带有标记的原子引用类“AtomicStampedReference”，
它可以通过控制变量值的版本来保证CAS的正确性，
如果需要解决ABA问题，改用传统的互斥同步
（典型的就是synchronized和Lock）可能会比原子类更高效。

## 21-2：原子类原理

原子量底层的实现均是采用CAS非阻塞算法实现的，
是无锁（lock-free）算法中最有名的一种
（无锁算法：不使用锁机制来实现线程安全的算法，
采用锁机制都会存在线程为请求锁而产生阻塞的情况）,
CAS不会阻塞线程从而不会带来CPU上下文切换的性能开销。
原子操作是指不会被线程调度机制打断的操作，这种操作一旦开始，
就一直运行到结束，中间不会有任何线程上下文切换。
原子操作可以是一个步骤，也可以是多个操作步骤，
但是其顺序不可以被打乱，也不可以被切割而只执行其中的一部分，
将整个操作视作一个整体是原子性的核心特征。

### 21-1-1：为什么要使用原子类

为了让Java程序员能够受益于CAS等CPU指令，JDK并发包有一个atomic包，
里面实现了一些直接使用CAS操作的线程安全的类型

### 21-1-2：原子类的作用？

提供一种简单、性能高效、线程安全地更新一个变量的方式。

### 21-4：i++自增操作不是原子性的，如何决绝原子性问题

Atomic原子类就是来解决这个问题的，

### 21-5：基本数据类型原子类的优势

多线程环境使用原子类保证线程安全（基本数据类型）

## 21-6：为什么是unsafe

java不能直接访问操作系统底层，而是通过本地方法来访问。
Unsafe类提供了硬件级别的原子操作，主要作用：
1、通过Unsafe类可以分配内存，可以释放内存；
类中提供的3个本地方法allocateMemory、reallocateMemory、
freeMemory分别用于分配内存，扩充内存和释放内存，
2、可以定位对象某字段的内存位置，也可以修改对象的字段值，即使它是私有的；
3、挂起与恢复
将一个线程进行挂起是通过park方法实现的，调用 park后，
线程将一直阻塞直到超时或者中断等条件出现。
unpark可以终止一个挂起的线程，使其恢复正常。
整个并发框架中对线程的挂起操作被封装在 LockSupport类中，
LockSupport类中有各种版本pack方法，但最终都调用了Unsafe.park()方法。
4、CAS操作
是通过compareAndSwapXXX方法实现的

### 26-2：Unsafe为什么是不安全的？

比如使用unsafe创建一个超级大的数组,但是这个数组jvm是不管理的,只能你自己操作,容易oom,也不利于资源的回收.

### 26-3：Unsafe的实例怎么获取？
a. 在jdk8和之前如果获得其单例对象是会抛出异常的，只能通过反射获取，在jdk9及以后，可以通过getUnsafe静态方法获取
b. 我们知道 unsafe是提供给java核心内库使用的，那么我们如何获取Unsafe的实例呢？当然是反射！
c. 代码：
		Field f = Unsafe.class.getDeclaredField("theUnsafe");
		f.setAccessible(true);
		Unsafe unsafe = (Unsafe) f.get(null);

### 26-4：讲一讲Unsafe中的CAS操作？
a. JUC中用到了大量的CAS，他们的底层其实都是采用Unsafe的CAS操作，
b. CAS（比较与交换，Compare and swap）是一种有名的无锁算法,因为不需要加锁，性能比加锁搞。CAS是一个CPU指令。CAS还是一个乐观锁技术
c. CAS存在的问题：
		i. 经典的ABA问题，危害有（以栈举例），解决方案：版本号控制，有的数据结构在高位用邮戳标记；不重复使用节点引用，而是构建新的节点，
		ii. CAS常常搭配自旋一起使用，如果自选长时间不成功，循环时间长 开销大
		iii. 只能保持一个共享变量的安全操作

### 26-5：unsafe的阻塞/唤醒操作？
a. LockSupport类中的park与unpark方法对unsafe中的park与unpark方法做了封装，LockSupport类中有各种版本pack方法，
但最终都调用了Unsafe.park()方法。



# 23.AQS（队列同步器）

## 23-1：对AQS原理分析

如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的⼯作线程，并且将共享资源设置为锁定状态。
如果被请求的共享资源被占⽤，那么就需要⼀套线程阻塞等待以及被唤醒时锁分配的机制，
这个机制AQS是⽤CLH队列锁实现的，即将暂时获取不到锁的线程加⼊到队列中。

## 23-2：AQS 对资源的共享⽅式

1. Exclusive（独占锁）：只有⼀个线程能执⾏，如ReentrantLock。⼜可分为公平锁和⾮公平锁：
   * 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁
   * ⾮公平锁：当线程要获取锁时，⽆视队列顺序直接去抢锁，谁抢到就是谁的
2. Share（共享）：多个线程可同时执⾏，如Semaphore/CountDownLatch。 Semaphore、
CountDownLatch、 CyclicBarrier、 ReadWriteLock 我们都会在后⾯讲到。

## 23-3：AQS 组件

1. Semaphore(信号量)-允许多个线程同时访问：
   synchronized 与 ReentrantLock 都是一次只允许一个线程访问某个资源，
   Semaphore(信号量)可以指定多个线程同时访问某个资源。

2. CountDownLatch （倒计时器）： 
                               CountDownLatch是一个同步工具类，用来协调多个线程之间的同步。
                               这个工具通常用来控制线程等待，
                               它可以让某一个线程等待直到倒计时结束，再开始执行。

3. CyclicBarrier(循环栅栏)： 
                           CyclicBarrier 和 CountDownLatch 非常类似，
                           它也可以实现线程间的技术等待，
                           但是它的功能比 CountDownLatch 更加复杂和强大。
                           主要应用场景和 CountDownLatch 类似。
                           CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。
                           它要做的事情是，
                           让一组线程到达一个屏障（也可以叫同步点）时被阻塞，
                           直到最后一个线程到达屏障时，
                           屏障才会开门，所有被屏障拦截的线程才会继续干活。
                           CyclicBarrier默认的
                           构造方法是 CyclicBarrier(int parties)，
                           其参数表示屏障拦截的线程数量，
                           每个线程调用await()方法告诉
                            CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。

## 23-4：AQS应用场景





# 24.并发容器

## 24-1：JDK 提供的并发容器总结

ConcurrentHashMap: 线程安全的 HashMap
CopyOnWriteArrayList: 线程安全的 List，
                      在读多写少的场合性能非常好，
                      远远好于 Vector.
ConcurrentLinkedQueue: 高效的并发队列，使用链表实现。
                       可以看做一个线程安全的 LinkedList，
                       这是一个非阻塞队列。
BlockingQueue（`堵塞队列`）: 这是一个接口，JDK 内部通过链表、
                             数组等方式实现了这个接口。
                             表示阻塞队列，
                             非常适合用于作为数据共享的通道。
ConcurrentSkipListMap: 跳表的实现。这是一个 Map，
                       使用跳表的数据结构进行快速查找。

## 24-2：CopyOnWriteArrayList 是如何做到的？

CopyOnWriteArrayList 类的所有可变操作（add，set 等等）
都是通过创建底层数组的新副本来实现的。
当 List 需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，
将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，
这样就可以保证写操作不会影响读操作了。
从 CopyOnWriteArrayList 的名字就能看出CopyOnWriteArrayList 
是满足CopyOnWrite 的 ArrayList，
在计算机，如果你想要对一块内存进行修改时，
我们不在原有内存块中进行写操作，
而是将内存拷贝一份，在新的内存中进行写操作，
写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。

## 24-3：BlockingQueue

### 24-3-1：什么是阻塞队列

当队列为空时，消费者挂起，队列已满时，生产者挂起，
这就是生产-消费者模型，堵塞其实就是将线程挂起。
因为生产者的生产速度和消费者的消费速度之间的不匹配，
就可以通过堵塞队列让速度快的暂时堵塞,
如生产者每秒生产两个数据，而消费者每秒消费一个数据，
当队列已满时，生产者就会堵塞（挂起），等待消费者消费后，再进行唤醒。
堵塞队列会通过挂起的方式来实现生产者和消费者之间的平衡，
这是和普通队列最大的区别。

### 24-3-2：阻塞队列的常用类

使用的就是是lock锁的多条件（condition）阻塞控制
直接提交队列、有界队列、无界队列、优先级任行队
`(1）直接提交队列`:设置为SynchronousOueue队列，SynchronousQueue是一个特殊的
BlockingQueue，它没有容量，没执行一个插入操作就会阻塞，需要再执行一个删除操作才会
被唤醒，反之每一个删除操作也都要等待对应的插入操作。
`(2）有界的任务队列`:有界的任务队列可以使用ArrayBlockingQucue实现。若有新的任务
需要执行时，线程池会创建新的线程，直到创建的线程数量达到corePoolSize时，则会将新
的任务加入到等待队列中。若等待队列已满，即超过ArrayBlockingQueue初始化的容量，则
继续创建线程，直到线程数量达到 maximumPoolSize设置的最大线程数量，
若大于ximumPoolSize，则执行拒绝策略。
`(3）无界的任务队列`:有界任务队列可以使用LinkedBlockingQueue实现。使用无界任务队
列，线程池的任务队列可以无限制的添加新的任务，而线程池创建的最大线程数量就是你
corePoolSize设置的数量，在这种情况下maximumPoolSize这个参数是无效的，即使任务队列中缓存了很多未执行的任务，
当线程池的线程数达到corePoolSize后，就不会再增加了;若后续有新的任务加入，
则直接进入队列等待，当使用这种任务队列模式时，
一定要注意你任务提交与处理之间的协调与控制，不然会出现队列中的任务由于无法及时处
理导致一直增长，直到最后资源耗尽的问题。
`（4）优先任务队列`:优先任务队列通过 PriorityBlockingQueue实现。
PriorityBlockingQueue它其实是一个特殊的无界队列，它其中无论添加了多少个任
务，线程池创建的线程数也不会超过corePoolSize的数量，只不过其他队列一般是按照先进
先出的规则处理任务，而PriorityBlockingQueue队列可以自定义规则根据任务的优先级顺序
先后执行。

### 24-3-3：手写堵塞队列

```java
public class YzBlockingQuery {
    private Object[] tab; //队列容器
    private int takeIndex; //出队下标
    private int putIndex; //入队下标
    private int size;//元素数量
    private ReentrantLock reentrantLock = new ReentrantLock();
    private Condition notEmpty;//读条件
    private Condition notFull;//写条件
    public YzBlockingQuery(int tabCount) {
        if (tabCount <= 0) {
            new NullPointerException();
        }
        tab = new Object[tabCount];
        notEmpty = reentrantLock.newCondition();
        notFull = reentrantLock.newCondition();
    }
    public boolean offer(Object obj) {
        if (obj == null) { throw new NullPointerException(); }
        try {
            //获取锁
            reentrantLock.lock();
            //队列已满
            while (size==tab.length){
                System.out.println("队列已满");
                //堵塞
                notFull.await();
            }
            tab[putIndex]=obj;
            if(++putIndex==tab.length){
                putIndex=0;
            }
            size++;
            //唤醒读线程
            notEmpty.signal();
            return true;
        } catch (Exception e) {
            //唤醒读线程
            notEmpty.signal();
        } finally {
            reentrantLock.unlock();
        }
        return false;
    }
    public Object take(){
        try {
            reentrantLock.lock();
            while (size==0){
                System.out.println("队列空了");
                //堵塞
                notEmpty.await();
            }
            Object obj= tab[takeIndex];
            //如果到了最后一个，则从头开始
            if(++takeIndex==tab.length){
                takeIndex=0;
            }
            size--;
            //唤醒写线程
            notFull.signal();
            return obj;
        }catch (Exception e){
            //唤醒写线程
            notFull.signal();
        }finally {
            reentrantLock.unlock();
        }
        return null;
    }
    public static void main(String[] args) {
        Random random = new Random(100);
        YzBlockingQuery yzBlockingQuery=new YzBlockingQuery(5);
        Thread thread1 = new Thread(() -> {
            for (int i=0;i<100;i++) {
                try {
                    Thread.sleep(300);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                yzBlockingQuery.offer(i);
                System.out.println("生产者生产了："+i);
            }
        });

        Thread thread2 = new Thread(() -> {
            for (int i=0;i<100;i++) {
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                Object take = yzBlockingQuery.take();
                System.out.println("消费者消费了："+take);
            }
        });

        thread1.start();
        thread2.start();
    }
}
```

# 25.快速失败与安全失败

一:快速失败（fail-fast）
在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改
（增加.删除、修改），则会抛出 Concurrent Modification Exception。

`场景`:java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改)。

二:安全失败（failsafe）

采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，
而是先复制原有集合内容，在拷贝的集合上进行遍历。

原理:由于迭代时是对原集合的拷贝进行遍历，
所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，
所以不会触发Concurrent Modification Exception。

`缺点`:基于拷贝内容的优点是避免了Concurrent Modification Exception，
但同样地，迭代器并不能访问到修改后的内容，即:迭代器遍历的是开始遍历那一刻拿到的集合拷贝，
在遍历期间原集合发生的修改迭代器是不知道的。

`场景`:java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。

